diff --git a/examples/3_NeuralNetworks/multilayer_perceptron.py b/examples/3_NeuralNetworks/multilayer_perceptron.py
index cf04b01..44e3986 100644
--- a/examples/3_NeuralNetworks/multilayer_perceptron.py
+++ b/examples/3_NeuralNetworks/multilayer_perceptron.py
@@ -58,11 +58,11 @@ biases = {
 # Create model
 def multilayer_perceptron(x):
     # Hidden fully connected layer with 256 neurons
-    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])
+    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['h1']), biases['b1']))
     # Hidden fully connected layer with 256 neurons
-    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])
+    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']))
     # Output fully connected layer with a neuron for each class
-    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']
+    out_layer = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['out']), biases['out']))
     return out_layer
 
 # Construct model
@@ -76,6 +76,9 @@ train_op = optimizer.minimize(loss_op)
 # Initializing the variables
 init = tf.global_variables_initializer()
 
+# 'Saver' op to save and restore all the variables
+saver = tf.train.Saver()
+
 with tf.Session() as sess:
     sess.run(init)
 
@@ -102,3 +105,5 @@ with tf.Session() as sess:
     # Calculate accuracy
     accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
     print("Accuracy:", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))
+    # Save model weights to disk
+    save_path = saver.save(sess, "/tmp/sampleMLP.ckpt")
