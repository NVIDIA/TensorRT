#
# SPDX-FileCopyrightText: Copyright (c) 1993-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

"""
Utils for accuracy check and performance measurements for Seq2Seq models.
"""

# torch
import torch

from NNDF.torch_utils import use_cuda
from NNDF.tensorrt_utils import TRTNativeRunner

@use_cuda
def calculate_perplexity_helper_encoder_decoder(
    encoder,
    decoder,
    tokenizer,
    input_str,
    reference_str,
    batch_size,
    max_length=None,
    use_cuda=True,
    use_mask=False,
):
    tokenizer_output = tokenizer([input_str] * batch_size, padding=True, return_tensors="pt")
    input_ids = tokenizer_output.input_ids
    attention_mask = tokenizer_output.attention_mask

    decoder_tokenizer_output = tokenizer([reference_str] * batch_size, padding=True, return_tensors="pt")
    decoder_input_ids = decoder_tokenizer_output.input_ids
    decoder_attention_mask = decoder_tokenizer_output.attention_mask

    if use_cuda:
        input_ids = input_ids.cuda()
        decoder_input_ids = decoder_input_ids.cuda()
        attention_mask = attention_mask.cuda()
        decoder_attention_mask = decoder_attention_mask.cuda()

    encoder_outputs = encoder(input_ids=input_ids, attention_mask=attention_mask if use_mask else None)

    # Set the first token to be 0.
    decoder_input_ids_padded = torch.zeros(
        decoder_input_ids.size()[:-1] + (decoder_input_ids.size()[-1] + 1,),
        dtype=decoder_input_ids.dtype,
        device=decoder_input_ids.device,
    )
    decoder_input_ids_padded[..., 1:] = decoder_input_ids
    decoder_attention_mask = torch.cat((decoder_attention_mask, torch.ones([decoder_input_ids.shape[0], 1], device = decoder_input_ids.device)), -1)

    if isinstance(decoder, TRTNativeRunner):
        decoder.set_encoder_hidden_states(encoder_outputs.last_hidden_state)

    with torch.no_grad():
        if max_length is not None:
            decoder_input_ids_padded = decoder_input_ids_padded[:, :max_length]
            decoder_attention_mask = decoder_attention_mask[:, :max_length]

        logits = decoder(
            input_ids=decoder_input_ids_padded,
            encoder_outputs=encoder_outputs,
            attention_mask=decoder_attention_mask if use_mask else None,
        ).logits

        # Truncate the last prediction
        logits = logits[:, :-1, :]
        loss = torch.nn.CrossEntropyLoss()(logits.permute((0, 2, 1)), decoder_input_ids)

    return torch.exp(loss).item()

@use_cuda
def calculate_perplexity_helper_decoder(
    decoder,
    tokenizer,
    input_str,
    batch_size,
    max_length=None,
    use_cuda=True,
    use_mask=False,
):

    if isinstance(input_str, list):
        input_str = [i.replace("\\n", "\n") for i in input_str] * (batch_size // len(input_str))
        tokenizer_output = tokenizer(input_str, padding=True, return_tensors="pt")
    else:
        input_str = input_str.replace("\\n", "\n")
        tokenizer_output = tokenizer([input_str] * batch_size, padding=True, return_tensors="pt")

    input_ids = tokenizer_output.input_ids
    attention_mask = tokenizer_output.attention_mask

    if use_cuda:
        input_ids = input_ids.to("cuda")
        attention_mask = attention_mask.to("cuda")

    with torch.no_grad():
        if max_length is not None:
            input_ids = input_ids[:, :max_length]
            attention_mask = attention_mask[:, :max_length]

        logits = decoder(input_ids=input_ids, attention_mask=attention_mask if use_mask else None).logits
        # Shift logits and target ids so that probabilities generated by token < n line up with output token n.
        shifted_logits = logits[:, :-1, :]
        target_ids = input_ids[:, 1:]
        loss = torch.nn.CrossEntropyLoss()(shifted_logits.permute((0, 2, 1)), target_ids)
    return torch.exp(loss).item()
