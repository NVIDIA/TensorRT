{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e6e614-e360-4292-965e-0d255027e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88dc1a-a92d-44cc-9fb7-d9e2ef20c8e2",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Accelerating HuggingFace T5 Inference with TensorRT\n",
    "\n",
    "T5 is an encoder-decoder model that converts all NLP problems into a text-to-text format. More specifically, it does so by encoding  different tasks as text directives in the input stream. This enables a single model to be trained supervised on a wide variety of NLP tasks such as translation, classification, Q&A and summarization.\n",
    "\n",
    "This notebook shows easy steps to convert a [HuggingFace PyTorch T5 model](https://huggingface.co/transformers/model_doc/t5.html) to a TensorRT engine for high-performance inference in a few lines of code. \n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "Follow the instruction at https://github.com/NVIDIA/TensorRT to build the TensorRT-OSS docker container required to run this notebook.\n",
    "\n",
    "Next, we install some extra dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c36ecb7-c622-4d95-a851-b9a6eb18e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bbdafb",
   "metadata": {},
   "source": [
    "**Note:** After this step, you should restart the Jupyter kernel for the change to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d2f1b-439e-4cd0-8286-1d63a13f2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import torch\n",
    "\n",
    "from T5.frameworks import T5HF\n",
    "from T5.trt import T5TRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c1e4cd",
   "metadata": {},
   "source": [
    "\n",
    "## API usage\n",
    "\n",
    "We have wrapped the process of importing models from PyTorch, exporting to onnx files and build TRT engines into a single class. We introduce new `T5HF` and `T5TRT` classes that both expose `generate` as the main entry point to run T5. `T5TRT` will automatically do all the 3 steps per user inputs. Here is an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e974ef4",
   "metadata": {},
   "source": [
    "\n",
    "### Specify model arguments\n",
    "\n",
    "You pick your favorite model and configurations, and TRT will run it for you! The main choice that you need to make is:\n",
    "- `use_cache`: kv cache to speed decoding\n",
    "- `num_beams`: beam search for better results\n",
    "- `fp16`: Using float16 to speed decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc41e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"variant\": \"t5-base\", # A HuggingFace model variant name. Required.\n",
    "    \"use_cache\": True, # We support decoder kv cache in generation. Default: False\n",
    "    \"fp16\": True, # Default: False\n",
    "    \"num_beams\": 3, # We support beam search in generation. Default: 1\n",
    "    \"batch_size\": 1, # Default: 1\n",
    "    # Folder name. Required. All the PyTorch, ONNX and TRT Engines will be stored in the folder.\n",
    "    \"working_dir\": \"models\",\n",
    "    # Log level.\n",
    "    \"info\": True,\n",
    "    # Benchmarking args\n",
    "    \"iterations\": 10,\n",
    "    \"number\": 1,\n",
    "    \"warmup\": 3,\n",
    "    \"duration\": 0,\n",
    "    \"percentile\": 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2f3b18",
   "metadata": {},
   "source": [
    "\n",
    "### Initialize the models\n",
    "Calling the API is just this easy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce5f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_model = T5HF(**args)\n",
    "trt_model = T5TRT(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1b0d9b",
   "metadata": {},
   "source": [
    "### Try your sentence!\n",
    "Both `T5HF` and `T5TRT` exposes `setup_tokenizer_and_model` and `generate`. If `setup_tokenizer_and_model` is not called prior to `generate`, it will be called first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701712bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"translate English to German: TensorRT is the best deep learning inference accelerator.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb76961",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_model.models = framework_model.setup_tokenizer_and_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d66302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_model.generate(input=input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1065b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_model.models = trt_model.setup_tokenizer_and_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dbf2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_model.generate(input=input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2c055d",
   "metadata": {},
   "source": [
    "\n",
    "### Performance benchmark\n",
    "You can see that TRT and PyTorch generates the same result, which is expected. To measure their performance, both `T5HF` and `T5TRT` exposes `execute_inference`, `full_inference`, `encoder_inference` and `decoder_inference` to measure the inference time. Let's take a look at how our latest TRT performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054bfac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "data = [\n",
    "    ['full p50(s)', 'decoder p50(s)', 'encoder p50(s)'],\n",
    "]\n",
    "\n",
    "def format_result(result):\n",
    "    entry = []\n",
    "    for segment in result.runtime:\n",
    "        entry.append('{:.4f}'.format(segment.runtime))\n",
    "    \n",
    "    return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_result = framework_model.execute_inference(input_str)\n",
    "data.append(format_result(framework_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a82bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_result = trt_model.execute_inference(input_str)\n",
    "data.append(format_result(trt_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5e5d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tabulate(data, headers='firstrow', tablefmt='github'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a498672-ba25-42b0-b89e-79e0b869943a",
   "metadata": {},
   "source": [
    "### Variable Input/Output Performance Benchmarking\n",
    "\n",
    "We can run more tests by varying input/output length, while using the same engines.\n",
    "\n",
    "Note that TensorRT performance depends on optimal selection of the kernels in the engine. The variable length test here uses the same engine built with max input/output length profile = `max_length` in HuggingFace config to represent the best use of the model. If you want to change the length, please change this field prior to calling `set_tokenizer_and_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e335010-ff7f-4822-85ae-bca8d235de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_output_len_list = [\n",
    "    (64, 128),\n",
    "    (128, 64), \n",
    "    (32, 32),\n",
    "    (128, 128),\n",
    "]\n",
    "\n",
    "data = [\n",
    "    ['(input_len, output_len)', 'HF p50 (s)', 'TRT p50 (s)'],\n",
    "]\n",
    "\n",
    "for (in_len, out_len) in input_output_len_list:\n",
    "\n",
    "    input_ids = torch.randint(0, framework_model.config.vocab_size, (framework_model.config.batch_size, in_len))\n",
    "    \n",
    "    framework_model.config.min_output_length = out_len\n",
    "    framework_model.config.max_output_length = out_len\n",
    "    trt_model.config.min_output_length = out_len\n",
    "    trt_model.config.max_output_length = out_len\n",
    "    \n",
    "    _, framework_e2e = framework_model.full_inference(input_ids)\n",
    "    _, trt_e2e = trt_model.full_inference(input_ids)\n",
    "\n",
    "    data.append([(in_len, out_len), framework_e2e, trt_e2e])\n",
    "\n",
    "print(tabulate(data, headers='firstrow', tablefmt='github'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fbb1d4",
   "metadata": {},
   "source": [
    "Did TensorRT's performance amaze you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4254e2-11fd-4bc7-ac0b-60b1a9e07c4e",
   "metadata": {},
   "source": [
    "## Conclusion and where-to next?\n",
    "\n",
    "Is this the end? The API sounds too simple. I am used to the previous version that walks me step by step, and/or I want to know more on the process of conversion. Just follow the directory and you will find that PyTorch model, ONNX files and TRT engines are there. Feel free to investigate them. We have wrapped the entire model conversion process in `setup_tokenizer_and_model`. The TensorRT inference engine can be conviniently used as a drop-in replacement for the orginial HuggingFace T5 model while providing significant speed up. If you are interested in further details of the conversion process, check out [T5](../T5) and [Seq2Seq/trt.py](../Seq2Seq/trt.py). You will find that all the Seq2Seq models could be treated in a similar way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a8b7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
