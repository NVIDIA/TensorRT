{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e6e614-e360-4292-965e-0d255027e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d86fae-c008-44c1-a9e9-43145094a333",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Accelerating HuggingFace GPT-2 Inference with TensorRT\n",
    "\n",
    "GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. The model was pretrained on the raw texts to predict the next word in sentences. As no human labeling was required, GPT-2 pretraining can use lots of publicly available data with an automatic process to generate inputs and labels from those data.\n",
    "\n",
    "This notebook shows 3 easy steps to convert a [HuggingFace PyTorch GPT-2 model](https://huggingface.co/gpt2) to a TensorRT engine for high-performance inference.\n",
    "\n",
    "1. [Download HuggingFace GPT-2 model ](#1)\n",
    "1. [Convert to ONNX format](#2)\n",
    "1. [Convert to TensorRT engine](#3)\n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "Follow the instruction at https://github.com/NVIDIA/TensorRT to build the TensorRT-OSS docker container required to run this notebook.\n",
    "\n",
    "Next, we install some extra dependencies and restart the Jupyter kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29dac08-e043-4310-9eb1-2ab989fa285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install -r ../requirements.txt\n",
    "\n",
    "# install Pytorch with A100 support\n",
    "!pip3 install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio===0.9.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "import IPython\n",
    "import time\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n",
    "\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d2f1b-439e-4cd0-8286-1d63a13f2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import torch \n",
    "\n",
    "# huggingface\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2Config,\n",
    ")\n",
    "\n",
    "# to display detailed TensorRT conversion process\n",
    "from NNDF.logger import G_LOGGER\n",
    "G_LOGGER.setLevel(level=G_LOGGER.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4254e2-11fd-4bc7-ac0b-60b1a9e07c4e",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1. Download HuggingFace GPT-2 model \n",
    "\n",
    "First, we download the original HuggingFace PyTorch GPT-2 model from HuggingFace model hubs, together with its associated tokernizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae66d58-f994-4987-8f1d-1fa8ac2ec8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download model and tokernizer\n",
    "# The GPT-2 variants supported by TensorRT 8.2 are: gpt2 (117M), gpt2-large (774M). However, as the conversion process takes long time with\n",
    "# gpt2-large, we recommend using the ../run.py script. See ../README.md for more details.\n",
    "GPT2_VARIANT = 'gpt2'\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(GPT2_VARIANT)\n",
    "\n",
    "config = GPT2Config(GPT2_VARIANT)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(GPT2_VARIANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7252ca90-1104-40dc-8e72-f51c07a4cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model locally\n",
    "pytorch_model_dir = './models/{}/pytorch'.format(GPT2_VARIANT)\n",
    "!mkdir -p $pytorch_model_dir\n",
    "\n",
    "model.save_pretrained(pytorch_model_dir)\n",
    "print(\"Pytorch Model saved to {}\".format(pytorch_model_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c5766-97ed-4d04-bab5-7fa18e89dee8",
   "metadata": {},
   "source": [
    "### Inference with PyTorch model\n",
    "\n",
    "#### Single example inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c5fe7-7733-49b5-89c5-c8278ff54fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carry out inference with a single sample\n",
    "inputs = tokenizer(\"TensorRT is a high performance deep learning inference platform that delivers low latency and high throughput for appssuch as recommenders, speech and image/video on NVIDIA GPUs.\", return_tensors=\"pt\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c0468b-976a-4a08-98d3-e87578ec067f",
   "metadata": {},
   "source": [
    "For benchmarking purposes, we will employ a helper function `gpt2_inference` which executes the inference on a single batch repeatedly and measures end to end execution time. Let's take note of this execution time for later comparison with TensorRT. \n",
    " \n",
    "`TimingProfile` is a named tuple that specifies the number of experiments and number of times to call the function per iteration (and number of warm-up calls although it is not used here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf8f00-0562-482b-9bec-b0b7596aec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2.measurements import gpt2_inference\n",
    "from NNDF.networks import TimingProfile\n",
    "\n",
    "# Benchmarking TensorRT performance on single batch\n",
    "output, decoder_e2e_median_time = gpt2_inference(\n",
    "            model.to('cuda:0'), inputs.input_ids.to('cuda:0'), TimingProfile(iterations=10, number=1, warmup=1)\n",
    "        )\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4805756f-81f9-43cf-88f6-b205ecd23034",
   "metadata": {},
   "source": [
    "#### Open-end text generation\n",
    "Next, we will employ the PyTorch model for the open-end text generation task, which GPT-2 is particularly good at. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3d01fc-9928-486b-9d15-de84d46528e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2.GPT2ModelConfig import GPT2ModelTRTConfig\n",
    "\n",
    "sample_output = model.to('cuda:0').generate(inputs.input_ids.to('cuda:0'), max_length=128)\n",
    "\n",
    "# de-tokenize model output to raw text\n",
    "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b016c2f-7982-44ac-81e5-d3854391a8b6",
   "metadata": {},
   "source": [
    "For benchmarking purposes, we will employ a helper function `full_inference_greedy` which executes the inference repeatedly and measures end to end execution time. Let's take note of this execution time for later comparison with TensorRT. \n",
    " \n",
    "TimingProfile is a named tuple that specifies the number of experiments and number of times to call the function per iteration (and number of warm-up calls although it is not used here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aea249-529e-4b5e-9759-e0c8370391a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2.measurements import full_inference_greedy\n",
    "\n",
    "# get complete decoder inference result and its timing profile\n",
    "sample_output, full_e2e_median_runtime = full_inference_greedy(\n",
    "    model.to('cuda:0'), inputs.input_ids, TimingProfile(iterations=10, number=1, warmup=1),\n",
    "    max_length=GPT2ModelTRTConfig.MAX_SEQUENCE_LENGTH[GPT2_VARIANT]\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d662701-e430-4fdc-ad46-1f296defcf8f",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Convert to ONNX format\n",
    "\n",
    "Prior to converting the model to a TensorRT engine, we will first convert the PyTorch model to an intermediate universal format: ONNX.\n",
    "\n",
    "ONNX is an open format for machine learning and deep learning models. It allows you to convert deep learning and machine learning models from different frameworks such as TensorFlow, PyTorch, MATLAB, Caffe, and Keras to a single format.\n",
    "\n",
    "At a high level, the steps to convert a PyTorch model to TensorRT are as follows:\n",
    "- Convert the pretrained image segmentation PyTorch model into ONNX.\n",
    "- Import the ONNX model into TensorRT.\n",
    "- Apply optimizations and generate an engine.\n",
    "- Perform inference on the GPU with the TensorRT engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b2be1a-021c-4f6c-957d-2ff7d1b95976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2.export import GPT2TorchFile\n",
    "from GPT2.GPT2ModelConfig import GPT2ModelTRTConfig\n",
    "from GPT2.GPT2ModelConfig import GPT2Metadata\n",
    "from NNDF.networks import NetworkMetadata, Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7144d206-c690-4d4c-b590-3eb25e31d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata=NetworkMetadata(variant=GPT2_VARIANT, precision=Precision(fp16=True), other=GPT2Metadata(kv_cache=False))\n",
    "gpt2 = GPT2TorchFile(model.to('cpu'), metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaa89e4-e83d-4380-a6f8-932fcfeb64d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./models/$GPT2_VARIANT/ONNX\n",
    "\n",
    "onnx_path = ('./models/{}/ONNX/model.onnx'.format(GPT2_VARIANT))\n",
    "gpt2.as_onnx_model(onnx_path, force_overwrite=False)\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf007e-5508-485c-a87f-9bfe16260452",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. Convert to TensorRT engine\n",
    "\n",
    "Now we are ready to parse the ONNX model and convert it to an optimized TensorRT engine.\n",
    "\n",
    "**Note:** As TensorRT carries out many optimizations, this conversion process might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037ac958-2627-439c-9db5-27640e3f7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2.export import GPT2ONNXFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd6e3fc-6797-46b0-a211-ce42d3769105",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./models/$GPT2_VARIANT/tensorrt\n",
    "trt_path = './models/{}/tensorrt/{}.onnx.engine'.format(GPT2_VARIANT, GPT2_VARIANT)\n",
    "gpt2_engine = GPT2ONNXFile(onnx_path, metadata).as_trt_engine(trt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7f6fc-1e6a-4ddc-8e9b-543d9e8dab4d",
   "metadata": {},
   "source": [
    "### Inference with TensorRT engine\n",
    "\n",
    "Great, if you have reached this stage, it means we now have an optimized TensorRT engine for the GPT-2 model, ready for us to carry out inference. \n",
    "\n",
    "The GPT-2 model with TensorRT backend can now be employed in place of the original HuggingFace GPT-2 model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae13aa-bf6f-4eb7-a453-389865562ae4",
   "metadata": {},
   "source": [
    "#### Single batch inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343b58f1-3d9f-4844-85c9-73058bd36a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2.trt import GPT2TRTDecoder\n",
    "\n",
    "gpt2_trt = GPT2TRTDecoder(gpt2_engine, metadata, config)\n",
    "\n",
    "outputs = gpt2_trt(inputs.input_ids)\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fc60ad-73a7-46df-85d7-a292a8abbd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking TensorRT performance on single batch\n",
    "output, decoder_e2e_median_time = gpt2_inference(\n",
    "            gpt2_trt, inputs.input_ids, TimingProfile(iterations=10, number=1, warmup=1)\n",
    "        )\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22122064-5a17-4990-bd6b-073fca5a3e9b",
   "metadata": {},
   "source": [
    "#### Open-end text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848bffb8-a7a4-4fcb-91c9-f4e9f7263e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = gpt2_trt.generate(inputs.input_ids.to('cuda:0'), max_length=GPT2ModelTRTConfig.MAX_SEQUENCE_LENGTH['gpt2'])\n",
    "\n",
    "# de-tokenize model output to raw text\n",
    "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8bc4c-bf3e-4cb5-afc6-c0bd7d8655cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get complete decoder inference result and its timing profile\n",
    "sample_output, full_e2e_median_runtime = full_inference_greedy(\n",
    "    gpt2_trt, inputs.input_ids, TimingProfile(iterations=10, number=1, warmup=1),\n",
    "    max_length=GPT2ModelTRTConfig.MAX_SEQUENCE_LENGTH['gpt2']\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b68a915-2c32-49e5-b1f6-e93d7618f637",
   "metadata": {},
   "source": [
    "You can now compare the output of the original PyTorch model and the TensorRT engine. Notice the speed difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfc6c04-ca47-4fc6-9a12-ed500722bb4a",
   "metadata": {},
   "source": [
    "## Conclusion and where-to next?\n",
    "\n",
    "This notebook has walked you through the process of converting a HuggingFace PyTorch GPT-2 model to an optimized TensorRT engine for inference in 3 easy steps. The TensorRT inference engine can be conviniently used as a drop-in replacement for the orginial HuggingFace GPT-2 model while providing significant speed up. \n",
    "\n",
    "Launch [gpt2-playground.ipynb](gpt2-playground.ipynb) for a text generation playground with GPT-2.\n",
    "\n",
    "If you are interested in further details of the conversion process, check out [GPT2/trt.py](../GPT2/trt.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a101274-cde3-4c7e-affc-a7424eda7b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
