{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e6e614-e360-4292-965e-0d255027e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88dc1a-a92d-44cc-9fb7-d9e2ef20c8e2",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Accelerating HuggingFace GPT-2 Inference with TensorRT\n",
    "\n",
    "GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. The model was pretrained on the raw texts to guess the next word in sentences. As no human labeling was required, GPT-2 pretraining can use lots of publicly available data with an automatic process to generate inputs and labels from those data.\n",
    "\n",
    "This notebook shows 3 easy steps to convert a [HuggingFace PyTorch GPT-2 model](https://huggingface.co/gpt2) to a TensorRT engine for high-performance inference.\n",
    "\n",
    "1. [Download HuggingFace GPT-2 model ](#1)\n",
    "1. [Convert to ONNX format](#2)\n",
    "1. [Convert to TensorRT engine](#3)\n",
    "1. [Advanced Topic: KV Cache](#4)\n",
    "1. [Advanced Topic: Beam Search](#5)\n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "Follow the instruction at https://github.com/NVIDIA/TensorRT to build the TensorRT-OSS docker container required to run this notebook.\n",
    "\n",
    "Next, we install some extra dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79281ed9-4855-4ade-a810-a2899a5872b9",
   "metadata": {
    "custom": {
     "metadata": {
      "tags": [
       "skip-execution"
      ]
     }
    },
    "language": "python"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e57ece",
   "metadata": {},
   "source": [
    "**Note:** After this step, you should restart the Jupyter kernel for the change to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d2f1b-439e-4cd0-8286-1d63a13f2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import torch \n",
    "\n",
    "# huggingface\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2Config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4254e2-11fd-4bc7-ac0b-60b1a9e07c4e",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1. Download HuggingFace GPT-2 model \n",
    "\n",
    "First, we download the original HuggingFace PyTorch GPT-2 model from HuggingFace model hubs, together with its associated tokernizer.\n",
    "\n",
    "The GPT-2 variants supported by TensorRT 8 are: gpt2 (117M), gpt2-large (774M)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae66d58-f994-4987-8f1d-1fa8ac2ec8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download model and tokernizer\n",
    "GPT2_VARIANT = 'gpt2' # choices: gpt2 | gpt2-medium | gpt2-large | gpt2-xl\n",
    "config = GPT2Config(GPT2_VARIANT)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(GPT2_VARIANT, force_download = False)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(GPT2_VARIANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7252ca90-1104-40dc-8e72-f51c07a4cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model locally\n",
    "pytorch_model_dir = './models/{}/pytorch'.format(GPT2_VARIANT)\n",
    "!mkdir -p $pytorch_model_dir\n",
    "\n",
    "model.save_pretrained(pytorch_model_dir)\n",
    "print(\"Pytorch Model saved to {}\".format(pytorch_model_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c5766-97ed-4d04-bab5-7fa18e89dee8",
   "metadata": {},
   "source": [
    "### Inference with PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43067c2-ecd9-4bd6-9047-a3f74621931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carry out inference with a single sample\n",
    "input_str = \"Hello, my dog is \"\n",
    "inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d347ddf-4504-4ab7-b15b-29d218bdd7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d6c2ea-3450-4b8b-9cc8-09943d967ece",
   "metadata": {},
   "source": [
    "#### Single example inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b844f057-e768-467d-9185-68fb4c74b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"], use_cache = False)\n",
    "\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717b2f68-9d92-474e-9937-8b42a1c60d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c0468b-976a-4a08-98d3-e87578ec067f",
   "metadata": {},
   "source": [
    "For benchmarking purposes, we will employ a helper function `gpt2_inference` which executes the inference on a single batch repeatedly and measures end to end execution time. Let's take note of this execution time for later comparison with TensorRT. \n",
    " \n",
    "`TimingProfile` is a named tuple that specifies the number of experiments and number of times to call the function per iteration (and number of warm-up calls although it is not used here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf8f00-0562-482b-9bec-b0b7596aec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2.measurements import gpt2_inference\n",
    "from NNDF.networks import TimingProfile\n",
    "\n",
    "# Benchmarking TensorRT performance on single batch\n",
    "_, decoder_e2e_median_time = gpt2_inference(\n",
    "            model.to('cuda:0'), input_ids, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    "        )\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4805756f-81f9-43cf-88f6-b205ecd23034",
   "metadata": {},
   "source": [
    "#### Open-end text generation\n",
    "Next, we will employ the PyTorch model for the open-end text generation task, which GPT-2 is particularly good at. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb282bf-a8f4-47c4-830e-f2fb69d9d8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2.GPT2ModelConfig import GPT2ModelTRTConfig\n",
    "# MAX_LENGTH represents the maximum length that GPT2 could be used in text generation. \n",
    "# This corresponds to max_length in task_specific_params for text-generation, which = 50 for each model config.\n",
    "# If the length exceeds max_length, the output becomes meaningless for the specific task.\n",
    "max_length = GPT2ModelTRTConfig.MAX_LENGTH[GPT2_VARIANT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3d01fc-9928-486b-9d15-de84d46528e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = model.to('cuda:0').generate(input_ids, max_length=max_length, use_cache = False)\n",
    "\n",
    "# de-tokenize model output to raw text\n",
    "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b016c2f-7982-44ac-81e5-d3854391a8b6",
   "metadata": {},
   "source": [
    "For benchmarking purposes, we will employ a helper function `full_inference` which executes the inference repeatedly and measures end to end execution time. Let's take note of this execution time for later comparison with TensorRT. \n",
    "\n",
    "TimingProfile is a named tuple that specifies the number of experiments and number of times to call the function per iteration (and number of warm-up calls although it is not used here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aea249-529e-4b5e-9759-e0c8370391a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2.measurements import full_inference\n",
    "\n",
    "# get complete decoder inference result and its timing profile\n",
    "_, full_e2e_median_runtime = full_inference(\n",
    "    model.to('cuda:0'), inputs.input_ids, tokenizer, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=max_length\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d662701-e430-4fdc-ad46-1f296defcf8f",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Convert to ONNX format\n",
    "\n",
    "Prior to converting the model to a TensorRT engine, we will first convert the PyTorch model to an intermediate universal format: ONNX.\n",
    "\n",
    "ONNX is an open format for machine learning and deep learning models. It allows you to convert deep learning and machine learning models from different frameworks such as TensorFlow, PyTorch, MATLAB, Caffe, and Keras to a single format.\n",
    "\n",
    "At a high level, the steps to convert a PyTorch model to TensorRT are as follows:\n",
    "- Convert the pretrained image segmentation PyTorch model into ONNX.\n",
    "- Import the ONNX model into TensorRT.\n",
    "- Apply optimizations and generate an engine.\n",
    "- Perform inference on the GPU with the TensorRT engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b2be1a-021c-4f6c-957d-2ff7d1b95976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NNDF.networks import NetworkMetadata, Precision\n",
    "from GPT2.export import GPT2TorchFile\n",
    "from GPT2.GPT2ModelConfig import GPT2Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7144d206-c690-4d4c-b590-3eb25e31d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = NetworkMetadata(variant=GPT2_VARIANT, precision=Precision(fp16=False), other=GPT2Metadata(kv_cache=False)) # kv_cache is disabled because it exports extra input/output to the model\n",
    "gpt2 = GPT2TorchFile(model.to('cpu'), metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaa89e4-e83d-4380-a6f8-932fcfeb64d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./models/$GPT2_VARIANT/ONNX\n",
    "\n",
    "onnx_path = ('./models/{}/ONNX/{}.onnx'.format(GPT2_VARIANT, GPT2_VARIANT))\n",
    "gpt2.as_onnx_model(onnx_path, force_overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b04de1-e887-445c-9bc8-e2a7e0fca7ea",
   "metadata": {},
   "source": [
    "Let's take a look at the onnx file and investigate its input and output. You should see that \"input_ids\" as the input, and \"logits\" as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4fff25-97da-4f9f-ae98-e918745faebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c409e6-d312-4cc7-b13f-4621609d5633",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2314caaf-836d-4140-93e4-4b3f4c931347",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model.graph.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe7a8d4-2bc3-49fc-863a-0e7f4be6565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model.graph.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf007e-5508-485c-a87f-9bfe16260452",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. Convert to TensorRT engine\n",
    "\n",
    "Now we are ready to parse the ONNX model and convert it to an optimized TensorRT model.\n",
    "\n",
    "Since the model contains dynamic input shapes, we can specify a valid input range with a TensorRT optimization profile.\n",
    "\n",
    "Note: As TensorRT carries out many optimization, this conversion process for the larger model might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037ac958-2627-439c-9db5-27640e3f7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polygraphy.backend.trt import Profile\n",
    "from tensorrt import PreviewFeature\n",
    "from GPT2.export import GPT2ONNXFile, GPT2TRTEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd6e3fc-6797-46b0-a211-ce42d3769105",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./models/$GPT2_VARIANT/trt-engine\n",
    "trt_engine_folder = './models/{}/trt-engine'.format(GPT2_VARIANT)\n",
    "\n",
    "# Create optimization profile for dynamic shape input. Can modify batch_size / max_sequence_length to build engines for different shapes\n",
    "batch_size = 1\n",
    "preview_dynamic_shapes = True # review_dynamic_shapes optimize the trt engine building time\n",
    "# We can either use input length as the optimal length, or use max_length // 2. \n",
    "# In T5 or BART, input_length is better, but in GPT-2, max_length // 2 is better because we need to generate max_length number of tokens\n",
    "\n",
    "use_input_length = False\n",
    "opt_length = input_id.shape[1] if use_input_length else max_length // 2 \n",
    "# Create different engine tags for different configurations\n",
    "engine_tag = f\"bs{batch_size}\"\n",
    "preview_features = []\n",
    "if preview_dynamic_shapes:\n",
    "    preview_features = [PreviewFeature.FASTER_DYNAMIC_SHAPES_0805]\n",
    "    engine_tag += \"-previewFasterDynamicShapes\"\n",
    "\n",
    "profiles = [Profile().add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size, 1),\n",
    "    opt=(batch_size, opt_length), # Optimized based on the inputs. \n",
    "    max=(batch_size, max_length),\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5538106b-3ae4-4d5f-b0ee-1f76174dcecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5934f0-46d3-45d7-8dd5-6cf81de61e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_path = os.path.join(trt_engine_folder, f\"{GPT2_VARIANT}-{engine_tag}.engine\")\n",
    "if not os.path.exists(engine_path):\n",
    "    gpt2_engine = GPT2ONNXFile(onnx_path, metadata).as_trt_engine(output_fpath=engine_path, profiles=profiles, preview_features=preview_features)\n",
    "else:\n",
    "    gpt2_engine = GPT2TRTEngine(engine_path, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7f6fc-1e6a-4ddc-8e9b-543d9e8dab4d",
   "metadata": {},
   "source": [
    "### Inference with TensorRT engine\n",
    "\n",
    "Great, if you have reached this stage, it means we now have an optimized TensorRT engine for the GPT-2 model, ready for us to carry out inference. \n",
    "\n",
    "The GPT-2 model with TensorRT backend can now be employed in place of the original HuggingFace GPT-2 model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae13aa-bf6f-4eb7-a453-389865562ae4",
   "metadata": {},
   "source": [
    "#### Single batch inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343b58f1-3d9f-4844-85c9-73058bd36a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2.trt import GPT2TRTDecoder\n",
    "config = GPT2Config(GPT2_VARIANT, use_cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfda583-b684-48b1-9046-15ab022ef982",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_trt = GPT2TRTDecoder(gpt2_engine, metadata, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fc60ad-73a7-46df-85d7-a292a8abbd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking TensorRT performance on single batch\n",
    "_, decoder_e2e_median_time = gpt2_inference(\n",
    "            gpt2_trt, input_ids, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50)\n",
    "        )\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d86d29-1c7b-4020-9ef2-b77ea5e52764",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = gpt2_trt(input_ids=input_ids)\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e0162-c9eb-473d-ace6-c4c61ff578b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22122064-5a17-4990-bd6b-073fca5a3e9b",
   "metadata": {},
   "source": [
    "#### Open-end text generation\n",
    "Let's generate the same task again. Since GPT-2 is an open-ended model, a small turbulent in the model might have a very different result. Since we have done some format changes and input/output restriction while exporting the model, you might see a different result compared to raw HuggingFace model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848bffb8-a7a4-4fcb-91c9-f4e9f7263e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = gpt2_trt.generate(input_ids, max_length=max_length)\n",
    "\n",
    "# de-tokenize model output to raw text\n",
    "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8bc4c-bf3e-4cb5-afc6-c0bd7d8655cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get complete decoder inference result and its timing profile\n",
    "_, full_e2e_median_runtime = full_inference(\n",
    "    gpt2_trt, input_ids, tokenizer, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=max_length\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b68a915-2c32-49e5-b1f6-e93d7618f637",
   "metadata": {},
   "source": [
    "You can now compare the output of the original PyTorch model and the TensorRT engine. Notice the speed difference. On an NVIDIA V100 32GB GPU, this results in about ~5x performance improvement for the GPT-2 model (from an average of 0.704s to 0.134s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2562388-d97b-45dd-8569-3f6c053f4e98",
   "metadata": {},
   "source": [
    "Now you have known how to convert a model to onnx, build TRT engine and optimize it. As you might have recalled, using kv cache and beam search are two important ways to improve the performance of the decoder models. We have recently added thse support to our HuggingFace demo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4132e54-aba7-42ec-8324-c68d82c17296",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"4\"></a>\n",
    "\n",
    "## 4. Advanced Topic: KV Cache\n",
    "\n",
    "As you have seen above, we put `use_cache = False` in some code blocks. This is because in the simplified model, we only take `input_ids` as input and `logits` as output. `input_ids` is growing as the sequence goes longer. In reality, we sometimes cache the self-attentions for each layer and reuse them in the later computations. This allows us to only take the last generated `input_ids`. This is a trade-off between space and time. When the model is small or the sequence is small, the D2D data copy time usually outweights the performance improvement of the model. However, performance improvements have been found in larger models with larger sequence length like 512. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d1dcb-250f-4d86-9726-b114d4962fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = True\n",
    "kv_config = GPT2Config(GPT2_VARIANT, use_cache = use_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8fdf0f-2da0-46c0-a948-e4e6e16b898a",
   "metadata": {},
   "source": [
    "#### Raw HuggingFace\n",
    "\n",
    "The model that we download from `GPT2LMHeadModel.from_pretrained` is dynamic in its inputs. It can take both kv and non-kv configurations. Changing `use_cache` will do it. You can see that changing this configuration, the output is changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b3c51a-07ee-4936-b620-50766a45b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get complete decoder inference result and its timing profile\n",
    "_, full_e2e_median_runtime = full_inference(\n",
    "    model.to('cuda:0'), inputs.input_ids, tokenizer, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=max_length, use_cache = use_cache\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14607bf-f449-4151-9076-d099ae1a3ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = model.to('cuda:0').generate(input_ids, max_length=max_length, use_cache = use_cache)\n",
    "\n",
    "# de-tokenize model output to raw text\n",
    "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9057ef83-0cdc-4631-9958-66d04fc7fc22",
   "metadata": {},
   "source": [
    "#### TensorRT\n",
    "\n",
    "For the 1st decoding step, we take `input_ids` and generate both `logits` and the kv cache. In other steps, we take the new `input_ids` with `past` kv-cache and the outputs are `logits` and the updated `present` kv-cache. Taking dynamic number of inputs for trt is not currently supported in our demo, so we need to output 2 onnx files and build 2 engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fbcfad-9c9c-47e2-894a-731c7a3a04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_metadata = NetworkMetadata(variant=GPT2_VARIANT, precision=Precision(fp16=False), other=GPT2Metadata(kv_cache=use_cache))\n",
    "kv_gpt2 = GPT2TorchFile(model.to('cpu'), kv_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe680c5-d9ff-466f-87fe-a7bb0cbee944",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_onnx_path = ('./models/{}/ONNX/{}-kv_cache.onnx'.format(GPT2_VARIANT, GPT2_VARIANT))\n",
    "kv_gpt2.as_onnx_model(kv_onnx_path, force_overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd47156f-bbe5-496f-8959-fdb6bfe92525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get non_kv path. If the input is model.onnx, output is model-non-kv.onnx\n",
    "def get_non_kv_path(kv_path):\n",
    "    fpath_root, fpath_ext = os.path.splitext(kv_path)\n",
    "    return fpath_root + '-non-kv' + fpath_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d6507-3f66-43aa-8b65-6fab58ff71ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_onnx_non_kv_path = get_non_kv_path(kv_onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0f6824-286d-4afa-926b-7eed4cafafc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_onnx_model = onnx.load(kv_onnx_path)\n",
    "kv_onnx_non_kv_model = onnx.load(kv_onnx_non_kv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71f0012-7a2d-41be-a8d8-c818dcb7c244",
   "metadata": {},
   "source": [
    "We could see that the kv model has #inputs = #outputs = num_layers * 2 + 1, while the non-kv model has #inputs = 1 and #outputs = kv model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7579aeec-2c7a-43de-b8f7-beff8d3d7784",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kv_onnx_model.graph.input), len(kv_onnx_model.graph.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854e0e8d-c917-4ff6-923d-c76e2997348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kv_onnx_non_kv_model.graph.input), len(kv_onnx_non_kv_model.graph.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add1139-aab0-4531-b2ac-c3aca90e5d49",
   "metadata": {},
   "source": [
    "The next blocks will set up the profile and build the engine. The only difference is that we now have the profile for kv cache, and have 2 engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c56220-1e5d-416c-b941-d13cc704e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae055cb-41b7-4523-86bc-490bc9edf204",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "preview_dynamic_shapes = True\n",
    "\n",
    "engine_tag = \"bs{}\".format(batch_size)\n",
    "\n",
    "preview_features = []\n",
    "if preview_dynamic_shapes:\n",
    "    preview_features = [PreviewFeature.FASTER_DYNAMIC_SHAPES_0805]\n",
    "    engine_tag += \"-previewFasterDynamicShapes\"\n",
    "    \n",
    "use_input_length = False\n",
    "opt_length = input_id.shape[1] if use_input_length else max_length // 2 \n",
    "\n",
    "# Setup profiles\n",
    "kv_profiles = Profile()\n",
    "kv_profiles = kv_profiles.add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size, 1),\n",
    "    opt=(batch_size, opt_length),\n",
    "    max=(batch_size, max_length),\n",
    ")\n",
    "\n",
    "# still need non-kv engine in kv mode\n",
    "kv_profiles_non_kv = copy.deepcopy(kv_profiles)\n",
    "kv_profiles_non_kv_list = [kv_profiles_non_kv]\n",
    "\n",
    "num_heads = GPT2ModelTRTConfig.NUMBER_OF_HEADS[GPT2_VARIANT]\n",
    "embedding_size_per_head = GPT2ModelTRTConfig.EMBEDDING_SIZE[GPT2_VARIANT] // num_heads\n",
    "num_decoder_layers = GPT2ModelTRTConfig.NUMBER_OF_LAYERS[GPT2_VARIANT]\n",
    "\n",
    "for i in range(num_decoder_layers):\n",
    "    self_attention_profile = {\n",
    "        \"min\": (batch_size, num_heads, 1, embedding_size_per_head),\n",
    "        \"opt\": (batch_size, num_heads, opt_length, embedding_size_per_head),\n",
    "        \"max\": (batch_size, num_heads, max_length, embedding_size_per_head),\n",
    "    }\n",
    "    dec_profiles = kv_profiles.add(\n",
    "        f\"past_key_values.{i}.decoder.key\",\n",
    "        **self_attention_profile\n",
    "    )\n",
    "    dec_profiles = kv_profiles.add(\n",
    "        f\"past_key_values.{i}.decoder.value\",\n",
    "        **self_attention_profile\n",
    "    )\n",
    "kv_profiles_list = [kv_profiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1118131a-1aa9-4cb7-82c4-b0f1396d2343",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_profiles_non_kv_list, kv_profiles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eadf843-9f60-41c7-90a9-098b33ce3603",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_engine_path = os.path.join(trt_engine_folder, f\"{GPT2_VARIANT}-kv_cache_{engine_tag}.engine\")\n",
    "kv_engine_non_kv_path = get_non_kv_path(kv_engine_path)\n",
    "\n",
    "# Set up the trt engine with both kv input/output augmented\n",
    "if not os.path.exists(kv_engine_path):\n",
    "    kv_gpt2_engine = GPT2ONNXFile(kv_onnx_path, kv_metadata).as_trt_engine(kv_engine_path,profiles=kv_profiles_list, preview_features=preview_features)\n",
    "else:\n",
    "    kv_gpt2_engine = GPT2TRTEngine(kv_engine_path, kv_metadata)\n",
    "\n",
    "# Set up the starter engine (engine w/o kv input)\n",
    "if not os.path.exists(kv_engine_non_kv_path):\n",
    "    kv_gpt2_non_kv_engine = GPT2ONNXFile(kv_onnx_non_kv_path, kv_metadata).as_trt_engine(kv_engine_non_kv_path,profiles=kv_profiles_non_kv_list, preview_features=preview_features)\n",
    "else:\n",
    "    kv_gpt2_non_kv_engine = GPT2TRTEngine(kv_engine_non_kv_path, kv_metadata)\n",
    "    \n",
    "kv_gpt2_trt = GPT2TRTDecoder(\n",
    "    kv_gpt2_engine, kv_metadata, kv_config, batch_size=batch_size\n",
    ")\n",
    "\n",
    "kv_gpt2_trt._set_non_kv_engine_for_kv_mode(kv_gpt2_non_kv_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090007db-9a09-4b6d-95ed-8a688ea05798",
   "metadata": {},
   "source": [
    "Since we have 2 engines, benchmarking single-run runtime does not make sense. We instead use `full_inference` to measure the time for the entire inference cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b93d88-21bb-4f87-9ff6-709d0babdf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get complete decoder inference result and its timing profile\n",
    "_, full_e2e_median_runtime = full_inference(\n",
    "    kv_gpt2_trt, input_ids, tokenizer, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=max_length, use_cache = use_cache\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89ab217-9ee4-435c-b689-69d98cef1cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_gpt2_trt.reset()\n",
    "kv_sample_output = kv_gpt2_trt.generate(input_ids, max_length=max_length)\n",
    "tokenizer.decode(kv_sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b614fb8-63d6-4711-84cf-c69ca8b3f141",
   "metadata": {},
   "source": [
    "In this short example, kv cache performance does not improve the performance, and may even be slightly worse than non kv cache mode. However, when we have larger input sequences for the model, it will be better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f764049f-0578-4305-b010-4e7a3156a377",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "\n",
    "## 5. Advanced Topic: Beam Search\n",
    "\n",
    "Beam search is a way to increase the model quality. It looks for the top `num_beams` number of possible words and pick the one that conditions the best to the current position. Similarly, the original HuggingFace PyTorch model supports beam search natively, while we need to build separate trt engine for different `num_beams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5808db-2cc0-4d88-aebe-1b6e17a023e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_config = GPT2Config(GPT2_VARIANT, use_cache = False)\n",
    "beam_metadata = NetworkMetadata(variant=GPT2_VARIANT, precision=Precision(fp16=False), other=GPT2Metadata(kv_cache=False))\n",
    "num_beams = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1403609-b24d-4e10-a8eb-852d3eab6fa0",
   "metadata": {},
   "source": [
    "#### HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd992c8-1eeb-427c-ae32-2c63766c6a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get complete decoder inference result and its timing profile\n",
    "_, full_e2e_median_runtime = full_inference(\n",
    "    model.to('cuda:0'), input_ids, tokenizer, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=max_length, num_beams = num_beams\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09418760-84bd-4308-b06b-8540945a6dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = model.to('cuda:0').generate(input_ids, max_length=max_length, num_beams = num_beams)\n",
    "\n",
    "# de-tokenize model output to raw text\n",
    "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b8d9fa-d74a-40dd-94ce-d98551d24608",
   "metadata": {},
   "source": [
    "You could see that the output is very different from the original one. If you change `num_beams`, the result will also change significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba01e0ec-68ad-4682-8ca4-2ecde7d70f7f",
   "metadata": {},
   "source": [
    "#### TensorRT\n",
    "It uses the same onnx file as the original configuration, but the engine set up is differently, because it expands the inputs by `num_beams` for the first dimension of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055fb314-8e0f-4edd-bf78-16890d196de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimization profile for dynamic shape input. Can modify batch_size / max_sequence_length to build engines for different shapes\n",
    "batch_size = 1\n",
    "preview_dynamic_shapes = True # review_dynamic_shapes optimize the trt engine building time\n",
    "# We can either use input length as the optimal length, or use max_length // 2. \n",
    "# In T5 or BART, input_length is better, but in GPT-2, max_length // 2 is better because we need to generate max_length number of tokens\n",
    "\n",
    "use_input_length = False\n",
    "opt_length = input_id.shape[1] if use_input_length else max_length // 2 \n",
    "# Create different engine tags for different configurations\n",
    "engine_tag = f\"bs{batch_size}-beam{num_beams}\"\n",
    "\n",
    "preview_features = []\n",
    "if preview_dynamic_shapes:\n",
    "    preview_features = [PreviewFeature.FASTER_DYNAMIC_SHAPES_0805]\n",
    "    engine_tag += \"-previewFasterDynamicShapes\"\n",
    "\n",
    "beam_profiles = [Profile().add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size * num_beams, 1),\n",
    "    opt=(batch_size * num_beams, opt_length), # Optimized based on the inputs. \n",
    "    max=(batch_size * num_beams, max_length),\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18986d0f-9509-463f-a489-a76dd4d28a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd04a7b-8aa6-4c97-8d85-96f14b06abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_engine_path = os.path.join(trt_engine_folder, f\"{GPT2_VARIANT}-{engine_tag}.engine\")\n",
    "if not os.path.exists(beam_engine_path):\n",
    "    beam_gpt2_engine = GPT2ONNXFile(onnx_path, beam_metadata).as_trt_engine(output_fpath=beam_engine_path, profiles=beam_profiles, preview_features=preview_features)\n",
    "else:\n",
    "    beam_gpt2_engine = GPT2TRTEngine(beam_engine_path, beam_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fe1dba-4e84-478e-9ea7-07c21856e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_gpt2_trt = GPT2TRTDecoder(beam_gpt2_engine, beam_metadata, beam_config, num_beams = num_beams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42614e14-c962-4c31-a469-7e0343efbdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get complete decoder inference result and its timing profile\n",
    "_, full_e2e_median_runtime = full_inference(\n",
    "    beam_gpt2_trt, input_ids, tokenizer, TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=50),\n",
    "    max_length=max_length, num_beams = num_beams\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ab05a-fe0d-42c3-9591-605ddab389ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_sample_output = beam_gpt2_trt.generate(input_ids, max_length=max_length, num_beams = num_beams)\n",
    "tokenizer.decode(beam_sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9543dbfd-4650-46f5-8f77-587dcb05785a",
   "metadata": {},
   "source": [
    "We could see that because of larger batch size, beam search will take slightly longer, but for most sequences, it will generate more meaningful outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfc6c04-ca47-4fc6-9a12-ed500722bb4a",
   "metadata": {},
   "source": [
    "## Conclusion and where-to next?\n",
    "\n",
    "This notebook has walked you through the process of converting a HuggingFace PyTorch GPT-2 model to an optimized TensorRT engine for inference in 3 easy steps. The TensorRT inference engine can be conviniently used as a drop-in replacement for the orginial HuggingFace GPT-2 model while providing significant speed up. \n",
    "\n",
    "If you are interested in further details of the conversion process, check out [GPT2/trt.py](../GPT2/trt.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14079b8f-738e-4137-9ca3-6a4254e8f006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
