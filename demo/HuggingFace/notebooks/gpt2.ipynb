{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e6e614-e360-4292-965e-0d255027e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88dc1a-a92d-44cc-9fb7-d9e2ef20c8e2",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Accelerating HuggingFace GPT-2 Inference with TensorRT\n",
    "\n",
    "GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. The model was pretrained on the raw texts to guess the next word in sentences. As no human labeling was required, GPT-2 pretraining can use lots of publicly available data with an automatic process to generate inputs and labels from those data.\n",
    "\n",
    "This notebook shows 3 easy steps to convert a [HuggingFace PyTorch GPT-2 model](https://huggingface.co/gpt2) to a TensorRT engine for high-performance inference.\n",
    "\n",
    "1. [Download HuggingFace GPT-2 model ](#1)\n",
    "1. [Convert to ONNX format](#2)\n",
    "1. [Convert to TensorRT engine](#3)\n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "Follow the instruction at https://github.com/NVIDIA/TensorRT to build the TensorRT-OSS docker container required to run this notebook.\n",
    "\n",
    "Next, we install some extra dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79281ed9-4855-4ade-a810-a2899a5872b9",
   "metadata": {
    "custom": {
     "metadata": {
      "tags": [
       "skip-execution"
      ]
     }
    },
    "language": "python"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e57ece",
   "metadata": {},
   "source": [
    "**Note:** After this step, you should restart the Jupyter kernel for the change to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d2f1b-439e-4cd0-8286-1d63a13f2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import torch \n",
    "\n",
    "# huggingface\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2Config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4254e2-11fd-4bc7-ac0b-60b1a9e07c4e",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1. Download HuggingFace GPT-2 model \n",
    "\n",
    "First, we download the original HuggingFace PyTorch GPT-2 model from HuggingFace model hubs, together with its associated tokernizer.\n",
    "\n",
    "The GPT-2 variants supported by TensorRT 8 are: gpt2 (117M), gpt2-large (774M)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae66d58-f994-4987-8f1d-1fa8ac2ec8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download model and tokernizer\n",
    "GPT2_VARIANT = 'gpt2' # choices: gpt2 | gpt2-large\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(GPT2_VARIANT)\n",
    "\n",
    "config = GPT2Config(GPT2_VARIANT)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(GPT2_VARIANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7252ca90-1104-40dc-8e72-f51c07a4cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model locally\n",
    "pytorch_model_dir = './models/{}/pytorch'.format(GPT2_VARIANT)\n",
    "!mkdir -p $pytorch_model_dir\n",
    "\n",
    "model.save_pretrained(pytorch_model_dir)\n",
    "print(\"Pytorch Model saved to {}\".format(pytorch_model_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c5766-97ed-4d04-bab5-7fa18e89dee8",
   "metadata": {},
   "source": [
    "### Inference with PyTorch model\n",
    "\n",
    "#### Single example inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c5fe7-7733-49b5-89c5-c8278ff54fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carry out inference with a single sample\n",
    "inputs = tokenizer(\"Hello, my dog is \", return_tensors=\"pt\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c0468b-976a-4a08-98d3-e87578ec067f",
   "metadata": {},
   "source": [
    "For benchmarking purposes, we will employ a helper function `gpt2_inference` which executes the inference on a single batch repeatedly and measures end to end execution time. Let's take note of this execution time for later comparison with TensorRT. \n",
    " \n",
    "`TimingProfile` is a named tuple that specifies the number of experiments and number of times to call the function per iteration (and number of warm-up calls although it is not used here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf8f00-0562-482b-9bec-b0b7596aec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2.measurements import gpt2_inference\n",
    "from NNDF.networks import TimingProfile\n",
    "\n",
    "# Benchmarking TensorRT performance on single batch\n",
    "output, decoder_e2e_median_time = gpt2_inference(\n",
    "            model.to('cuda:0'), inputs.input_ids.to('cuda:0'), TimingProfile(iterations=10, number=1, warmup=1, duration=0)\n",
    "        )\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4805756f-81f9-43cf-88f6-b205ecd23034",
   "metadata": {},
   "source": [
    "#### Open-end text generation\n",
    "Next, we will employ the PyTorch model for the open-end text generation task, which GPT-2 is particularly good at. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3d01fc-9928-486b-9d15-de84d46528e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2.GPT2ModelConfig import GPT2ModelTRTConfig\n",
    "\n",
    "sample_output = model.to('cuda:0').generate(inputs.input_ids.to('cuda:0'), max_length=GPT2ModelTRTConfig.MAX_SEQUENCE_LENGTH['gpt2'])\n",
    "\n",
    "# de-tokenize model output to raw text\n",
    "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b016c2f-7982-44ac-81e5-d3854391a8b6",
   "metadata": {},
   "source": [
    "For benchmarking purposes, we will employ a helper function `full_inference_greedy` which executes the inference repeatedly and measures end to end execution time. Let's take note of this execution time for later comparison with TensorRT. \n",
    " \n",
    "TimingProfile is a named tuple that specifies the number of experiments and number of times to call the function per iteration (and number of warm-up calls although it is not used here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aea249-529e-4b5e-9759-e0c8370391a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2.measurements import full_inference_greedy\n",
    "\n",
    "# get complete decoder inference result and its timing profile\n",
    "sample_output, full_e2e_median_runtime = full_inference_greedy(\n",
    "    model.to('cuda:0'), inputs.input_ids, TimingProfile(iterations=10, number=1, warmup=1, duration=0),\n",
    "    max_length=GPT2ModelTRTConfig.MAX_SEQUENCE_LENGTH[GPT2_VARIANT]\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d662701-e430-4fdc-ad46-1f296defcf8f",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Convert to ONNX format\n",
    "\n",
    "Prior to converting the model to a TensorRT engine, we will first convert the PyTorch model to an intermediate universal format: ONNX.\n",
    "\n",
    "ONNX is an open format for machine learning and deep learning models. It allows you to convert deep learning and machine learning models from different frameworks such as TensorFlow, PyTorch, MATLAB, Caffe, and Keras to a single format.\n",
    "\n",
    "At a high level, the steps to convert a PyTorch model to TensorRT are as follows:\n",
    "- Convert the pretrained image segmentation PyTorch model into ONNX.\n",
    "- Import the ONNX model into TensorRT.\n",
    "- Apply optimizations and generate an engine.\n",
    "- Perform inference on the GPU with the TensorRT engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b2be1a-021c-4f6c-957d-2ff7d1b95976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2.export import GPT2TorchFile\n",
    "from GPT2.GPT2ModelConfig import GPT2ModelTRTConfig, GPT2Metadata\n",
    "from NNDF.networks import NetworkMetadata, Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7144d206-c690-4d4c-b590-3eb25e31d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = NetworkMetadata(variant=GPT2_VARIANT, precision=Precision(fp16=False), other=GPT2Metadata(kv_cache=False))\n",
    "gpt2 = GPT2TorchFile(model.to('cpu'), metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaa89e4-e83d-4380-a6f8-932fcfeb64d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./models/$GPT2_VARIANT/ONNX\n",
    "\n",
    "onnx_path = ('./models/{}/ONNX/model.onnx'.format(GPT2_VARIANT))\n",
    "gpt2.as_onnx_model(onnx_path, force_overwrite=False)\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf007e-5508-485c-a87f-9bfe16260452",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. Convert to TensorRT engine\n",
    "\n",
    "Now we are ready to parse the ONNX model and convert it to an optimized TensorRT model.\n",
    "\n",
    "Since the model contains dynamic input shapes, we can specify a valid input range with a TensorRT optimization profile.\n",
    "\n",
    "Note: As TensorRT carries out many optimization, this conversion process for the larger model might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037ac958-2627-439c-9db5-27640e3f7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2.export import GPT2ONNXFile\n",
    "from polygraphy.backend.trt import Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd6e3fc-6797-46b0-a211-ce42d3769105",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./models/$GPT2_VARIANT/trt-engine\n",
    "trt_path = './models/{}/trt-engine/{}.onnx.engine'.format(GPT2_VARIANT, GPT2_VARIANT)\n",
    "\n",
    "# Create optimization profile for dynamic shape input. Can modify batch_size / max_sequence_length to build engines for different shapes\n",
    "batch_size = 1\n",
    "max_sequence_length = GPT2ModelTRTConfig.MAX_SEQUENCE_LENGTH[metadata.variant]\n",
    "profiles = [Profile().add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size, 1),\n",
    "    opt=(batch_size, max_sequence_length // 2),\n",
    "    max=(batch_size, max_sequence_length),\n",
    ")]\n",
    "\n",
    "gpt2_engine = GPT2ONNXFile(onnx_path, metadata).as_trt_engine(output_fpath=trt_path, profiles=profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b9a50-320d-4ea2-a230-2f7f9ad66356",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_engine.fpath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7f6fc-1e6a-4ddc-8e9b-543d9e8dab4d",
   "metadata": {},
   "source": [
    "### Inference with TensorRT engine\n",
    "\n",
    "Great, if you have reached this stage, it means we now have an optimized TensorRT engine for the GPT-2 model, ready for us to carry out inference. \n",
    "\n",
    "The GPT-2 model with TensorRT backend can now be employed in place of the original HuggingFace GPT-2 model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae13aa-bf6f-4eb7-a453-389865562ae4",
   "metadata": {},
   "source": [
    "#### Single batch inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343b58f1-3d9f-4844-85c9-73058bd36a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2.trt import GPT2TRTDecoder\n",
    "\n",
    "gpt2_trt = GPT2TRTDecoder(gpt2_engine, metadata, config)\n",
    "\n",
    "outputs = gpt2_trt(inputs.input_ids)\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fc60ad-73a7-46df-85d7-a292a8abbd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking TensorRT performance on single batch\n",
    "output, decoder_e2e_median_time = gpt2_inference(\n",
    "            gpt2_trt, inputs.input_ids, TimingProfile(iterations=10, number=1, warmup=1, duration=0)\n",
    "        )\n",
    "decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22122064-5a17-4990-bd6b-073fca5a3e9b",
   "metadata": {},
   "source": [
    "#### Open-end text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848bffb8-a7a4-4fcb-91c9-f4e9f7263e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = gpt2_trt.generate(inputs.input_ids.to('cuda:0'), max_length=GPT2ModelTRTConfig.MAX_SEQUENCE_LENGTH['gpt2'])\n",
    "\n",
    "# de-tokenize model output to raw text\n",
    "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8bc4c-bf3e-4cb5-afc6-c0bd7d8655cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get complete decoder inference result and its timing profile\n",
    "sample_output, full_e2e_median_runtime = full_inference_greedy(\n",
    "    gpt2_trt, inputs.input_ids, TimingProfile(iterations=10, number=1, warmup=1, duration=0),\n",
    "    max_length=GPT2ModelTRTConfig.MAX_SEQUENCE_LENGTH['gpt2']\n",
    ")\n",
    "full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b68a915-2c32-49e5-b1f6-e93d7618f637",
   "metadata": {},
   "source": [
    "You can now compare the output of the original PyTorch model and the TensorRT engine. Notice the speed difference. On an NVIDIA V100 32GB GPU, this results in about ~5x performance improvement for the GPT-2 small model (from an average of 0.704s to 0.134s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfc6c04-ca47-4fc6-9a12-ed500722bb4a",
   "metadata": {},
   "source": [
    "## Conclusion and where-to next?\n",
    "\n",
    "This notebook has walked you through the process of converting a HuggingFace PyTorch GPT-2 model to an optimized TensorRT engine for inference in 3 easy steps. The TensorRT inference engine can be conviniently used as a drop-in replacement for the orginial HuggingFace GPT-2 model while providing significant speed up. \n",
    "\n",
    "If you are interested in further details of the conversion process, check out [GPT2/trt.py](../GPT2/trt.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14079b8f-738e-4137-9ca3-6a4254e8f006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
