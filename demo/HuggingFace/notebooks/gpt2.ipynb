{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e6e614-e360-4292-965e-0d255027e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88dc1a-a92d-44cc-9fb7-d9e2ef20c8e2",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Accelerating HuggingFace GPT-2 Inference with TensorRT\n",
    "\n",
    "GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. The model was pretrained on the raw texts to guess the next word in sentences. As no human labeling was required, GPT-2 pretraining can use lots of publicly available data with an automatic process to generate inputs and labels from those data.\n",
    "\n",
    "This notebook shows how to convert a [HuggingFace PyTorch GPT-2 model](https://huggingface.co/gpt2) to a TensorRT engine for high-performance inference in a few lines of code.\n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "Follow the instruction at https://github.com/NVIDIA/TensorRT to build the TensorRT-OSS docker container required to run this notebook.\n",
    "\n",
    "Next, we install some extra dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c36ecb7-c622-4d95-a851-b9a6eb18e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bbdafb",
   "metadata": {},
   "source": [
    "**Note:** After this step, you should restart the Jupyter kernel for the change to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d2f1b-439e-4cd0-8286-1d63a13f2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import torch\n",
    "\n",
    "from GPT2.frameworks import GPT2HF\n",
    "from GPT2.trt import GPT2TRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c1e4cd",
   "metadata": {},
   "source": [
    "\n",
    "## API usage\n",
    "\n",
    "We have wrapped the process of importing models from PyTorch, exporting to onnx files and build TRT engines into a single class. We introduce new `GPT2HF` and `GPT2TRT` classes that both expose `generate` as the main entry point to run GPT2. `GPT2TRT` will automatically do all the 3 steps per user inputs. Here is an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e974ef4",
   "metadata": {},
   "source": [
    "\n",
    "### Specify model arguments\n",
    "\n",
    "You pick your favorite model and configurations, and TRT will run it for you! The main choice that you need to make is:\n",
    "- `use_cache`: kv cache to speed decoding\n",
    "- `num_beams`: beam search for better results\n",
    "- `fp16`: Using float16 to speed decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc41e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"variant\": \"gpt2\", # A HuggingFace model variant name. Required.\n",
    "    \"use_cache\": False, # We support decoder kv cache in generation. Default: False\n",
    "    \"fp16\": True, # Default: False\n",
    "    \"num_beams\": 1, # We support beam search in generation. Default: 1\n",
    "    \"batch_size\": 1, # Default: 1\n",
    "    # Folder name. Required. All the PyTorch, ONNX and TRT Engines will be stored in the folder.\n",
    "    \"working_dir\": \"models\",\n",
    "    # Log level.\n",
    "    \"info\": True,\n",
    "    # Benchmarking args\n",
    "    \"iterations\": 10,\n",
    "    \"number\": 1,\n",
    "    \"warmup\": 3,\n",
    "    \"duration\": 0,\n",
    "    \"percentile\": 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2f3b18",
   "metadata": {},
   "source": [
    "\n",
    "### Initialize the models\n",
    "Calling the API is just this easy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce5f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_model = GPT2HF(**args)\n",
    "trt_model = GPT2TRT(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1b0d9b",
   "metadata": {},
   "source": [
    "### Try your sentence!\n",
    "Both `GPT2HF` and `GPT2TRT` exposes `setup_tokenizer_and_model` and `generate`. If `setup_tokenizer_and_model` is not called prior to `generate`, it will be called first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701712bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"TensorRT is a deep learning accelerator software developed by NVIDIA. It can run \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb76961",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_model.models = framework_model.setup_tokenizer_and_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d66302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_model.generate(input_str = input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1065b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_model.models = trt_model.setup_tokenizer_and_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dbf2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_model.generate(input_str = input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2c055d",
   "metadata": {},
   "source": [
    "\n",
    "### Performance benchmark\n",
    "You can see that TRT and PyTorch generates reasonable results, which is expected. To measure their performance, both `GPT2HF` and `GPT2TRT` exposes `execute_inference`, `full_inference`, `encoder_inference` and `decoder_inference` to measure the inference time. Let's take a look at how our latest TRT performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054bfac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "data = [\n",
    "    ['full p50(s)', 'decoder p50(s)'],\n",
    "]\n",
    "\n",
    "def format_result(result):\n",
    "    entry = []\n",
    "    for segment in result.median_runtime:\n",
    "        entry.append('{:.4f}'.format(segment.runtime))\n",
    "    \n",
    "    return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_result = framework_model.execute_inference(input_str)\n",
    "data.append(format_result(framework_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a82bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_result = trt_model.execute_inference(input_str)\n",
    "data.append(format_result(trt_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5e5d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tabulate(data, headers='firstrow', tablefmt='github'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fbb1d4",
   "metadata": {},
   "source": [
    "Did TensorRT's performance amaze you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4254e2-11fd-4bc7-ac0b-60b1a9e07c4e",
   "metadata": {},
   "source": [
    "## Conclusion and where-to next?\n",
    "\n",
    "Is this the end? The API sounds too simple. I am used to the previous version that walks me step by step, and/or I want to know more on the process of conversion. Just follow the directory and you will find that PyTorch model, ONNX files and TRT engines are there. Feel free to investigate them. We have wrapped the entire model conversion process in `setup_tokenizer_and_model`. The TensorRT inference engine can be conviniently used as a drop-in replacement for the orginial HuggingFace GPT2 model while providing significant speed up. If you are interested in further details of the conversion process, check out [GPT2](../GPT2) and [Seq2Seq/trt.py](../Seq2Seq/trt.py). You will find that all the Seq2Seq models could be treated in a similar way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a8b7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
