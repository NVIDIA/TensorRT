{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e6e614-e360-4292-965e-0d255027e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-FileCopyrightText: Copyright (c) 1993-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88dc1a-a92d-44cc-9fb7-d9e2ef20c8e2",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Accelerating HuggingFace BART Inference with TensorRT\n",
    "\n",
    "BART is an encoder-decoder model that converts all NLP problems into a text-to-text format. More specifically, it does so by encoding different tasks as text directives in the input stream. This enables a single model to be trained supervised on a wide variety of NLP tasks such as translation, classification, Q&A and summarization.\n",
    "\n",
    "This notebook shows easy steps to convert a [HuggingFace PyTorch BART model](https://huggingface.co/docs/transformers/model_doc/bart) to a TensorRT engine for high-performance inference, with performance comparison between PyTorch and TensorRT inference.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Follow the instructions at https://github.com/NVIDIA/TensorRT to build the TensorRT-OSS docker container required to run this notebook.\n",
    "\n",
    "Next, we install some extra dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c36ecb7-c622-4d95-a851-b9a6eb18e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "!pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bbdafb",
   "metadata": {},
   "source": [
    "**Note:** After this step, you should restart the Jupyter kernel for the change to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d2f1b-439e-4cd0-8286-1d63a13f2cf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "# disable warning in notebook\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# notebook widgets\n",
    "import ipywidgets as widgets\n",
    "widget_style = {'description_width': 'initial'}\n",
    "widget_layout = widgets.Layout(width='auto')\n",
    "\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from BART.frameworks import BARTHF\n",
    "from BART.trt import BARTTRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c1e4cd",
   "metadata": {},
   "source": [
    "\n",
    "## API usage\n",
    "\n",
    "We have wrapped the process of importing models from PyTorch, exporting to onnx files and build TRT engines into a single class. We introduce new `BARTHF` and `BARTTRT` classes that both expose `generate` as the main entry point to run BART. `BARTTRT` will automatically do all the 3 steps per user inputs. Here is an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e974ef4",
   "metadata": {},
   "source": [
    "\n",
    "### Specify model arguments\n",
    "\n",
    "You pick your favorite model and configurations, and TRT will run it for you! The main choice that you need to make is:\n",
    "- `variant`: You need to state which model variant you want to run\n",
    "- `use_cache`: kv cache to speed decoding\n",
    "- `num_beams`: beam search for better results\n",
    "- `fp16`: Using float16 to speed decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4254e2-11fd-4bc7-ac0b-60b1a9e07c4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "The BART variants that are suported by TensorRT are: facebook/bart-base (139M), facebook/bart-large (406M), facebook/bart-large-cnn (406M), facebook/mbart-large-50 (680M), and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14eabc-d863-454d-9078-849acc857bb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model and Inference Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077dd494-e8d8-42f9-bdbd-0362f1213118",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"variant\": \"facebook/bart-base\", # A HuggingFace model variant name. Required.\n",
    "    \"fp16\": True, # Default: False\n",
    "    \"use_cache\": True, # We support decoder kv cache in generation. Default: False\n",
    "    \"num_beams\": 1, # We support beam search in generation. Default: 1\n",
    "    \"batch_size\": 1, # Default: 1\n",
    "    # Folder name. Required. All the PyTorch, ONNX and TRT Engines will be stored in the folder.\n",
    "    \"working_dir\": \"models\",\n",
    "    # Log level.\n",
    "    \"info\": True,\n",
    "    # Benchmarking args\n",
    "    \"iterations\": 10,\n",
    "    \"number\": 1,\n",
    "    \"warmup\": 3,\n",
    "    \"duration\": 0,\n",
    "    \"percentile\": 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2f3b18",
   "metadata": {},
   "source": [
    "\n",
    "### Initialize the models\n",
    "Calling the API is just this easy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00e92e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_model = BARTHF(**args)\n",
    "trt_model = BARTTRT(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1b0d9b",
   "metadata": {},
   "source": [
    "### Try your sentence!\n",
    "Both `BARTHF` and `BARTTRT` exposes `setup_tokenizer_and_model` and `generate`. If `setup_tokenizer_and_model` is not called prior to `generate`, it will be called first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701712bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"NVIDIA TensorRT-based applications perform up to 36X faster than CPU-only platforms during inference, enabling developers to optimize neural network models trained on all major frameworks, calibrate for lower precision with high accuracy, and deploy to hyperscale data centers, embedded platforms, or automotive product platforms. TensorRT, built on the NVIDIA CUDA parallel programming model, enables developers to optimize inference by leveraging libraries, development tools, and technologies in CUDA-X for AI, autonomous machines, high performance computing, and graphics. With new NVIDIA Ampere Architecture GPUs, TensorRT also uses sparse tensor cores for an additional performance boost.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb76961",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_model.models = framework_model.setup_tokenizer_and_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d66302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_model.generate(input=input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1065b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_model.models = trt_model.setup_tokenizer_and_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dbf2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_model.generate(input=input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2c055d",
   "metadata": {},
   "source": [
    "\n",
    "### Performance benchmark\n",
    "You can see that TRT and PyTorch generates the same result, which is expected. To measure their performance, both `T5HF` and `T5TRT` exposes `execute_inference`, `full_inference`, `encoder_inference` and `decoder_inference` to measure the inference time. Let's take a look at how our latest TRT performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054bfac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "data = [\n",
    "    ['full p50(s)', 'decoder p50(s)', 'encoder p50(s)'],\n",
    "]\n",
    "\n",
    "def format_result(result):\n",
    "    entry = []\n",
    "    for segment in result.runtime:\n",
    "        entry.append('{:.4f}'.format(segment.runtime))\n",
    "    \n",
    "    return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_result= framework_model.execute_inference(input=input_str)\n",
    "data.append(format_result(framework_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a82bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_result = trt_model.execute_inference(input_str)\n",
    "data.append(format_result(trt_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e01796",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tabulate(data, headers='firstrow', tablefmt='github'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92031643-8ee8-4d50-864b-a08e4d551dc6",
   "metadata": {},
   "source": [
    "We can now compare the HuggingFace model and the TensorRT engine, from both separate encoder/decoder and end-to-end speed difference. For bart-base variant on an NVIDIA Titan V GPU and input/output sequence length around 130, this results in about 4x performance improvement with FP16 inference + kv cache."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a498672-ba25-42b0-b89e-79e0b869943a",
   "metadata": {},
   "source": [
    "### Variable Input/Output Length Performance Benchmarking\n",
    "\n",
    "We can run more tests by varying input/output length, while using the same engines.\n",
    "\n",
    "Note that TensorRT performance depends on optimal selection of the kernels in the engine. The variable length test here uses the same engine built with max input/output length profile = `max_length` in HuggingFace config to represent the best use of the model. If you want to change the length, please change this field prior to calling `set_tokenizer_and_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e335010-ff7f-4822-85ae-bca8d235de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "input_output_len_list = [\n",
    "    (64, 128), # generation task\n",
    "    (128, 64), # summarization task\n",
    "    (32, 32), # translation task\n",
    "    (128, 128),\n",
    "]\n",
    "\n",
    "data = [\n",
    "    ['(input_len, output_len)', 'HF p50 (s)', 'TRT p50 (s)'],\n",
    "]\n",
    "\n",
    "for (in_len, out_len) in input_output_len_list:\n",
    "\n",
    "    input_ids = torch.randint(0, framework_model.config.vocab_size, (framework_model.config.batch_size, in_len))\n",
    "    \n",
    "    framework_model.config.min_output_length = out_len\n",
    "    framework_model.config.max_output_length = out_len\n",
    "    trt_model.config.min_output_length = out_len\n",
    "    trt_model.config.max_output_length = out_len\n",
    "    \n",
    "    _, framework_e2e = framework_model.full_inference(input_ids)\n",
    "    _, trt_e2e = trt_model.full_inference(input_ids)\n",
    "\n",
    "    data.append([(in_len, out_len), framework_e2e, trt_e2e])\n",
    "\n",
    "print(tabulate(data, headers='firstrow', tablefmt='github'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a598a0ae-2e21-4898-ae56-8429a5d00760",
   "metadata": {},
   "source": [
    "Does TRT performance amaze you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f5dca-397c-4c8c-9200-61b30cdba824",
   "metadata": {},
   "source": [
    "## Conclusion and where-to next?\n",
    "\n",
    "Is this the end? The API sounds too simple. I am used to the previous version that walks me step by step, and/or I want to know more on the process of conversion. Just follow the directory and you will find that PyTorch model, ONNX files and TRT engines are there. Feel free to investigate them. We have wrapped the entire model conversion process in `setup_tokenizer_and_model`. The TensorRT inference engine can be conviniently used as a drop-in replacement for the orginial HuggingFace BART model while providing significant speed up. If you are interested in further details of the conversion process, check out [BART](../BART) and [Seq2Seq/trt.py](../Seq2Seq/trt.py). You will find that all the Seq2Seq models could be treated in a similar way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff3f96a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
