{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e6e614-e360-4292-965e-0d255027e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-FileCopyrightText: Copyright (c) 1993-2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88dc1a-a92d-44cc-9fb7-d9e2ef20c8e2",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Accelerating HuggingFace BART Inference with TensorRT\n",
    "\n",
    "BART is an encoder-decoder model that converts all NLP problems into a text-to-text format. More specifically, it does so by encoding different tasks as text directives in the input stream. This enables a single model to be trained supervised on a wide variety of NLP tasks such as translation, classification, Q&A and summarization.\n",
    "\n",
    "This notebook shows easy steps to convert a [HuggingFace PyTorch BART model](https://huggingface.co/docs/transformers/model_doc/bart) to a TensorRT engine for high-performance inference, with performance comparison between PyTorch and TensorRT inference.\n",
    "\n",
    "1. [Download HuggingFace BART model](#1)\n",
    "1. [PyTorch HuggingFace Inference](#2)\n",
    "1. [TensorRT Engine Building](#3)\n",
    "1. [TensorRT Inference](#4)\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Follow the instructions at https://github.com/NVIDIA/TensorRT to build the TensorRT-OSS docker container required to run this notebook.\n",
    "\n",
    "Next, we install some extra dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c36ecb7-c622-4d95-a851-b9a6eb18e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "!pip3 install -r ../requirements.txt\n",
    "!pip3 install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bbdafb",
   "metadata": {},
   "source": [
    "**Note:** After this step, you should restart the Jupyter kernel for the change to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d2f1b-439e-4cd0-8286-1d63a13f2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "# disable warning in notebook\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# notebook widgets\n",
    "import ipywidgets as widgets\n",
    "widget_style = {'description_width': 'initial'}\n",
    "widget_layout = widgets.Layout(width='auto')\n",
    "\n",
    "import torch\n",
    "import tensorrt as trt\n",
    "from tensorrt import PreviewFeature\n",
    "from polygraphy.backend.trt import Profile\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# huggingface\n",
    "from transformers import (\n",
    "    AutoModelForPreTraining,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    ")\n",
    "\n",
    "# BART\n",
    "from BART.BARTModelConfig import BARTModelTRTConfig, BARTMetadata\n",
    "from BART.measurements import encoder_inference, decoder_inference, full_inference_greedy, full_inference_beam\n",
    "from BART.export import BARTEncoderTorchFile, BARTDecoderTorchFile, BARTEncoderONNXFile, BARTDecoderONNXFile, BARTEncoderTRTEngine, BARTDecoderTRTEngine\n",
    "from BART.trt import BARTTRTEncoder, BARTTRTDecoder\n",
    "\n",
    "# NNDF\n",
    "from NNDF.networks import NetworkMetadata, Precision\n",
    "from NNDF.networks import TimingProfile\n",
    "from NNDF.general_utils import measure_python_inference_code\n",
    "from NNDF.torch_utils import expand_inputs_for_beam_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4254e2-11fd-4bc7-ac0b-60b1a9e07c4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1. Download HuggingFace BART model\n",
    "\n",
    "First, we download the original HuggingFace PyTorch BART model from HuggingFace model hubs, together with its associated tokernizer.\n",
    "\n",
    "The BART variants that are suported by TensorRT are: facebook/bart-base (139M), facebook/bart-large (406M), facebook/bart-large-cnn (406M), facebook/mbart-large-50 (680M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14eabc-d863-454d-9078-849acc857bb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model and Inference Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774c89f3-7dbb-423d-88b2-1de693324389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI\n",
    "model_widget = widgets.Select(\n",
    "    options=['facebook/bart-base', 'facebook/bart-large', 'facebook/bart-large-cnn', 'facebook/mbart-large-50'],\n",
    "    value='facebook/bart-base',\n",
    "    description='Model variant:',\n",
    "    disabled=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "display(model_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed04130e-7f20-4a3e-bf76-52aa335f402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BART_VARIANT = model_widget.value\n",
    "\n",
    "preview_dynamic_feature_widget = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Preview 8.5 EA dynamic shapes feature',\n",
    "    disabled=False,\n",
    "    indent=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "FP16_widget = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='FP16',\n",
    "    disabled=False,\n",
    "    indent=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "HF_KV_widget = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='HuggingFace KV cache',\n",
    "    disabled=False,\n",
    "    indent=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "TRT_KV_widget = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='TensorRT KV cache (disabled due to performance improvements in progress, not beating non-KV version yet)', #  \n",
    "    disabled=True,\n",
    "    indent=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "KV_widgets = widgets.HBox([HF_KV_widget,TRT_KV_widget])\n",
    "\n",
    "batch_size_widget = widgets.BoundedIntText(\n",
    "    value=1,\n",
    "    min=1,\n",
    "    max=100000,\n",
    "    step=1,\n",
    "    description='Batch size',\n",
    "    disabled=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "max_input_len_widget = widgets.BoundedIntText(\n",
    "    value=BARTModelTRTConfig.MAX_SEQUENCE_LENGTH[BART_VARIANT],\n",
    "    min=1,\n",
    "    max=100000,\n",
    "    step=1,\n",
    "    description='Max input length',\n",
    "    disabled=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "min_output_len_widget = widgets.BoundedIntText(\n",
    "    value=BARTModelTRTConfig.MIN_OUTPUT_LENGTH[BART_VARIANT],\n",
    "    min=0,\n",
    "    max=100000,\n",
    "    step=1,\n",
    "    description='Min output length',\n",
    "    disabled=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "max_output_len_widget = widgets.BoundedIntText(\n",
    "    value=BARTModelTRTConfig.MAX_OUTPUT_LENGTH[BART_VARIANT],\n",
    "    min=1,\n",
    "    max=100000,\n",
    "    step=1,\n",
    "    description='Max output length',\n",
    "    disabled=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "encoder_hidden_size_widget = widgets.BoundedIntText(\n",
    "    value=BARTModelTRTConfig.ENCODER_HIDDEN_SIZE[BART_VARIANT],\n",
    "    min=1,\n",
    "    max=100000,\n",
    "    step=1,\n",
    "    description='Encoder hidden size',\n",
    "    disabled=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "num_beam_widget = widgets.BoundedIntText(\n",
    "    value=1,\n",
    "    min=1,\n",
    "    max=100000,\n",
    "    step=1,\n",
    "    description='Number of beams',\n",
    "    disabled=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "widgets_all = widgets.VBox([\n",
    "    FP16_widget, \n",
    "    preview_dynamic_feature_widget,\n",
    "    KV_widgets,\n",
    "    batch_size_widget, \n",
    "    max_input_len_widget,\n",
    "    min_output_len_widget,\n",
    "    max_output_len_widget, \n",
    "    encoder_hidden_size_widget,\n",
    "    num_beam_widget\n",
    "])\n",
    "\n",
    "display(widgets_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077dd494-e8d8-42f9-bdbd-0362f1213118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference config\n",
    "FP16 = FP16_widget.value # flag to use FP16 precision in PyTorch & TRT\n",
    "preview_dynamic_shapes = preview_dynamic_feature_widget.value # flag to preview 8.5 EA feature\n",
    "HF_KV = HF_KV_widget.value # flag to use KV cache in HF\n",
    "TRT_KV = TRT_KV_widget.value # flag to use KV cache in TRT\n",
    "\n",
    "# Model config\n",
    "batch_size = batch_size_widget.value\n",
    "max_input_len = max_input_len_widget.value\n",
    "min_output_len = min_output_len_widget.value\n",
    "max_output_len = max_output_len_widget.value\n",
    "encoder_hidden_size = encoder_hidden_size_widget.value\n",
    "num_beams = num_beam_widget.value\n",
    "\n",
    "# Benchmark config\n",
    "# `TimingProfile` is a named tuple that specifies the number of experiments and number of times to call the function per iteration and number of warm-up calls, oercentiles, etc.\n",
    "timing_profile = TimingProfile(iterations=10, number=1, warmup=1, duration=0, percentile=[50,99])\n",
    "\n",
    "def percentile_print(timing):\n",
    "    return ', '.join(['p{} {:.2f}ms'.format(timing_profile.percentile[i], p*1000) for i,p in enumerate(timing)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae66d58-f994-4987-8f1d-1fa8ac2ec8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mbart variant can't be recognized by HF AutoClass yet\n",
    "if \"mbart\" not in BART_VARIANT:    \n",
    "    bart_model = AutoModelForPreTraining.from_pretrained(BART_VARIANT) # BartForConditionalGeneration\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BART_VARIANT) # BartTokenizer\n",
    "else:\n",
    "    from transformers import MBartForConditionalGeneration, MBart50Tokenizer\n",
    "    bart_model = MBartForConditionalGeneration.from_pretrained(BART_VARIANT)\n",
    "    tokenizer = MBart50Tokenizer.from_pretrained(BART_VARIANT, src_lang=\"en_XX\")\n",
    "\n",
    "config = AutoConfig.from_pretrained(BART_VARIANT)\n",
    "\n",
    "bart_model = bart_model.to('cuda').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7252ca90-1104-40dc-8e72-f51c07a4cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model locally\n",
    "pytorch_model_dir = './models/{}/pytorch'.format(BART_VARIANT)\n",
    "!mkdir -p $pytorch_model_dir\n",
    "\n",
    "if os.path.exists(pytorch_model_dir) and len(os.listdir(pytorch_model_dir)) != 0:\n",
    "    print('PyTorch model already exists. Skipping...')\n",
    "else:\n",
    "    bart_model.save_pretrained(pytorch_model_dir)\n",
    "    print(\"PyTorch model saved to {}\".format(pytorch_model_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4d1d6e-1cad-43a2-a8c3-4bc221070dc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1d0d09-be28-42a3-9135-46b796e5be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input sequence\n",
    "inputs = \"NVIDIA TensorRT-based applications perform up to 36X faster than CPU-only platforms during inference, enabling developers to optimize neural network models trained on all major frameworks, calibrate for lower precision with high accuracy, and deploy to hyperscale data centers, embedded platforms, or automotive product platforms. TensorRT, built on the NVIDIA CUDA parallel programming model, enables developers to optimize inference by leveraging libraries, development tools, and technologies in CUDA-X for AI, autonomous machines, high performance computing, and graphics. With new NVIDIA Ampere Architecture GPUs, TensorRT also uses sparse tensor cores for an additional performance boost.\"\n",
    "\n",
    "input_ids = tokenizer(inputs, padding=True, return_tensors=\"pt\").input_ids.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea023d-c4d4-43bb-9d77-c76684e0b06f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. PyTorch HuggingFace Inference\n",
    "\n",
    "Next, we will carry out inference with the HuggingFace PyTorch model as a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1d921-db47-4c45-bdcc-08ccc500ad99",
   "metadata": {},
   "source": [
    "### End-to-End HuggingFace Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d886e29a-1d1d-49e0-a351-3e4418f4bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder-decoder inference \n",
    "with torch.no_grad():\n",
    "    output_ids = bart_model.generate(input_ids, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False)    \n",
    "    outputs = tokenizer.decode(output_ids[-1,:], skip_special_tokens=True)    \n",
    "outputs_hf = outputs\n",
    "\n",
    "# timing\n",
    "# FP32\n",
    "bart_model.float()\n",
    "hf_nonkv_time = measure_python_inference_code(lambda: bart_model.generate(input_ids, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False), timing_profile)\n",
    "hf_kv_time = measure_python_inference_code(lambda: bart_model.generate(input_ids, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=True), timing_profile)\n",
    "\n",
    "# FP16\n",
    "bart_model.half()\n",
    "hf_nonkv_time_fp16 = measure_python_inference_code(lambda: bart_model.generate(input_ids, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=False), timing_profile)\n",
    "hf_kv_time_fp16 = measure_python_inference_code(lambda: bart_model.generate(input_ids, max_length=max_output_len, min_length=min_output_len, num_beams=num_beams, use_cache=True), timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab5c682-049a-48b3-830c-e1eecccbd553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results and timing statistics\n",
    "print(f'Input length: {input_ids.size(1)}')\n",
    "print(inputs)\n",
    "print('\\n')      \n",
    "print(f'Output length: {output_ids[-1,:].size(0)}')\n",
    "print(outputs_hf)\n",
    "print('\\n')      \n",
    "print(f'Device: {torch.cuda.get_device_name()}')\n",
    "print(f\"Precision: FP32, Number of Beams: {num_beams}\")\n",
    "print(f\"HF time (no KV cache): {percentile_print(hf_nonkv_time)}\")\n",
    "print(f\"HF time (w/ KV cache): {percentile_print(hf_kv_time)}\")\n",
    "print(f\"Precision: FP16, Number of Beams: {num_beams}\")\n",
    "print(f\"HF time (no KV cache): {percentile_print(hf_nonkv_time_fp16)}\")\n",
    "print(f\"HF time (w/ KV cache): {percentile_print(hf_kv_time_fp16)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667fcacc-02cb-415d-a9ff-2d2ec44ef225",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Time Measurement of Encoder, Decoder, and Full E2E\n",
    "For benchmarking purposes, we will employ helper functions `encoder_inference`, `decoder_inference`, and `full_inference_greedy` which execute the inference repeatedly for the BART encoder and decoder stacks separately as well as end-to-end for the entire output sequence, and measure the execution time. These execution times can be later on compared with TensorRT counterpart to demonstrate the speedup. \n",
    "\n",
    "Encoder and decoder of BART are wrapped as standalone PyTorch module for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c07516f-b02b-4722-b0bd-06b632259702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP32\n",
    "bart_model.float()\n",
    "bart_torch_encoder = BARTEncoderTorchFile.TorchModule(bart_model.get_encoder())\n",
    "bart_torch_decoder = BARTDecoderTorchFile.TorchModule(bart_model.get_decoder(), bart_model.lm_head, bart_model.final_logits_bias, bart_model.config)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    encoder_last_hidden_state, encoder_pytorch_time = encoder_inference(bart_torch_encoder, input_ids, timing_profile)\n",
    "    _, decoder_pytorch_time = decoder_inference(bart_torch_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, timing_profile, use_cache=HF_KV)\n",
    "    if num_beams == 1:\n",
    "        output_ids, full_pytorch_time = full_inference_greedy(bart_torch_encoder,bart_torch_decoder,input_ids,tokenizer,timing_profile,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    else:\n",
    "        output_ids, full_pytorch_time = full_inference_beam(bart_torch_encoder,bart_torch_decoder,input_ids,tokenizer,timing_profile,num_beams=num_beams,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    outputs = tokenizer.decode(output_ids[0], skip_special_tokens=True)    \n",
    "\n",
    "outputs_pytorch = outputs\n",
    "\n",
    "# FP16\n",
    "bart_model.half()\n",
    "bart_torch_encoder_fp16 = BARTEncoderTorchFile.TorchModule(bart_model.get_encoder())\n",
    "bart_torch_decoder_fp16 = BARTDecoderTorchFile.TorchModule(bart_model.get_decoder(), bart_model.lm_head, bart_model.final_logits_bias, bart_model.config)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    encoder_last_hidden_state, encoder_pytorch_time_fp16 = encoder_inference(bart_torch_encoder_fp16, input_ids, timing_profile)\n",
    "    _, decoder_pytorch_time_fp16 = decoder_inference(bart_torch_decoder_fp16, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_state, num_beams) if num_beams > 1 else encoder_last_hidden_state, timing_profile, use_cache=HF_KV)\n",
    "    if num_beams == 1:\n",
    "        output_ids_fp16, full_pytorch_time_fp16 = full_inference_greedy(bart_torch_encoder_fp16,bart_torch_decoder_fp16,input_ids,tokenizer,timing_profile,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    else:\n",
    "        output_ids_fp16, full_pytorch_time_fp16 = full_inference_beam(bart_torch_encoder_fp16,bart_torch_decoder_fp16,input_ids,tokenizer,timing_profile,num_beams=num_beams,max_length=max_output_len, min_length=min_output_len, use_cache=HF_KV)\n",
    "    outputs_fp16 = tokenizer.decode(output_ids_fp16[0], skip_special_tokens=True)    \n",
    "\n",
    "outputs_pytorch_fp16 = outputs_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a103e3a6-920b-4c97-818e-6140654abc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print\n",
    "print(f'PyTorch FP32 Output identical to HF results? {outputs_pytorch == outputs_hf}')\n",
    "print(f'PyTorch FP16 Output identical to HF results? {outputs_pytorch_fp16 == outputs_hf}')\n",
    "print('\\n')      \n",
    "print(f'Device: {torch.cuda.get_device_name()}')\n",
    "print(f\"Precision: FP32, Number of Beams: {num_beams}\")\n",
    "print(f\"Encoder time: {percentile_print(encoder_pytorch_time)}\")\n",
    "print(f\"Decoder time: {percentile_print(decoder_pytorch_time)}\")\n",
    "print(f\"Full E2E time: {percentile_print(full_pytorch_time)}\")\n",
    "print(f\"Precision: FP16, Number of Beams: {num_beams}\")\n",
    "print(f\"Encoder time: {percentile_print(encoder_pytorch_time_fp16)}\")\n",
    "print(f\"Decoder time: {percentile_print(decoder_pytorch_time_fp16)}\")\n",
    "print(f\"Full E2E time: {percentile_print(full_pytorch_time_fp16)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d662701-e430-4fdc-ad46-1f296defcf8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. TensorRT Engine Building\n",
    "\n",
    "### Convert PyTorch to ONNX\n",
    "\n",
    "Prior to converting the model to a TensorRT engine, we will first convert the PyTorch model to an intermediate universal format.\n",
    "\n",
    "ONNX is an open format for machine learning and deep learning models. It allows you to convert deep learning and machine learning models from different frameworks such as TensorFlow, PyTorch, MATLAB, Caffe, and Keras to a single format.\n",
    "\n",
    "The steps to convert a PyTorch model to TensorRT are as follows:\n",
    "- Convert the pretrained PyTorch model into ONNX.\n",
    "- Import the ONNX model into TensorRT, apply optimizations and generate a TensorRT engine.\n",
    "- Perform inference on the GPU using the engine. \n",
    "\n",
    "For the BART model, we will convert the encoder and decoder to ONNX and build each engine seperately. The logistics of this separate building approach come from the nature of sequence-to-sequence models. BART and T5 are good examples of sequence-to-sequence models which use encoder-decoder architecture. The encoder is only executed once on the input and generates hidden states. Next, the decoder is executed repeatedly in an auto-regressive manner until the entire output finishes generating, i.e. the output sequence length is the number of times the decoder runs. The most efficient way to run encoder-decoder models with TensorRT is to have two separate engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea48be5-1dae-4e93-92a4-840d7017ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = './models/{}/onnx'.format(BART_VARIANT)\n",
    "!mkdir -p $onnx_model_path\n",
    "\n",
    "# FP32\n",
    "bart_model.float()\n",
    "metadata = NetworkMetadata(variant=BART_VARIANT, precision=Precision(fp16=False), other=BARTMetadata(kv_cache=TRT_KV))\n",
    "trt_config = BARTModelTRTConfig()\n",
    "metadata_string = trt_config.get_metadata_string(metadata)\n",
    "\n",
    "encoder_onnx_model_fpath = metadata_string + \"-encoder.onnx\"\n",
    "decoder_onnx_model_fpath = metadata_string + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "# for onnx conversion, ensure model is on CPU and FP32 precision in this step\n",
    "bart_torchfile_encoder = BARTEncoderTorchFile(bart_model.to('cpu'), metadata)\n",
    "bart_torchfile_decoder = BARTDecoderTorchFile(bart_model.to('cpu'), metadata)\n",
    "\n",
    "onnx_bart_encoder = bart_torchfile_encoder.as_onnx_model(os.path.join(onnx_model_path, encoder_onnx_model_fpath), force_overwrite=False)\n",
    "onnx_bart_decoder = bart_torchfile_decoder.as_onnx_model(os.path.join(onnx_model_path, decoder_onnx_model_fpath), force_overwrite=False)\n",
    "\n",
    "# FP16\n",
    "metadata_fp16 = NetworkMetadata(variant=BART_VARIANT, precision=Precision(fp16=True), other=BARTMetadata(kv_cache=TRT_KV))\n",
    "trt_config_fp16 = BARTModelTRTConfig()\n",
    "metadata_string_fp16 = trt_config.get_metadata_string(metadata_fp16)\n",
    "\n",
    "encoder_onnx_model_fpath_fp16 = metadata_string_fp16 + \"-encoder.onnx\"\n",
    "decoder_onnx_model_fpath_fp16 = metadata_string_fp16 + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "# for onnx conversion, ensure model is on CPU and FP32 precision in this step\n",
    "bart_torchfile_encoder = BARTEncoderTorchFile(bart_model.to('cpu'), metadata)\n",
    "bart_torchfile_decoder = BARTDecoderTorchFile(bart_model.to('cpu'), metadata)\n",
    "\n",
    "onnx_bart_encoder_fp16 = bart_torchfile_encoder.as_onnx_model(os.path.join(onnx_model_path, encoder_onnx_model_fpath_fp16), force_overwrite=False)\n",
    "onnx_bart_decoder_fp16 = bart_torchfile_decoder.as_onnx_model(os.path.join(onnx_model_path, decoder_onnx_model_fpath_fp16), force_overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf007e-5508-485c-a87f-9bfe16260452",
   "metadata": {},
   "source": [
    "### Convert ONNX to TensorRT\n",
    "\n",
    "Now we are ready to parse the ONNX encoder and decoder models and convert them to optimized TensorRT engines.\n",
    "\n",
    "Since the models contains dynamic input shapes, we can specify a valid input range with a TensorRT optimization profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd6e3fc-6797-46b0-a211-ce42d3769105",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorrt_model_path = './models/{}/tensorrt'.format(BART_VARIANT)\n",
    "!mkdir -p $tensorrt_model_path\n",
    "\n",
    "# Encoder optimization profiles\n",
    "encoder_profile = Profile()\n",
    "encoder_profile.add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size, 1),\n",
    "    opt=(batch_size, max_input_len // 2),\n",
    "    max=(batch_size, max_input_len),\n",
    ")\n",
    "\n",
    "# Decoder optimization profiles\n",
    "decoder_profile = Profile()\n",
    "decoder_profile.add(\n",
    "    \"input_ids\",\n",
    "    min=(batch_size * num_beams, 1),\n",
    "    opt=(batch_size * num_beams, max_output_len // 2),\n",
    "    max=(batch_size * num_beams, max_output_len),\n",
    ")\n",
    "decoder_profile.add(\n",
    "    \"encoder_hidden_states\",\n",
    "    min=(batch_size * num_beams, 1, encoder_hidden_size),\n",
    "    opt=(batch_size * num_beams, max_input_len // 2, encoder_hidden_size),\n",
    "    max=(batch_size * num_beams, max_input_len, encoder_hidden_size),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5738ff-790e-47a0-ba03-27af87742646",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_tag = f\"bs{batch_size}\"\n",
    "\n",
    "if num_beams > 1:\n",
    "    engine_tag += \"-beam{}\".format(num_beams)\n",
    "\n",
    "preview_features = []\n",
    "if preview_dynamic_shapes:\n",
    "    preview_features = [PreviewFeature.FASTER_DYNAMIC_SHAPES_0805]\n",
    "    engine_tag += \"-previewFasterDynamicShapes\"\n",
    "\n",
    "# FP32\n",
    "encoder_engine_name = os.path.join(tensorrt_model_path, encoder_onnx_model_fpath) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "decoder_engine_name = os.path.join(tensorrt_model_path, decoder_onnx_model_fpath) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(encoder_engine_name):\n",
    "    bart_trt_encoder_engine = BARTEncoderONNXFile(os.path.join(onnx_model_path, encoder_onnx_model_fpath), metadata).as_trt_engine(\n",
    "        encoder_engine_name, \n",
    "        profiles=[encoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    bart_trt_encoder_engine = BARTEncoderTRTEngine(encoder_engine_name, metadata)\n",
    "    \n",
    "if not os.path.exists(decoder_engine_name):\n",
    "    bart_trt_decoder_engine = BARTDecoderONNXFile(os.path.join(onnx_model_path, decoder_onnx_model_fpath), metadata).as_trt_engine(\n",
    "        decoder_engine_name, \n",
    "        profiles=[decoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    bart_trt_decoder_engine = BARTDecoderTRTEngine(decoder_engine_name, metadata)\n",
    "\n",
    "# FP16\n",
    "encoder_engine_name_fp16 = os.path.join(tensorrt_model_path, encoder_onnx_model_fpath_fp16) + f\"-{engine_tag}.engine\".replace(f\"-beam{num_beams}\", \"\") # encoder engine not affected by beam search\n",
    "decoder_engine_name_fp16 = os.path.join(tensorrt_model_path, decoder_onnx_model_fpath_fp16) + f\"-{engine_tag}.engine\"\n",
    "\n",
    "if not os.path.exists(encoder_engine_name_fp16):\n",
    "    bart_trt_encoder_engine_fp16 = BARTEncoderONNXFile(os.path.join(onnx_model_path, encoder_onnx_model_fpath_fp16), metadata_fp16).as_trt_engine(\n",
    "        encoder_engine_name_fp16, \n",
    "        profiles=[encoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    bart_trt_encoder_engine_fp16 = BARTEncoderTRTEngine(encoder_engine_name_fp16, metadata_fp16)\n",
    "    \n",
    "if not os.path.exists(decoder_engine_name_fp16):\n",
    "    bart_trt_decoder_engine_fp16 = BARTDecoderONNXFile(os.path.join(onnx_model_path, decoder_onnx_model_fpath_fp16), metadata_fp16).as_trt_engine(\n",
    "        decoder_engine_name_fp16, \n",
    "        profiles=[decoder_profile], \n",
    "        preview_features=preview_features\n",
    "    )\n",
    "else:\n",
    "    bart_trt_decoder_engine_fp16 = BARTDecoderTRTEngine(decoder_engine_name_fp16, metadata_fp16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7f6fc-1e6a-4ddc-8e9b-543d9e8dab4d",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "\n",
    "## 4. TensorRT Inference\n",
    "\n",
    "Great, if you have reached this stage, it means we now have successfully built optimized TensorRT engines for the BART model, ready for us to carry out inference. The BART model with TensorRT backend can now be employed in place of the original HuggingFace BART model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3954f2f4-c393-463b-a44b-3e5335032b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorRT engines\n",
    "trt_config = AutoConfig.from_pretrained(BART_VARIANT)\n",
    "trt_config.use_cache = metadata.other.kv_cache\n",
    "trt_config.num_layers = BARTModelTRTConfig.NUMBER_OF_LAYERS[BART_VARIANT]\n",
    "\n",
    "# FP32\n",
    "bart_trt_encoder = BARTTRTEncoder(bart_trt_encoder_engine, metadata, trt_config, batch_size=batch_size)\n",
    "bart_trt_decoder = BARTTRTDecoder(bart_trt_decoder_engine, metadata, trt_config, batch_size=batch_size, num_beams=num_beams)\n",
    "\n",
    "# FP16\n",
    "bart_trt_encoder_fp16 = BARTTRTEncoder(bart_trt_encoder_engine_fp16, metadata_fp16, trt_config, batch_size=batch_size)\n",
    "bart_trt_decoder_fp16 = BARTTRTDecoder(bart_trt_decoder_engine_fp16, metadata_fp16, trt_config, batch_size=batch_size, num_beams=num_beams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7025246-4f14-4449-bb93-6c1566f48773",
   "metadata": {},
   "source": [
    "### End-to-End TensorRT Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a5bbfe-a576-4a94-99d1-f0862b31fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation_logits_process import (\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    MinLengthLogitsProcessor,\n",
    "    ForcedBOSTokenLogitsProcessor,\n",
    "    ForcedEOSTokenLogitsProcessor,\n",
    "    LogitsProcessorList,\n",
    ")\n",
    "from transformers.generation_stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n",
    "from transformers.generation_beam_search import (\n",
    "    BeamSearchScorer,\n",
    ")\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_output_len)])\n",
    "no_repeat_ngram_size = BARTModelTRTConfig.NO_REPEAT_NGRAM_SIZE\n",
    "min_length = BARTModelTRTConfig.MIN_OUTPUT_LENGTH[BART_VARIANT]\n",
    "logits_processor = LogitsProcessorList([\n",
    "    NoRepeatNGramLogitsProcessor(no_repeat_ngram_size), \n",
    "    MinLengthLogitsProcessor(min_length, tokenizer.convert_tokens_to_ids(tokenizer.eos_token)),\n",
    "    ForcedBOSTokenLogitsProcessor(tokenizer.convert_tokens_to_ids(tokenizer.bos_token)),\n",
    "    ForcedEOSTokenLogitsProcessor(max_output_len, tokenizer.convert_tokens_to_ids(tokenizer.eos_token))\n",
    "]) # by checking HuggingFace's generate() implementation carefully, the default logits processor for BART has no_repeat_ngram_size = 3 and forced_eos_token_id = 2. In this way we can ensure identical results with raw HuggingFace\n",
    "\n",
    "decoder_initial_input = torch.full(\n",
    "    (batch_size, 1), tokenizer.convert_tokens_to_ids(tokenizer.eos_token), dtype=torch.int32\n",
    ").to('cuda')\n",
    "\n",
    "if num_beams > 1:\n",
    "    decoder_initial_input = expand_inputs_for_beam_search(decoder_initial_input, expand_size=num_beams)\n",
    "    \n",
    "# FP32\n",
    "def e2e_trt():\n",
    "    with torch.no_grad():\n",
    "        encoder_last_hidden_states = bart_trt_encoder(input_ids=input_ids)\n",
    "        \n",
    "        if num_beams > 1:\n",
    "            # prepare input for beam search\n",
    "            encoder_last_hidden_states = expand_inputs_for_beam_search(encoder_last_hidden_states, expand_size=num_beams)\n",
    "\n",
    "            # beam scorer must be reset before each beam search run, otherwise beam search will be skipped due to scorer cache\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=num_beams,\n",
    "                device=\"cuda\",\n",
    "                do_early_stopping=True,\n",
    "            )\n",
    "        \n",
    "        bart_trt_decoder.set_encoder_hidden_states_for_inference_cycle(encoder_last_hidden_states)\n",
    "        \n",
    "        if num_beams == 1:\n",
    "            decoder_output = bart_trt_decoder.greedy_search(\n",
    "                input_ids=decoder_initial_input,\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "        else:\n",
    "            decoder_output = bart_trt_decoder.beam_search(\n",
    "                input_ids=decoder_initial_input,\n",
    "                beam_scorer=beam_scorer,\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "    return decoder_output\n",
    "\n",
    "output_ids = e2e_trt()\n",
    "outputs_trt = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "trt_time = measure_python_inference_code(e2e_trt, timing_profile)\n",
    "\n",
    "# FP16\n",
    "def e2e_trt_fp16():\n",
    "    with torch.no_grad():\n",
    "        encoder_last_hidden_states = bart_trt_encoder_fp16(input_ids=input_ids)\n",
    "        \n",
    "        if num_beams > 1:\n",
    "            # prepare input for beam search\n",
    "            encoder_last_hidden_states = expand_inputs_for_beam_search(encoder_last_hidden_states, expand_size=num_beams)\n",
    "            \n",
    "            # beam scorer must be reset before each beam search run, otherwise beam search will be skipped due to scorer cache\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=num_beams,\n",
    "                device=\"cuda\",\n",
    "                do_early_stopping=True,\n",
    "            )\n",
    "        \n",
    "        bart_trt_decoder_fp16.set_encoder_hidden_states_for_inference_cycle(encoder_last_hidden_states)\n",
    "        \n",
    "        if num_beams == 1:\n",
    "            decoder_output = bart_trt_decoder_fp16.greedy_search(\n",
    "                input_ids=decoder_initial_input,\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "        else:\n",
    "            decoder_output = bart_trt_decoder_fp16.beam_search(\n",
    "                input_ids=decoder_initial_input,\n",
    "                beam_scorer=beam_scorer,\n",
    "                encoder_hidden_states=encoder_last_hidden_states,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "                logits_processor=logits_processor,\n",
    "                use_cache=metadata.other.kv_cache,\n",
    "                use_cuda=True\n",
    "            )\n",
    "    return decoder_output\n",
    "\n",
    "output_ids_fp16 = e2e_trt_fp16()\n",
    "outputs_trt_fp16 = tokenizer.decode(output_ids_fp16[0], skip_special_tokens=True)\n",
    "trt_time_fp16 = measure_python_inference_code(e2e_trt_fp16, timing_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6198afcf-70d1-46ef-a515-dcf5ea4c17b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results and timing statistics\n",
    "print(f'Device: {torch.cuda.get_device_name()}')\n",
    "print(f\"Using engine: {metadata_string + '-' + engine_tag}\")   \n",
    "print(f'Output identical to HF results? {outputs_trt == outputs_hf}')\n",
    "print(f\"Precision: FP32\")\n",
    "print(f'TRT time: {percentile_print(trt_time)}')\n",
    "print()\n",
    "print(f\"Using engine: {metadata_string_fp16 + '-' + engine_tag}\")   \n",
    "print(f'Output identical to HF results? {outputs_trt_fp16 == outputs_hf}')\n",
    "print(f\"Precision: FP16\")\n",
    "print(f'TRT time: {percentile_print(trt_time_fp16)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9d4a98-b034-470e-a9f8-096d4100b8d4",
   "metadata": {},
   "source": [
    "### Time Measurement of Encoder, Decoder, and Full E2E\n",
    "We will benchmark the encoder, decoder, and full end-to-end as we did for HuggingFace before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2320e4bf-94f2-40d8-9a86-3a1ea352fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP32\n",
    "encoder_last_hidden_states, encoder_trt_time = encoder_inference(bart_trt_encoder, input_ids, timing_profile)\n",
    "_, decoder_trt_time = decoder_inference(bart_trt_decoder, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_states, num_beams) if num_beams > 1 else encoder_last_hidden_states, timing_profile)\n",
    "\n",
    "if num_beams == 1:\n",
    "    _, full_trt_time = full_inference_greedy(\n",
    "        bart_trt_encoder,\n",
    "        bart_trt_decoder,\n",
    "        input_ids,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        max_length=max_output_len,\n",
    "        min_length=BARTModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "    )\n",
    "else:\n",
    "    _, full_trt_time = full_inference_beam(\n",
    "        bart_trt_encoder,\n",
    "        bart_trt_decoder,\n",
    "        input_ids,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        num_beams=num_beams,\n",
    "        max_length=max_output_len,\n",
    "        min_length=BARTModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    \n",
    "print(f'Encoder time: {percentile_print(encoder_trt_time)}')\n",
    "print(f'Decoder time: {percentile_print(decoder_trt_time)}')\n",
    "print(f'Full E2E time: {percentile_print(full_trt_time)}')\n",
    "\n",
    "# FP16\n",
    "encoder_last_hidden_states, encoder_trt_time_fp16 = encoder_inference(bart_trt_encoder_fp16, input_ids, timing_profile)\n",
    "_, decoder_trt_time_fp16 = decoder_inference(bart_trt_decoder_fp16, expand_inputs_for_beam_search(input_ids, num_beams) if num_beams > 1 else input_ids, expand_inputs_for_beam_search(encoder_last_hidden_states, num_beams) if num_beams > 1 else encoder_last_hidden_states, timing_profile)\n",
    "\n",
    "if num_beams == 1:\n",
    "    _, full_trt_time_fp16 = full_inference_greedy(\n",
    "        bart_trt_encoder_fp16,\n",
    "        bart_trt_decoder_fp16,\n",
    "        input_ids,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        max_length=max_output_len,\n",
    "        min_length=BARTModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "    )\n",
    "else:\n",
    "    _, full_trt_time_fp16 = full_inference_beam(\n",
    "        bart_trt_encoder_fp16,\n",
    "        bart_trt_decoder_fp16,\n",
    "        input_ids,\n",
    "        tokenizer,\n",
    "        timing_profile,\n",
    "        num_beams=num_beams,\n",
    "        max_length=max_output_len,\n",
    "        min_length=BARTModelTRTConfig.MIN_OUTPUT_LENGTH[metadata.variant],\n",
    "        batch_size=batch_size,\n",
    "        use_cache=metadata.other.kv_cache,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "print(f'Encoder FP16 time: {percentile_print(encoder_trt_time_fp16)}')\n",
    "print(f'Decoder FP16 time: {percentile_print(decoder_trt_time_fp16)}')\n",
    "print(f'Full E2E FP16 time: {percentile_print(full_trt_time_fp16)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27cda12-7e56-4a87-935d-ce598557cf26",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1090b46c-adec-4684-8c53-a54a196dedb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "data = [\n",
    "    ['Framework', 'Precision', 'Encoder p50 (ms)', 'Decoder p50 (ms)', 'Full E2E p50 (ms)', 'Accuracy'],\n",
    "    ['HuggingFace (w/o cache)', 'FP32', '-', '-', f'{hf_nonkv_time[0]*1000:.2f}', '-'],\n",
    "    ['HuggingFace (w/ cache)', 'FP32', '-', '-', f'{hf_kv_time[0]*1000:.2f}', '-'],\n",
    "    ['HuggingFace (w/o cache)', 'FP16', '-', '-', f'{hf_nonkv_time_fp16[0]*1000:.2f}', '-'],\n",
    "    ['HuggingFace (w/ cache)', 'FP16', '-', '-', f'{hf_kv_time_fp16[0]*1000:.2f}', '-'],\n",
    "    ['PyTorch', 'FP32', f'{encoder_pytorch_time[0]*1000:.2f}', f'{decoder_pytorch_time[0]*1000:.2f}', f'{full_pytorch_time[0]*1000:.2f}', outputs_pytorch == outputs_hf],\n",
    "    ['PyTorch', 'FP16', f'{encoder_pytorch_time_fp16[0]*1000:.2f}', f'{decoder_pytorch_time_fp16[0]*1000:.2f}', f'{full_pytorch_time_fp16[0]*1000:.2f}', outputs_pytorch_fp16 == outputs_hf],\n",
    "    ['TensorRT', 'FP32', f'{encoder_trt_time[0]*1000:.2f}', f'{decoder_trt_time[0]*1000:.2f}', f'{full_trt_time[0]*1000:.2f}', outputs_trt == outputs_hf],\n",
    "    ['TensorRT', 'FP16', f'{encoder_trt_time_fp16[0]*1000:.2f}', f'{decoder_trt_time_fp16[0]*1000:.2f}', f'{full_trt_time_fp16[0]*1000:.2f}', outputs_trt_fp16 == outputs_hf],\n",
    "]\n",
    "\n",
    "print(tabulate(data, headers='firstrow', tablefmt='github'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92031643-8ee8-4d50-864b-a08e4d551dc6",
   "metadata": {},
   "source": [
    "We can now compare the original HuggingFace model and the TensorRT engine, from both separate encoder/decoder and end-to-end speed difference. For bart-base variant on an NVIDIA Titan V GPU and input/output sequence length around 130, this results in about 2x performance improvement with FP16 inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a498672-ba25-42b0-b89e-79e0b869943a",
   "metadata": {},
   "source": [
    "## Variable Input/Output Length\n",
    "\n",
    "We can run more tests by varying input/output length, while using the same engines.\n",
    "\n",
    "Note that TensorRT performance depends on optimal selection of the kernels in the engine. The variable length test here uses the same engine built with max input/output length profile, therefore may not represent the best perf. If the use case has known input/output length ranges, it is highly recommended to specify in the TensorRT engine profiles to ensure optimized kernel selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f25217-be31-45bf-8652-0e18162fa360",
   "metadata": {},
   "source": [
    "### Single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d8a01-e5b7-449e-9e43-7c8315a2578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure HF model are on GPU for testing (cells above moved it CPU)\n",
    "bart_model = bart_model.to('cuda').eval()\n",
    "\n",
    "in_len, out_len = 24, 24\n",
    "\n",
    "data = [\n",
    "    ['(input_len, output_len)', 'HF FP32 p50 (s)', 'HF FP16 p50 (s)', 'TRT FP32 p50 (s)', 'TRT FP16 p50 (s)'],\n",
    "]\n",
    "\n",
    "assert in_len <= max_input_len and out_len <= max_output_len\n",
    "    \n",
    "in_ids = torch.randint(0, BARTModelTRTConfig.VOCAB_SIZE[BART_VARIANT], (batch_size, in_len)).to('cuda')\n",
    "\n",
    "# HF\n",
    "bart_model.float()\n",
    "hf_32 = measure_python_inference_code(lambda: bart_model.generate(in_ids, min_length=out_len, max_length=out_len, num_beams=num_beams, use_cache=True), timing_profile)\n",
    "bart_model.half()\n",
    "hf_16 = measure_python_inference_code(lambda: bart_model.generate(in_ids, min_length=out_len, max_length=out_len, num_beams=num_beams, use_cache=True), timing_profile)\n",
    "\n",
    "# TRT\n",
    "if num_beams == 1:\n",
    "    _, trt_32 = full_inference_greedy(bart_trt_encoder, bart_trt_decoder, in_ids, tokenizer, timing_profile, max_length=out_len, min_length=out_len, batch_size=batch_size, use_cache=metadata.other.kv_cache, use_cuda=True,)\n",
    "    _, trt_16 = full_inference_greedy(bart_trt_encoder_fp16, bart_trt_decoder_fp16, in_ids, tokenizer, timing_profile, max_length=out_len, min_length=out_len, batch_size=batch_size, use_cache=metadata.other.kv_cache, use_cuda=True,)\n",
    "else:\n",
    "    _, trt_32 = full_inference_beam(bart_trt_encoder, bart_trt_decoder, in_ids, tokenizer, timing_profile, num_beams=num_beams, max_length=out_len, min_length=out_len, batch_size=batch_size, use_cache=metadata.other.kv_cache, early_stopping=True,)\n",
    "    _, trt_16 = full_inference_beam(bart_trt_encoder_fp16, bart_trt_decoder_fp16, in_ids, tokenizer, timing_profile, num_beams=num_beams, max_length=out_len, min_length=out_len, batch_size=batch_size, use_cache=metadata.other.kv_cache, early_stopping=True,)\n",
    "\n",
    "data.append([(in_len, out_len), hf_32[0], hf_16[0], trt_32[0], trt_16[0]])\n",
    "\n",
    "print(tabulate(data, headers='firstrow', tablefmt='github'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edf4f5c-49a0-4509-a4d7-8b561dba3f88",
   "metadata": {},
   "source": [
    "### Several representative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e335010-ff7f-4822-85ae-bca8d235de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure HF model are on GPU for testing (cells above moved it CPU)\n",
    "bart_model = bart_model.to('cuda').eval()\n",
    "\n",
    "input_output_len_list = [\n",
    "    (64, 128), # generation task\n",
    "    (64, 512),\n",
    "    (512, 64), # summarization task\n",
    "    (128, 64),\n",
    "    (32, 32), # translation task\n",
    "    (128, 128),\n",
    "    (512, 512),\n",
    "]\n",
    "\n",
    "data = [\n",
    "    ['(input_len, output_len)', 'HF FP32 p50 (s)', 'HF FP16 p50 (s)', 'TRT FP32 p50 (s)', 'TRT FP16 p50 (s)'],\n",
    "]\n",
    "\n",
    "for (in_len, out_len) in input_output_len_list:\n",
    "    assert in_len <= max_input_len and out_len <= max_output_len\n",
    "    \n",
    "    in_ids = torch.randint(0, BARTModelTRTConfig.VOCAB_SIZE[BART_VARIANT], (batch_size, in_len)).to('cuda')\n",
    "    \n",
    "    # HF\n",
    "    bart_model.float()\n",
    "    hf_32 = measure_python_inference_code(lambda: bart_model.generate(in_ids, min_length=out_len, max_length=out_len, num_beams=num_beams, use_cache=True), timing_profile)\n",
    "    bart_model.half()\n",
    "    hf_16 = measure_python_inference_code(lambda: bart_model.generate(in_ids, min_length=out_len, max_length=out_len, num_beams=num_beams, use_cache=True), timing_profile)\n",
    "    \n",
    "    # TRT\n",
    "    if num_beams == 1:\n",
    "        _, trt_32 = full_inference_greedy(bart_trt_encoder, bart_trt_decoder, in_ids, tokenizer, timing_profile, max_length=out_len, min_length=out_len, batch_size=batch_size, use_cache=metadata.other.kv_cache, use_cuda=True,)\n",
    "        _, trt_16 = full_inference_greedy(bart_trt_encoder_fp16, bart_trt_decoder_fp16, in_ids, tokenizer, timing_profile, max_length=out_len, min_length=out_len, batch_size=batch_size, use_cache=metadata.other.kv_cache, use_cuda=True,)\n",
    "    else:\n",
    "        _, trt_32 = full_inference_beam(bart_trt_encoder, bart_trt_decoder, in_ids, tokenizer, timing_profile, num_beams=num_beams, max_length=out_len, min_length=out_len, batch_size=batch_size, use_cache=metadata.other.kv_cache, early_stopping=True,)\n",
    "        _, trt_16 = full_inference_beam(bart_trt_encoder_fp16, bart_trt_decoder_fp16, in_ids, tokenizer, timing_profile, num_beams=num_beams, max_length=out_len, min_length=out_len, batch_size=batch_size, use_cache=metadata.other.kv_cache, early_stopping=True,)\n",
    "    \n",
    "    data.append([(in_len, out_len), hf_32[0], hf_16[0], trt_32[0], trt_16[0]])\n",
    "\n",
    "print(tabulate(data, headers='firstrow', tablefmt='github'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a598a0ae-2e21-4898-ae56-8429a5d00760",
   "metadata": {},
   "source": [
    "It shows around 2x speedup comparing to HuggingFace's KV-cache optimized timing, for relatively short output sequence length. For long output sequence length, due to memory copies overhead between the decoding steps, TensorRT may not provide significant speedup at the current stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f5dca-397c-4c8c-9200-61b30cdba824",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has walked you through the process of converting a HuggingFace PyTorch BART model to an optimized TensorRT engine for inference in easy steps. The TensorRT inference engine can be conviniently used as a drop-in replacement for the orginial HuggingFace BART model while providing speed up. \n",
    "\n",
    "If you are interested in further details of the conversion process, check out [BART/trt.py](../BART/trt.py)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
