{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af87501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f305aa",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Accelerating HuggingFace BLIP Inference with TensorRT\n",
    "\n",
    "BLIP is an encoder-decoder multimodal model that can perform different vision-language tasks. Here we show BLIP working on image captioning tasks, containing a vision encoder and text decoder in the model architecture.\n",
    "\n",
    "This notebook shows easy steps to convert a [HuggingFace PyTorch BLIP model](https://huggingface.co/transformers/model_doc/blip.html) to a TensorRT engine for high-performance inference in a few lines of code. \n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "Follow the instruction at https://github.com/NVIDIA/TensorRT to build the TensorRT-OSS docker container required to run this notebook.\n",
    "\n",
    "Next, we install some extra dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b9887b",
   "metadata": {},
   "source": [
    "**Note:** After this step, you should restart the Jupyter kernel for the change to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d83eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7751fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import torch\n",
    "\n",
    "from BLIP.frameworks import BLIPHF\n",
    "from BLIP.trt import BLIPTRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ef3e8f",
   "metadata": {},
   "source": [
    "\n",
    "## API usage\n",
    "\n",
    "We have wrapped the process of importing models from PyTorch, exporting to onnx files and build TRT engines into a single class. We introduce new `BLIPHF` and `BLIPTRT` classes that both expose `generate` as the main entry point to run BLIP. `BLIPTRT` will automatically do all the 3 steps per user inputs. Here is an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965ae342",
   "metadata": {},
   "source": [
    "\n",
    "### Specify model arguments\n",
    "\n",
    "You pick your favorite model and configurations, and TRT will run it for you! The main choice that you need to make is:\n",
    "- `use_cache`: kv cache to speed decoding\n",
    "- `num_beams`: beam search for better results\n",
    "- `fp16`: Using float16 to speed decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaba86cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"variant\": \"Salesforce/blip-image-captioning-base\", # A HuggingFace model variant name. Required.\n",
    "    \"use_cache\": True, # We support decoder kv cache in generation. Default: True\n",
    "    \"fp16\": True, # Default: True\n",
    "    \"num_beams\": 3, # We support beam search in generation. Default: 1\n",
    "    \"batch_size\": 1, # Default: 1\n",
    "    \"use_mask\": False, # Default: False\n",
    "    # Folder name. Required. All the PyTorch, ONNX and TRT Engines will be stored in the folder.\n",
    "    \"working_dir\": \"models\",\n",
    "    # Log level.\n",
    "    \"info\": True,\n",
    "    # Benchmarking args\n",
    "    \"iterations\": 10,\n",
    "    \"number\": 1,\n",
    "    \"warmup\": 3,\n",
    "    \"duration\": 0,\n",
    "    \"percentile\": 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6303c9fb",
   "metadata": {},
   "source": [
    "\n",
    "### Initialize the models\n",
    "Calling the API is just this easy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e5c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_model = BLIPHF(**args)\n",
    "trt_model = BLIPTRT(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96a7194",
   "metadata": {},
   "source": [
    "### Try your photo!\n",
    "Both `BLIPHF` and `BLIPTRT` exposes `setup_tokenizer_and_model` and `generate`. If `setup_tokenizer_and_model` is not called prior to `generate`, it will be called first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8362b492",
   "metadata": {},
   "source": [
    "Load the image captioning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c196ace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "dataset = load_dataset(\"lambdalabs/pokemon-blip-captions\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead67332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d428b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "example = dataset[3]\n",
    "image = example[\"image\"]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88636fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    " # prepare image for the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "inputs = processor(images=image, text=\"\", return_tensors=\"pt\").to(device)\n",
    "pixel_values = inputs.pixel_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283c8d4f",
   "metadata": {},
   "source": [
    "#### Run BLIP HF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f29b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_model.models = framework_model.setup_tokenizer_and_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62158d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['batch_size'] > 1:\n",
    "    pixel_values = pixel_values.repeat_interleave(args['batch_size'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3987f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_generated_caption = framework_model.generate(pixel_values=pixel_values, input=\"\")\n",
    "\n",
    "print(hf_generated_caption[0])\n",
    "print(hf_generated_caption[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d214ebad",
   "metadata": {},
   "source": [
    "#### Run BLIP HF model - reference run with HF BLIP workflow (in fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890c1886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipForConditionalGeneration\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = model.to(device)\n",
    "\n",
    "hf_fp32_generated_ids = model.generate(pixel_values=pixel_values, input_ids=inputs.input_ids)\n",
    "hf_fp32_generated_caption = processor.batch_decode(hf_fp32_generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(hf_fp32_generated_ids)\n",
    "print(hf_fp32_generated_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fb1c92",
   "metadata": {},
   "source": [
    "#### Run BLIP TRT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3776c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_model.models = trt_model.setup_tokenizer_and_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5be147",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_generated_caption = trt_model.generate(pixel_values=pixel_values, input=\"\")\n",
    "\n",
    "print(trt_generated_caption[0])\n",
    "print(trt_generated_caption[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e418b",
   "metadata": {},
   "source": [
    "\n",
    "### Evaluate\n",
    "\n",
    "Image captioning models are typically evaluated with the [Rouge Score](https://huggingface.co/spaces/evaluate-metric/rouge) or [Word Error Rate](https://huggingface.co/spaces/evaluate-metric/wer). For this guide, you will use the Word Error Rate (WER). \n",
    "\n",
    "We use the ðŸ¤— Evaluate library to do so. For potential limitations and other gotchas of the WER, refer to [this guide](https://huggingface.co/spaces/evaluate-metric/wer).\n",
    "\n",
    "We will evaluate the WER between the framework model (pytorch/HF) and the TRT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c6c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate jiwer -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3202f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "wer = load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf1c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hf_fp32_captions:\", hf_fp32_generated_caption, \"\\nhf_generated_caption\", hf_generated_caption[1][0], \"\\ntrt_generated_caption\", trt_generated_caption[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5325f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_score = wer.compute(predictions=[trt_generated_caption[1][0]], references=[hf_generated_caption[1][0]])\n",
    "print(\"wer_score\", wer_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdad270",
   "metadata": {},
   "source": [
    "\n",
    "### Performance benchmark\n",
    "You can see that TRT and PyTorch generates the same result, which is expected. To measure their performance, both `BLIPHF` and `BLIPTRT` exposes `execute_inference`, `full_inference`, `encoder_inference` and `decoder_inference` to measure the inference time. Let's take a look at how our latest TRT performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368c52a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "data = [\n",
    "    ['full p50(s)', 'decoder p50(s)', 'encoder p50(s)'],\n",
    "]\n",
    "\n",
    "def format_result(result):\n",
    "    entry = []\n",
    "    for segment in result.runtime:\n",
    "        entry.append('{:.4f}'.format(segment.runtime))\n",
    "    \n",
    "    return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51a30d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_result = framework_model.execute_inference(pixel_values=pixel_values, input=[\"\"])\n",
    "data.append(format_result(framework_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55078703",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_result = trt_model.execute_inference(pixel_values=pixel_values, input=[\"\"])\n",
    "data.append(format_result(trt_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec2c03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tabulate(data, headers='firstrow', tablefmt='github'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f9c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37734c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514a5faf",
   "metadata": {},
   "source": [
    "### Variable Input/Output Performance Benchmarking\n",
    "\n",
    "We can run more tests by varying input/output length, while using the same engines.\n",
    "\n",
    "Note that TensorRT performance depends on optimal selection of the kernels in the engine. The variable length test here uses the same engine built with max input/output length profile = `max_length` in HuggingFace config to represent the best use of the model. If you want to change the length, please change this field prior to calling `set_tokenizer_and_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758ff2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_output_len_list = [\n",
    "    (2, 32),\n",
    "    (16, 8), \n",
    "    (32, 32),\n",
    "    (64, 128),\n",
    "]\n",
    "\n",
    "data = [\n",
    "    ['(input_len, output_len)', 'HF p50 (s)', 'TRT p50 (s)'],\n",
    "]\n",
    "\n",
    "for (in_len, out_len) in input_output_len_list:\n",
    "\n",
    "    input_ids = torch.randint(0, framework_model.config.vocab_size, (framework_model.config.batch_size, in_len))\n",
    "        \n",
    "    # Note: the min/max output len configs change the default generation config for both the framework and trt models. The above generate() functions will behave differently unless set those configs back to default\n",
    "    # The config change here is only to fix the out_len for benchmarking purpose\n",
    "    framework_model.config.min_output_length = in_len + out_len\n",
    "    framework_model.config.max_output_length = in_len + out_len\n",
    "    trt_model.config.min_output_length = in_len + out_len\n",
    "    trt_model.config.max_output_length = in_len + out_len\n",
    "    \n",
    "    _, framework_e2e = framework_model.full_inference(input_ids=input_ids, pixel_values=pixel_values)\n",
    "    _, trt_e2e = trt_model.full_inference(input_ids=input_ids, pixel_values=pixel_values)\n",
    "\n",
    "    data.append([(in_len, out_len), framework_e2e, trt_e2e])\n",
    "\n",
    "print(tabulate(data, headers='firstrow', tablefmt='github'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6df636",
   "metadata": {},
   "source": [
    "Did TensorRT's performance amaze you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f4f7c9",
   "metadata": {},
   "source": [
    "## Conclusion and where-to next?\n",
    "\n",
    "Is this the end? The API sounds too simple. I am used to the previous version that walks me step by step, and/or I want to know more on the process of conversion. Just follow the directory and you will find that PyTorch model, ONNX files and TRT engines are there. Feel free to investigate them. We have wrapped the entire model conversion process in `setup_tokenizer_and_model`. The TensorRT inference engine can be conviniently used as a drop-in replacement for the orginial HuggingFace BLIP model while providing significant speed up. If you are interested in further details of the conversion process, check out [BLIP](../BLIP) and [Vison2Seq/trt.py](../Vision2Seq/trt.py). You will find that all the Vision2Seq models could be treated in a similar way!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
