{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64974d33-d028-440c-86fa-1a0633b3d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-FileCopyrightText: Copyright (c) 1993-2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f0ff46-9958-4d57-9067-a64be34e75da",
   "metadata": {},
   "source": [
    "##### <img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# BART Playground\n",
    "\n",
    "This notebook demonstrates BART model on the task of text summarization and mask filling.\n",
    "\n",
    "The TensorRT HuggingFace BART model is a plug-in replacement for the original PyTorch modules in HuggingFace BART model.\n",
    "\n",
    "**Notes**: \n",
    " - For \"CPU - PyTorch\" and \"GPU - PyTorch\", a BART-base model from HuggingFace model repository is employed. Inference is carried out in FP32 for CPU-PyTorch, and FP16 for GPU-PyTorch and TensorRT. All models run with batch size 1.\n",
    "Average run time across 5 runs is reported.\n",
    " - Prior to running this notebook, run [bart.ipynb](bart.ipynb) to download the BART model and generate the TensorRT engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a005d22d-5b54-4e0c-866e-6eee6a6f98e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "model_selection = widgets.RadioButtons(\n",
    "    options=['facebook/bart-base', \n",
    "             'facebook/bart-large', \n",
    "             'facebook/bart-large-cnn', \n",
    "             'facebook/mbart-large-50'],\n",
    "    description='Model:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "display(model_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a33fd-4e85-4a1e-9989-af5adf903f79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import torch \n",
    "\n",
    "# huggingface\n",
    "from transformers import (\n",
    "    AutoModelForPreTraining,\n",
    "    AutoTokenizer,\n",
    "    MBartForConditionalGeneration, \n",
    "    MBart50Tokenizer,\n",
    "    AutoConfig,\n",
    ")\n",
    "\n",
    "# download HuggingFace model and tokernizer\n",
    "BART_VARIANT = model_selection.value\n",
    "\n",
    "# mbart variant can't be recognized by HF AutoClass yet\n",
    "if \"mbart\" not in BART_VARIANT:    \n",
    "    bart_model = AutoModelForPreTraining.from_pretrained(BART_VARIANT) # BartForConditionalGeneration\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BART_VARIANT) # BartTokenizer\n",
    "else:\n",
    "    bart_model = MBartForConditionalGeneration.from_pretrained(BART_VARIANT)\n",
    "    tokenizer = MBart50Tokenizer.from_pretrained(BART_VARIANT, src_lang=\"en_XX\")\n",
    "\n",
    "config = AutoConfig.from_pretrained(BART_VARIANT)\n",
    "\n",
    "# load TensorRT engine\n",
    "from BART.trt import BARTTRTEncoder, BARTTRTDecoder, TRTHFRunner\n",
    "from BART.BARTModelConfig import BARTModelTRTConfig, BARTMetadata\n",
    "from BART.export import BARTDecoderTRTEngine, BARTEncoderTRTEngine\n",
    "from NNDF.networks import NetworkMetadata, Precision\n",
    "\n",
    "from transformers.generation_logits_process import (\n",
    "    NoRepeatNGramLogitsProcessor,\n",
    "    MinLengthLogitsProcessor,\n",
    "    ForcedBOSTokenLogitsProcessor,\n",
    "    ForcedEOSTokenLogitsProcessor,\n",
    "    LogitsProcessorList,\n",
    ")\n",
    "from transformers.generation_stopping_criteria import (\n",
    "    MaxLengthCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n",
    "\n",
    "trt_config = AutoConfig.from_pretrained(BART_VARIANT)\n",
    "trt_config.use_cache = False\n",
    "trt_config.num_layers = BARTModelTRTConfig.NUMBER_OF_LAYERS[BART_VARIANT]\n",
    "\n",
    "metadata=NetworkMetadata(variant=BART_VARIANT, precision=Precision(fp16=True), other=BARTMetadata(kv_cache=False))\n",
    "metadata_string = BARTModelTRTConfig().get_metadata_string(metadata)\n",
    "\n",
    "encoder_stem = metadata_string + \"-encoder.onnx\"\n",
    "decoder_stem = metadata_string + \"-decoder-with-lm-head.onnx\"\n",
    "\n",
    "encoder_path = glob.glob(f'./models/{BART_VARIANT}/tensorrt/{encoder_stem}*')[0]\n",
    "decoder_path = glob.glob(f'./models/{BART_VARIANT}/tensorrt/{decoder_stem}*')[0]\n",
    "\n",
    "if not os.path.exists(encoder_path) or not os.path.exists(decoder_path):\n",
    "    print(f\"Error: TensorRT engine not found at ./models/{BART_VARIANT}/tensorrt/. Please run bart.ipynb to generate the TensorRT engines first!\")\n",
    "else:\n",
    "    encoder_engine = BARTEncoderTRTEngine(encoder_path, metadata)\n",
    "    decoder_engine = BARTDecoderTRTEngine(decoder_path, metadata)\n",
    "\n",
    "bart_trt_encoder = BARTTRTEncoder(encoder_engine, metadata, trt_config)\n",
    "bart_trt_decoder = BARTTRTDecoder(decoder_engine, metadata, trt_config)\n",
    "\n",
    "decoder_input_ids = torch.full(\n",
    "    (1, 1), tokenizer.convert_tokens_to_ids(tokenizer.pad_token), dtype=torch.int32\n",
    ").to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b8c94-ba8e-47c8-8624-57da462a0496",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = widgets.RadioButtons(\n",
    "    options=['CPU - PyTorch', \n",
    "             'GPU - PyTorch', \n",
    "             'GPU - TensorRT'],\n",
    "    description='Device:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "task = widgets.RadioButtons(\n",
    "    options=['Summarization', \n",
    "             'Mask Filling', \n",
    "             ],\n",
    "    description='Task:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "example_text = {\n",
    "    task.options[0]:\n",
    "         \"NVIDIA TensorRT-based applications perform up to 36X faster than CPU-only platforms during inference, enabling developers to optimize neural network models trained on all major frameworks, calibrate for lower precision with high accuracy, and deploy to hyperscale data centers, embedded platforms, or automotive product platforms.\",\n",
    "    task.options[1]: \n",
    "         \"My friends are <mask> but they eat too many carbs.\"\n",
    "    }\n",
    "    \n",
    "paragraph_text = widgets.Textarea(\n",
    "    value=example_text[task.options[0]],\n",
    "    placeholder='Type something',\n",
    "    description='Context:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width=\"auto\"),\n",
    "    rows=5,  \n",
    ")\n",
    "\n",
    "generated_text = widgets.Textarea(\n",
    "    value='...',\n",
    "    placeholder='Context',\n",
    "    description='BART output:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width=\"auto\"),\n",
    "    rows=5,\n",
    ")\n",
    "button = widgets.Button(description=\"Generate\")\n",
    "\n",
    "display(paragraph_text)\n",
    "display(generated_text)\n",
    "display(device)\n",
    "display(task)\n",
    "\n",
    "from IPython.display import display\n",
    "box_layout = widgets.Layout(display='flex',\n",
    "                flex_flow='column',\n",
    "                align_items='center',\n",
    "                width='100%')\n",
    "N_RUN = 6\n",
    "progress_bar = widgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=N_RUN,\n",
    "    description='Progress:',\n",
    "    bar_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    style={'bar_color': 'green'},\n",
    "    orientation='horizontal', \n",
    "    layout=widgets.Layout(width='100%', height='50px')\n",
    ")\n",
    "\n",
    "box = widgets.HBox(children=[button],layout=box_layout)\n",
    "output = widgets.Output()\n",
    "display(box)\n",
    "display(progress_bar)\n",
    "display(output)\n",
    "\n",
    "max_output_length = BARTModelTRTConfig.MAX_OUTPUT_LENGTH[BART_VARIANT]\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_output_length)])\n",
    "no_repeat_ngram_size = BARTModelTRTConfig.NO_REPEAT_NGRAM_SIZE\n",
    "min_length = BARTModelTRTConfig.MIN_OUTPUT_LENGTH[BART_VARIANT]\n",
    "logits_processor = LogitsProcessorList([\n",
    "    NoRepeatNGramLogitsProcessor(no_repeat_ngram_size), \n",
    "    MinLengthLogitsProcessor(min_length, tokenizer.convert_tokens_to_ids(tokenizer.eos_token)),\n",
    "    ForcedBOSTokenLogitsProcessor(tokenizer.convert_tokens_to_ids(tokenizer.bos_token)),\n",
    "    ForcedEOSTokenLogitsProcessor(max_output_length, tokenizer.convert_tokens_to_ids(tokenizer.eos_token))\n",
    "])\n",
    "\n",
    "def generate(b):\n",
    "    progress_bar.value = 0\n",
    "    inference_time_arr = []\n",
    "    inputs = tokenizer(paragraph_text.value, return_tensors=\"pt\")\n",
    "    \n",
    "    with output:\n",
    "        if device.value == 'GPU - TensorRT':\n",
    "            for _ in range(N_RUN):\n",
    "                start_time = time.time()\n",
    "                encoder_last_hidden_state = bart_trt_encoder(input_ids=inputs.input_ids)\n",
    "                outputs = bart_trt_decoder.greedy_search(\n",
    "                            input_ids=decoder_input_ids,\n",
    "                            encoder_hidden_states=encoder_last_hidden_state,\n",
    "                            stopping_criteria = stopping_criteria,\n",
    "                            logits_processor=logits_processor,\n",
    "                        )\n",
    "                inference_time_arr.append(time.time()-start_time)\n",
    "                progress_bar.value += 1\n",
    "            print(\"GPU - TensorRT - Average inference time: %.2f (ms)\"%(1000*np.mean(inference_time_arr[1:])))                   \n",
    "                \n",
    "        elif device.value == 'CPU - PyTorch':\n",
    "            for _ in range(N_RUN):\n",
    "                start_time = time.time()\n",
    "                outputs = bart_model.float().to('cpu').generate(inputs.input_ids.to('cpu'), num_beams=1, max_length=max_output_length)\n",
    "                inference_time_arr.append(time.time()-start_time)\n",
    "                progress_bar.value += 1\n",
    "            print(\"CPU - PyTorch - Average inference time: %.2f (ms)\"%(1000*np.mean(inference_time_arr[1:])))\n",
    "            \n",
    "        elif  device.value == 'GPU - PyTorch':  \n",
    "            for _ in range(N_RUN):\n",
    "                start_time = time.time()\n",
    "                outputs = bart_model.half().to('cuda:0').generate(inputs.input_ids.to('cuda:0'), num_beams=1, max_length=max_output_length)\n",
    "                inference_time_arr.append(time.time()-start_time)\n",
    "                progress_bar.value += 1\n",
    "            print(\"GPU - PyTorch - Average inference time: %.2f (ms)\"%(1000*np.mean(inference_time_arr[1:])))    \n",
    "           \n",
    "        # de-tokenize model output to raw text\n",
    "        text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_text.value = text\n",
    "\n",
    "\n",
    "def switch_task(change):\n",
    "    with output:\n",
    "        paragraph_text.value = example_text[task.value]\n",
    "\n",
    "task.observe(switch_task, 'value')\n",
    "\n",
    "button.on_click(generate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
