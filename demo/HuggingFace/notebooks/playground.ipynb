{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64974d33-d028-440c-86fa-1a0633b3d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-FileCopyrightText: Copyright (c) 1993-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f0ff46-9958-4d57-9067-a64be34e75da",
   "metadata": {},
   "source": [
    "##### <img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Playground\n",
    "\n",
    "This notebook demonstrates HuggingFace models on a variety of NLP tasks. You can select the model from a list of pretrained models from HuggingFace and convert them into TRT engines to speed up decoding, and run any customized prompts. Even if a model is not in the list, it is highly possible that TRT can run it! Let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a33fd-4e85-4a1e-9989-af5adf903f79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "from BART.frameworks import BARTHF\n",
    "from BART.trt import BARTTRT\n",
    "\n",
    "from T5.frameworks import T5HF\n",
    "from T5.trt import T5TRT\n",
    "\n",
    "from GPT2.frameworks import GPT2HF\n",
    "from GPT2.trt import GPT2TRT\n",
    "\n",
    "from BLOOM.frameworks import BLOOMHF\n",
    "from BLOOM.trt import BLOOMTRT\n",
    "\n",
    "from OPT.frameworks import OPTHF\n",
    "from OPT.trt import OPTTRT\n",
    "\n",
    "from Seq2Seq.frameworks import Seq2SeqHF\n",
    "from Seq2Seq.trt import Seq2SeqTRT\n",
    "\n",
    "import ipywidgets as widgets\n",
    "widget_style = {'description_width': 'initial'}\n",
    "widget_layout = widgets.Layout(width='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a005d22d-5b54-4e0c-866e-6eee6a6f98e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPPORT_MODELS = [\"BART\", \"T5\", \"GPT2\", \"BLOOM\", \"OPT\", \"Other Seq2Seq\"]\n",
    "\n",
    "BART_VARIANTS = [\n",
    "    'facebook/bart-base', \n",
    "    'facebook/bart-large', \n",
    "    'facebook/bart-large-cnn', \n",
    "    'facebook/mbart-large-50'\n",
    "]\n",
    "\n",
    "GPT2_VARIANTS = [\n",
    "    \"gpt2\",\n",
    "    \"gpt2-medium\",\n",
    "    \"gpt2-large\",\n",
    "    \"gpt2-xl\",\n",
    "    \"EleutherAI/gpt-neo-125m\",\n",
    "    \"EleutherAI/gpt-neo-1.3B\",\n",
    "    \"EleutherAI/gpt-neo-2.7B\",\n",
    "    \"EleutherAI/gpt-j-6b\",\n",
    "    \"EleutherAI/gpt-neox-20b\",\n",
    "    \"cerebras/Cerebras-GPT-111M\",\n",
    "    \"cerebras/Cerebras-GPT-256M\",\n",
    "    \"cerebras/Cerebras-GPT-1.3B\",\n",
    "    \"cerebras/Cerebras-GPT-2.7B\",\n",
    "    \"cerebras/Cerebras-GPT-6.7B\",\n",
    "    \"cerebras/Cerebras-GPT-13B\",\n",
    "]\n",
    "\n",
    "T5_VARIANTS = [\n",
    "    \"t5-small\",\n",
    "    \"t5-base\",\n",
    "    \"t5-large\",\n",
    "    \"t5-3b\",\n",
    "    \"t5-11b\",\n",
    "    \"google/flan-t5-small\",\n",
    "    \"google/flan-t5-base\",\n",
    "    \"google/flan-t5-large\",\n",
    "    \"google/flan-t5-xl\",\n",
    "    \"google/flan-t5-xxl\",\n",
    "]\n",
    "\n",
    "BLOOM_VARIANTS = [\n",
    "    \"bigscience/bloom-560m\",\n",
    "    \"bigscience/bloom-1b1\",\n",
    "    \"bigscience/bloom-1b7\",\n",
    "    \"bigscience/bloom-3b\",\n",
    "    \"bigscience/bloom-7b1\",\n",
    "    \"bigscience/bloomz-560m\",\n",
    "    \"bigscience/bloomz-1b1\",\n",
    "    \"bigscience/bloomz-1b7\",\n",
    "    \"bigscience/bloomz-3b\",\n",
    "    \"bigscience/bloomz-7b1\",\n",
    "]\n",
    "\n",
    "OPT_VARIANTS = [\n",
    "    \"facebook/opt-125m\",\n",
    "    \"facebook/opt-350m\",\n",
    "    \"facebook/opt-1.3b\",\n",
    "    \"facebook/opt-2.7b\",\n",
    "    \"facebook/opt-6.7b\",\n",
    "    \"facebook/opt-13b\",\n",
    "]\n",
    "\n",
    "VARIANTS = {\n",
    "    \"BART\": BART_VARIANTS,\n",
    "    \"T5\": T5_VARIANTS,\n",
    "    \"GPT2\": GPT2_VARIANTS,\n",
    "    \"BLOOM\": BLOOM_VARIANTS,\n",
    "    \"OPT\": OPT_VARIANTS\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8b452",
   "metadata": {},
   "source": [
    "\n",
    "## Select your model!\n",
    "\n",
    "You may want to \n",
    "1. Select the model variant from a list of supported models of our demo. You can also run some other Seq2Seq models from [HuggingFace](https://huggingface.co/), just put the model name in the text book and see if there is surprise!\n",
    "2. Select the model configurations. If you have run our notebooks and command line, you will know that we have the following configs:\n",
    "- `use_cache`: kv cache to speed decoding\n",
    "- `num_beams`: beam search for better results\n",
    "- `fp16`: Using float16 to speed decoding\n",
    "- `batch_size`: batch size for the inputs.\n",
    "- `cpu`: Only affects PyTorch model. If `cpu` is specified, PyTorch model will run on CPU instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aaf5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection_widget = widgets.RadioButtons(\n",
    "    options=SUPPORT_MODELS,\n",
    "    description='Please select a model and variant:',\n",
    "    disabled=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "variant_selection_widgets = {\n",
    "    \"BART\": widgets.RadioButtons(\n",
    "        options=BART_VARIANTS,\n",
    "        description='BART:',\n",
    "        disabled=False,\n",
    "        style=widget_style,\n",
    "        layout=widget_layout\n",
    "    ),\n",
    "    \"T5\": widgets.RadioButtons(\n",
    "        options=T5_VARIANTS,\n",
    "        description='T5:',\n",
    "        disabled=True,\n",
    "        style=widget_style,\n",
    "        layout=widget_layout\n",
    "    ),\n",
    "    \"GPT2\": widgets.RadioButtons(\n",
    "        options=GPT2_VARIANTS,\n",
    "        description='GPT2:',\n",
    "        disabled=True,\n",
    "        style=widget_style,\n",
    "        layout=widget_layout\n",
    "    ),\n",
    "    \"BLOOM\": widgets.RadioButtons(\n",
    "        options=BLOOM_VARIANTS,\n",
    "        description='BLOOM:',\n",
    "        disabled=True,\n",
    "        style=widget_style,\n",
    "        layout=widget_layout\n",
    "    ),\n",
    "    \"OPT\": widgets.RadioButtons(\n",
    "        options=OPT_VARIANTS,\n",
    "        description='OPT:',\n",
    "        disabled=True,\n",
    "        style=widget_style,\n",
    "        layout=widget_layout\n",
    "    ),\n",
    "}\n",
    "\n",
    "def display_model_selection(change):\n",
    "    if change[\"new\"] == \"Other Seq2Seq\":\n",
    "        # Disable all selection if user choose other models\n",
    "        for i in variant_selection_widgets:\n",
    "            variant_selection_widgets[i].disabled = True\n",
    "    else:\n",
    "        if change[\"old\"] != \"Other Seq2Seq\":\n",
    "            hidden_widget = variant_selection_widgets[change[\"old\"]]\n",
    "            hidden_widget.disabled = True\n",
    "        display_widget = variant_selection_widgets[change[\"new\"]]\n",
    "        display_widget.disabled = False\n",
    "\n",
    "model_selection_widget.observe(display_model_selection, names='value')\n",
    "\n",
    "variant_hbox = widgets.HBox(\n",
    "    [variant_selection_widgets[i] for i in variant_selection_widgets],\n",
    ")\n",
    "\n",
    "model_variant_text = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Default = None',\n",
    "    description='Not in the list?',\n",
    "    disabled=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "\n",
    "fp16_widget = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='fp16',\n",
    "    disabled=False,\n",
    "    indent=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "cache_widget = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Use KV Cache',\n",
    "    disabled=False,\n",
    "    indent=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "batch_size_widget = widgets.BoundedIntText(\n",
    "    value=1,\n",
    "    min=1,\n",
    "    max=100000,\n",
    "    step=1,\n",
    "    description='Batch size',\n",
    "    disabled=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "num_beam_widget = widgets.BoundedIntText(\n",
    "    value=3,\n",
    "    min=1,\n",
    "    max=100000,\n",
    "    step=1,\n",
    "    description='Number of beams',\n",
    "    disabled=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "cpu_widget = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Use CPU for PyTorch',\n",
    "    disabled=False,\n",
    "    indent=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout\n",
    ")\n",
    "\n",
    "widgets_all = widgets.VBox([\n",
    "    model_selection_widget,\n",
    "    variant_hbox,\n",
    "    model_variant_text,\n",
    "    fp16_widget, \n",
    "    cache_widget,\n",
    "    batch_size_widget,\n",
    "    num_beam_widget,\n",
    "    cpu_widget\n",
    "])\n",
    "\n",
    "display(widgets_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974b6b44",
   "metadata": {},
   "source": [
    "## Build TRT Engine\n",
    "\n",
    "Same as other [notebooks](.), you will need to call the APIs to build TRT model. All the PyTorch, ONNX and TRT models will be stored in [models](./models) folder for you to inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c77b557-75ed-4946-b630-ee054347d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(model, args):\n",
    "    if model == \"BART\":\n",
    "        torch_model = BARTHF(**args)\n",
    "        trt_model = BARTTRT(**args)\n",
    "    elif model == \"T5\":\n",
    "        torch_model = T5HF(**args)\n",
    "        trt_model = T5TRT(**args)\n",
    "    elif model == \"GPT2\":\n",
    "        torch_model = GPT2HF(**args)\n",
    "        trt_model = GPT2TRT(**args)\n",
    "    elif model == \"BLOOM\":\n",
    "        torch_model = BLOOMHF(**args)\n",
    "        trt_model = BLOOMTRT(**args)\n",
    "    elif model == \"OPT\":\n",
    "        torch_model = OPTHF(**args)\n",
    "        trt_model = OPTTRT(**args)\n",
    "    else:\n",
    "        torch_model = Seq2SeqHF(**args)\n",
    "        trt_model = Seq2SeqTRT(**args)\n",
    "    return torch_model, trt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f7ea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_selection_widget.value\n",
    "customized_variant = model_variant_text.value\n",
    "if model == \"Other Seq2Seq\":\n",
    "    variant = customized_variant\n",
    "    assert variant != '', \"Please specify a model variant for the demo\"\n",
    "else:\n",
    "    selected_variant = variant_selection_widgets[model].value\n",
    "    variant = selected_variant if customized_variant == '' else customized_variant\n",
    "    \n",
    "args = {\n",
    "    \"variant\": variant, \n",
    "    \"use_cache\": cache_widget.value, \n",
    "    \"fp16\": fp16_widget.value,\n",
    "    \"num_beams\": num_beam_widget.value, \n",
    "    \"batch_size\": batch_size_widget.value, \n",
    "    \"working_dir\": \"models\",\n",
    "    \"info\": True,\n",
    "    \"iterations\": 10,\n",
    "    \"number\": 1,\n",
    "    \"warmup\": 3,\n",
    "    \"duration\": 0,\n",
    "    \"percentile\": 50,\n",
    "    \"cpu\": cpu_widget.value,\n",
    "}\n",
    "\n",
    "\n",
    "torch_model, trt_model = select_model(model, args)\n",
    "torch_model.models = torch_model.setup_tokenizer_and_model()\n",
    "trt_model.models = trt_model.setup_tokenizer_and_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668d71ff",
   "metadata": {},
   "source": [
    "\n",
    "## Run Inference!\n",
    "\n",
    "Now it's time to play with the tasks. Each model has some tasks that they are able to complete. If you are not satisfied with the examples we provide, you can give them your own prompt. Enjoy playing with the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b8c94-ba8e-47c8-8624-57da462a0496",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    'Text Generation',\n",
    "    'Summarize', \n",
    "    'Mask Filling',\n",
    "    'Text Classification',\n",
    "    'Translate English to German',\n",
    "    'Translate English to French',\n",
    "    'Other',\n",
    "]\n",
    "\n",
    "tasks_per_model = {\n",
    "    \"BART\": [False, True, True, False, False, False, True],\n",
    "    \"T5\": [False, True, False, True, True, True, True],\n",
    "    \"GPT2\": [True, False, False, False, False, False, True],\n",
    "    \"BLOOM\": [True, False, False, False, False, False, True],\n",
    "    \"OPT\": [True, False, False, False, False, False, True],\n",
    "    \"Other Seq2Seq\": [True, True, True, True, True, True, True]\n",
    "}\n",
    "\n",
    "valid_tasks = [tasks[i] for i in range(len(tasks)) if tasks_per_model[model][i]]\n",
    "\n",
    "task_widget = widgets.RadioButtons(\n",
    "    options=valid_tasks,\n",
    "    description='Task:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "example_text = {\n",
    "    tasks[0]:\n",
    "        \"TensorRT is a machine learning inference accelerator.\",\n",
    "    tasks[1]:\n",
    "        \"NVIDIA TensorRT-based applications perform up to 36X faster than CPU-only platforms during inference, enabling developers to optimize neural network models trained on all major frameworks, calibrate for lower precision with high accuracy, and deploy to hyperscale data centers, embedded platforms, or automotive product platforms.\",\n",
    "    tasks[2]:\n",
    "        \"My friends are <mask> but they eat too many carbs.\",\n",
    "    tasks[3]:\n",
    "        \"premise: I do not like vegetable. hypothesis: I like eating lettuce a lot.\",\n",
    "    tasks[4]:\n",
    "        \"TensorRT is a machine learning inference accelerator.\",\n",
    "    tasks[5]:\n",
    "        \"TensorRT is a machine learning inference accelerator.\",\n",
    "    tasks[6]:\n",
    "        \"What is inside your mind?\"\n",
    "}\n",
    "\n",
    "framework_widget = widgets.RadioButtons(\n",
    "    options=['PyTorch', \n",
    "             'TensorRT'],\n",
    "    description='Framework:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "paragraph_text = widgets.Textarea(\n",
    "    value=example_text[tasks[0]],\n",
    "    placeholder='Type something',\n",
    "    description='Input:',\n",
    "    disabled=False,\n",
    "    style=widget_style,\n",
    "    layout=widget_layout,\n",
    "    rows=5,  \n",
    ")\n",
    "\n",
    "generated_text = widgets.Textarea(\n",
    "    value='...',\n",
    "    placeholder='...',\n",
    "    description='Model output:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width=\"auto\"),\n",
    "    rows=5,\n",
    ")\n",
    "button = widgets.Button(description=\"Generate\")\n",
    "output = widgets.Output()\n",
    "\n",
    "display(task_widget)\n",
    "display(framework_widget)\n",
    "\n",
    "display(paragraph_text)\n",
    "display(generated_text)\n",
    "display(output)\n",
    "\n",
    "display(button)\n",
    "\n",
    "def switch_task(change):\n",
    "    with output:\n",
    "        paragraph_text.value = example_text[task_widget.value]\n",
    "\n",
    "task_widget.observe(switch_task, 'value')\n",
    "\n",
    "\n",
    "def generate(b):\n",
    "    task = task_widget.value\n",
    "    input_str = paragraph_text.value\n",
    "    if task == \"Translate English to German\":\n",
    "        input_str = \"translate English to German: \" + input_str\n",
    "    elif task == \"Translate English to French\":\n",
    "        input_str = \"translate English to French: \" + input_str\n",
    "    elif task == \"Summarize\":\n",
    "        input_str = \"summarize: \" + input_str\n",
    "    \n",
    "    framework = framework_widget.value\n",
    "    if framework == 'PyTorch':\n",
    "        model = torch_model\n",
    "    elif framework == 'TensorRT':\n",
    "        model = trt_model\n",
    "    \n",
    "    with output:\n",
    "        # Need to specify device.\n",
    "        use_cuda = not (framework == 'PyTorch' and cpu_widget.value)\n",
    "        _, text = model.generate(input_str=input_str, use_cuda=use_cuda)\n",
    "        generated_text.value = text\n",
    "\n",
    "\n",
    "button.on_click(generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d2ea69-c966-4e13-a1c1-c9ba68fa6200",
   "metadata": {},
   "source": [
    "## Performance Benchmarking\n",
    "\n",
    "We are curious on how much acceleration TRT could provide to the e2e generation. We can run the demo in benchmarking mode, which uses randomized inputs/outputs with user-specified length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6372f-9865-4cee-9de5-e99f29215f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change these benchmarking parameters, or loop through multiple configs\n",
    "model = model\n",
    "variant = variant\n",
    "input_seq_len = 256\n",
    "output_seq_len = 512\n",
    "num_beams = 1\n",
    "batch_size = 1\n",
    "\n",
    "model_dir = {\"torch\": {}, \"trt\": {}}\n",
    "result_dir = {\"torch\": {}, \"trt\": {}}\n",
    "print(f\"Running variant:{variant}, batch_size={batch_size}, num_beams={num_beams}, input_seq_len={input_seq_len}, output_seq_len={output_seq_len}\")\n",
    "for use_cache in [False, True]:\n",
    "    for precision in [\"fp32\", \"fp16\"]:\n",
    "        metadata = f\"use_cache:{use_cache}-precision:{precision}\"\n",
    "        print(\"Running:\", metadata)\n",
    "        args = {\n",
    "            \"variant\": variant, \n",
    "            \"use_cache\": use_cache, \n",
    "            \"fp16\": True if precision == \"fp16\" else False,\n",
    "            \"num_beams\": num_beams, \n",
    "            \"batch_size\": batch_size, \n",
    "            \"working_dir\": \"models\",\n",
    "            \"info\": True,\n",
    "            \"iterations\": 10,\n",
    "            \"number\": 1,\n",
    "            \"warmup\": 3,\n",
    "            \"duration\": 0,\n",
    "            \"percentile\": 50,\n",
    "            \"benchmarking_mode\": True,\n",
    "            \"input_seq_len\": input_seq_len,\n",
    "            \"output_seq_len\": output_seq_len,\n",
    "        }\n",
    "        torch_model, trt_model = select_model(model, args)\n",
    "        model_dir[\"torch\"][metadata] = torch_model\n",
    "        model_dir[\"trt\"][metadata] = trt_model\n",
    "        result_dir[\"torch\"][metadata] = torch_model.run()\n",
    "        result_dir[\"trt\"][metadata] = trt_model.run()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb581ed-6536-4567-82f7-b768c80335bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "def process_results(result_dir):\n",
    "    headers = None\n",
    "    rows = []\n",
    "    for use_cache in [False, True]:\n",
    "        for precision in [\"fp32\", \"fp16\"]:\n",
    "            metadata = f\"use_cache:{use_cache}-precision:{precision}\"\n",
    "            torch_result = result_dir[\"torch\"][metadata][0].median_runtime\n",
    "            trt_result = result_dir[\"trt\"][metadata][0].median_runtime\n",
    "            if headers is None:\n",
    "                headers = [\"use_cache\", \"precision\", \"framework\"] + [i.name for i in torch_result]\n",
    "            torch_entry = [use_cache, precision, \"torch\"] + [i.runtime for i in torch_result]\n",
    "            trt_entry = [use_cache, precision, \"trt\"] + [i.runtime for i in trt_result]\n",
    "            rows.append(torch_entry)\n",
    "            rows.append(trt_entry)\n",
    "    return rows, headers\n",
    "\n",
    "rows, headers = process_results(result_dir)\n",
    "print(tabulate(rows, headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd795f33-5303-45ae-870d-267b8fd60027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
