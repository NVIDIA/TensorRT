{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "herbal-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-dakota",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# BERT QA Inference with TensorRT FP16\n",
    "\n",
    "\n",
    "Bidirectional Embedding Representations from Transformers ([BERT](https://arxiv.org/abs/1810.04805)) is a method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks. \n",
    "\n",
    "BERT provided a leap in accuracy for NLU tasks that brought high-quality language-based services within the reach of companies across many industries. To use the model in production, you need to consider factors such as latency, in addition to accuracy, which influences end user satisfaction with a service. BERT requires significant compute during inference due to its 12/24-layer stacked multi-head attention network. This has posed a challenge for companies to deploy BERT as part of real-time applications until now.\n",
    "\n",
    "NVIDIA® [TensorRT](https://developer.nvidia.com/tensorrt)™ is an SDK for high-performance deep learning inference. TensorRT provides INT8 and FP16 optimizations for production deployments of deep learning inference applications such as video streaming, speech recognition, recommendation, fraud detection, and natural language processing.\n",
    "TensorRT optimizations for BERT allows you to perform inference in 2.2 ms on T4 GPUs. This is 17x faster than CPU-only platforms and is well within the 10ms latency budget necessary for conversational AI applications. These optimizations make it practical to use BERT in production, for example, as part of a conversation AI service.\n",
    "\n",
    "This notebook demonstrates the inference of BERT models for question and answering applications with TensorRT in FP16 mode.\n",
    "\n",
    "## Pre-requisite\n",
    "Follow the instruction at https://github.com/NVIDIA/TensorRT to build the TensorRT-OSS docker container required to run this notebook.\n",
    "\n",
    "\n",
    "## Content\n",
    "1. [Download data and model](#1)\n",
    "1. [Building a FP16 TensorRT optimized BERT model](#2)\n",
    "1. [Running inference examples](#3)\n",
    "1. [Inference benchmarking](#4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-frequency",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1. Download data and model\n",
    "First, we download the \n",
    "Stanford Question Answering Dataset ([SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)) and a pre-trained BERT QA model from the NVIDIA GPU Cloud ([NGC](https://ngc.nvidia.com/catalog/models/nvidia:bert_pyt_ckpt_base_qa_squad11_amp)).\n",
    "### SQuAD dataset\n",
    "\n",
    "Stanford Question Answering Dataset ([SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "studied-sheffield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading SQuAD-v1.1 training and dev datasets\n",
      "/workspace/TensorRT/demo/BERT/notebooks/squad /workspace/TensorRT/demo/BERT/notebooks\n",
      "--2021-07-06 22:30:44--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
      "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.110.153, 185.199.108.153, 185.199.111.153, ...\n",
      "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.110.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30288272 (29M) [application/json]\n",
      "Saving to: ‘train-v1.1.json’\n",
      "\n",
      "train-v1.1.json     100%[===================>]  28.88M   104MB/s    in 0.3s    \n",
      "\n",
      "2021-07-06 22:30:45 (104 MB/s) - ‘train-v1.1.json’ saved [30288272/30288272]\n",
      "\n",
      "--2021-07-06 22:30:45--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
      "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
      "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4854279 (4.6M) [application/json]\n",
      "Saving to: ‘dev-v1.1.json’\n",
      "\n",
      "dev-v1.1.json       100%[===================>]   4.63M  30.4MB/s    in 0.2s    \n",
      "\n",
      "2021-07-06 22:30:45 (30.4 MB/s) - ‘dev-v1.1.json’ saved [4854279/4854279]\n",
      "\n",
      "/workspace/TensorRT/demo/BERT/notebooks\n"
     ]
    }
   ],
   "source": [
    "!bash ../scripts/download_squad.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-arthur",
   "metadata": {},
   "source": [
    "### Fine-tuned BERT Large Model download\n",
    "\n",
    "Many AI applications have common needs: classification, object detection, language translation, text-to-speech, recommender engines, sentiment analysis, and more. When developing applications with these capabilities, it is much faster to start with a model that is pre-trained and then tune it for a specific use case. The NGC [catalog](https://ngc.nvidia.com/catalog/models) offers pre-trained models for a variety of common AI tasks that are optimized for NVIDIA Tensor Core GPUs, and can be easily re-trained by updating just a few layers, saving valuable time.\n",
    "\n",
    "Herein, we download a pretrained, fine-tuned BERT large model, trained with automatic mixed precision, from NGC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "signed-symposium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/TensorRT/demo/BERT/notebooks/models/fine-tuned /workspace/TensorRT/demo/BERT/notebooks\n",
      "Downloading BERT-tf large checkpoints for sequence length 384 and fine-tuned for SQuAD 2.\n",
      "Downloaded 3.42 GB in 1m 43s, Download speed: 33.96 MB/s               \n",
      "----------------------------------------------------\n",
      "Transfer id: bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1 Download status: Completed.\n",
      "Downloaded local path: /workspace/TensorRT/demo/BERT/notebooks/models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1\n",
      "Total files downloaded: 6 \n",
      "Total downloaded size: 3.42 GB\n",
      "Started at: 2021-07-06 22:30:54.315057\n",
      "Completed at: 2021-07-06 22:32:37.457227\n",
      "Duration taken: 1m 43s\n",
      "----------------------------------------------------\n",
      "/workspace/TensorRT/demo/BERT/notebooks\n"
     ]
    }
   ],
   "source": [
    "!bash ../scripts/download_model.sh large 384 v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5159a2",
   "metadata": {},
   "source": [
    "### Install extra dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ceba10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformers'...\n",
      "remote: Enumerating objects: 75186, done.\u001b[K\n",
      "remote: Total 75186 (delta 0), reused 0 (delta 0), pack-reused 75186\u001b[K\n",
      "Receiving objects: 100% (75186/75186), 58.22 MiB | 22.60 MiB/s, done.\n",
      "Resolving deltas: 100% (53604/53604), done.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Processing /tmp/transformers\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.7.0.dev0) (1.21.0)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 11.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers==4.7.0.dev0) (2.22.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==4.7.0.dev0) (20.9)\n",
      "Collecting huggingface-hub==0.0.8\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.61.2-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 4.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2021.7.6-cp38-cp38-manylinux2014_x86_64.whl (737 kB)\n",
      "\u001b[K     |████████████████████████████████| 737 kB 24.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 25.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers==4.7.0.dev0) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.7.0.dev0) (1.15.0)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "\u001b[K     |████████████████████████████████| 303 kB 23.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.0.1-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 6.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.7.0.dev0-py3-none-any.whl size=2473024 sha256=c4e933006374b20cc8171e647bd97644296aa0f31cdacc8607e5af12d53f6ebf\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-a8mlgf8a/wheels/f0/fa/68/0d01fb6395eb4274ca5ee9138521c58694b7aea2a66a1c41b3\n",
      "Successfully built transformers\n",
      "Installing collected packages: tqdm, regex, joblib, filelock, click, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/home/trtuser/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script sacremoses is installed in '/home/trtuser/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/trtuser/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script transformers-cli is installed in '/home/trtuser/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed click-8.0.1 filelock-3.0.12 huggingface-hub-0.0.8 joblib-1.0.1 regex-2021.7.6 sacremoses-0.0.45 tokenizers-0.10.3 tqdm-4.61.2 transformers-4.7.0.dev0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.8.1+cu111\n",
      "  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.1%2Bcu111-cp38-cp38-linux_x86_64.whl (1982.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1982.2 MB 6.6 kB/s  eta 0:00:01    |██████▋                         | 406.9 MB 61.2 MB/s eta 0:00:26     |████████                        | 489.9 MB 73.2 MB/s eta 0:00:21     |██████████████▏                 | 880.2 MB 69.3 MB/s eta 0:00:16     |████████████████████████        | 1480.1 MB 72.4 MB/s eta 0:00:07     |████████████████████████        | 1483.5 MB 72.4 MB/s eta 0:00:07     |████████████████████████▉       | 1535.5 MB 74.9 MB/s eta 0:00:06     |█████████████████████████       | 1546.8 MB 74.9 MB/s eta 0:00:06     |██████████████████████████████▉ | 1910.3 MB 84.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision==0.9.1+cu111\n",
      "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.9.1%2Bcu111-cp38-cp38-linux_x86_64.whl (17.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.6 MB 28.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchaudio===0.8.1\n",
      "  Downloading torchaudio-0.8.1-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 13.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch==1.8.1+cu111) (1.21.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.1+cu111) (3.7.4.3)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.9.1+cu111) (8.1.2)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx and convert-onnx-to-caffe2 are installed in '/home/trtuser/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed torch-1.8.1+cu111 torchaudio-0.8.1 torchvision-0.9.1+cu111\n"
     ]
    }
   ],
   "source": [
    "!cd /tmp && git clone https://github.com/vinhngx/transformers && cd transformers && pip install .\n",
    "!pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-slovakia",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Building a FP16 TensorRT optimized BERT model\n",
    "\n",
    "In this section, we will be optimizing the BERT model for inference with TensorRT using FP16. The overal workflow is as below.\n",
    "\n",
    "<img src=\"Figure-1-generating-bert-trt.png\">\n",
    "\n",
    "To optimize BERT with TensorRT, we focused on optimizing the transformer cell. Since several Transformer cells are stacked in BERT, we were able to achieve significant performance gains through this set of optimizations. We use custom plugins that accelerate key operations in the Transformer Encoder elements in a BERT model. The plugins fuse multiple operations into a sub-graph in a single CUDA kernel.  Each sub-graph consists of several elementary computations, each of which requires a read and write to the global memory of the GPU (i.e. the slowest on-device memory).  By fusing the elementary operations together into a single CUDA kernel we allow for the computation to happen on a larger sub-graph while visiting the global memory a minimal amount of times. \n",
    "<img src=\"Figure-5-optimizations-through-trt.jpg\">\n",
    "\n",
    "For more information, see our developer [blog](https://developer.nvidia.com/blog/nlu-with-tensorrt-bert/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "leading-reliance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT version: 8.0.1.6\n",
     ]
    }
   ],
   "source": [
    "import tensorrt as trt;\n",
    "TRT_VERSION = trt.__version__\n",
    "\n",
    "print(\"TensorRT version: {}\".format(TRT_VERSION))\n",
    "!mkdir -p engines_$TRT_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "studied-profession",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum TensorRT inference batch size\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unique-batman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-06 22:34:46.653747: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "[TensorRT] INFO: Using configuration file: models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/bert_config.json\n",
      "[TensorRT] INFO: Found 394 entries in weight map\n",
      "[TensorRT] INFO: [MemUsageChange] Init CUDA: CPU +491, GPU +0, now: CPU 3310, GPU 863 (MiB)\n",
      "../builder.py:435: DeprecationWarning: Use build_serialized_network instead.\n",
      "  engine = builder.build_engine(network, builder_config)\n",
      "[TensorRT] INFO: [MemUsageSnapshot] Builder begin: CPU 3634 MiB, GPU 1175 MiB\n",
      "[TensorRT] INFO: [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +747, GPU +318, now: CPU 4682, GPU 1793 (MiB)\n",
      "[TensorRT] WARNING: Detected invalid timing cache, setup a local cache instead\n",
      "[TensorRT] INFO: Detected 3 inputs and 1 output network tensors.\n",
      "[TensorRT] INFO: Total Host Persistent Memory: 189536\n",
      "[TensorRT] INFO: Total Device Persistent Memory: 352989696\n",
      "[TensorRT] INFO: Total Scratch Memory: 4194304\n",
      "[TensorRT] INFO: [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 344 MiB, GPU 8 MiB\n",
      "[TensorRT] INFO: [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 5323, GPU 2635 (MiB)\n",
      "[TensorRT] INFO: [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 5323, GPU 2627 (MiB)\n",
      "[TensorRT] INFO: [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 5323, GPU 2619 (MiB)\n",
      "[TensorRT] INFO: [MemUsageSnapshot] Builder end: CPU 5022 MiB, GPU 2319 MiB\n",
      "[TensorRT] INFO: build engine in 609.265 Sec\n",
      "[TensorRT] INFO: Saving Engine to engines_8.0.1.6/bert_large_384.engine\n",
      "[TensorRT] INFO: Done.\n"
     ]
    }
   ],
   "source": [
    "# Build BERT TensorRT FP16 model from NGC checkpoint\n",
    "!python3 ../builder.py -m models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/model.ckpt -w 40000 -o engines_$TRT_VERSION/bert_large_384.engine -b $BATCH_SIZE -s 384 --fp16 -c models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-assignment",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. Running inference examples\n",
    "\n",
    "Now that we've got a TensorRT engine, the inference workflow using the optimized network is as follows:\n",
    "\n",
    " -   Start the TensorRT runtime with this engine.\n",
    " -   Feed a passage and a question to the TensorRT runtime and receive as output the answer predicted by the network.\n",
    "\n",
    "<img src=\"Figure-2-workflow-to-perform-inference-with-trt.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "incorporate-psychiatry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT] INFO: [MemUsageChange] Init CUDA: CPU +492, GPU +0, now: CPU 516, GPU 864 (MiB)\n",
      "[TensorRT] INFO: Loaded engine size: 1841 MB\n",
      "[TensorRT] INFO: [MemUsageSnapshot] deserializeCudaEngine begin: CPU 2357 MiB, GPU 864 MiB\n",
      "[TensorRT] INFO: [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +8, now: CPU 3431, GPU 2026 (MiB)\n",
      "[TensorRT] INFO: [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 3431, GPU 2018 (MiB)\n",
      "[TensorRT] INFO: [MemUsageSnapshot] deserializeCudaEngine end: CPU 3431 MiB, GPU 2018 MiB\n",
      "[TensorRT] INFO: [MemUsageSnapshot] ExecutionContext creation begin: CPU 1589 MiB, GPU 2018 MiB\n",
      "[TensorRT] INFO: [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 1589, GPU 2026 (MiB)\n",
      "[TensorRT] INFO: [MemUsageSnapshot] ExecutionContext creation end: CPU 1890 MiB, GPU 3340 MiB\n",
      "../inference.py:142: DeprecationWarning: Use set_optimization_profile_async instead.\n",
      "  context.active_optimization_profile = selected_profile\n",
      "\n",
      "Passage: TensorRT is a high performance deep learning inference platform that delivers low latency and high throughput for appssuch as recommenders, speech and image/video on NVIDIA GPUs. It includes parsers to import models, and plugins to support novel opsand layers before applying optimizations for inference. Today NVIDIA is open-sourcing parsers and plugins in TensorRT so that the deeplearning community can customize and extend these components to take advantage of powerful TensorRT optimizations for your apps.\n",
      "\n",
      "Question: What is TensorRT?\n",
      "------------------------\n",
      "Running inference in 56.593 Sentences/Sec\n",
      "------------------------\n",
      "Answer: 'a high performance deep learning inference platform'\n",
      "With probability: 51.597\n",
      "[TensorRT] INFO: [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 2085, GPU 4215 (MiB)\n"
     ]
    }
   ],
   "source": [
    "PASSAGE = 'TensorRT is a high performance deep learning inference platform that delivers low latency and high throughput for apps'\\\n",
    "'such as recommenders, speech and image/video on NVIDIA GPUs. It includes parsers to import models, and plugins to support novel ops'\\\n",
    "'and layers before applying optimizations for inference. Today NVIDIA is open-sourcing parsers and plugins in TensorRT so that the deep'\\\n",
    "'learning community can customize and extend these components to take advantage of powerful TensorRT optimizations for your apps.'\n",
    "QUESTION=\"What is TensorRT?\"\n",
    "\n",
    "!python3 ../inference.py -e engines_$TRT_VERSION/bert_large_384.engine -s 384 -p $PASSAGE -q $QUESTION -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340fa7fb-d997-4fbf-b3bf-684085584c94",
   "metadata": {},
   "source": [
    "Let's ask a different question. Feel free to plugin your own question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "legal-brief",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT] INFO: [MemUsageChange] Init CUDA: CPU +492, GPU +0, now: CPU 516, GPU 864 (MiB)\n",
      "[TensorRT] INFO: Loaded engine size: 1841 MB\n",
      "[TensorRT] INFO: [MemUsageSnapshot] deserializeCudaEngine begin: CPU 2357 MiB, GPU 864 MiB\n",
      "[TensorRT] INFO: [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +8, now: CPU 3431, GPU 2026 (MiB)\n",
      "[TensorRT] INFO: [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 3431, GPU 2018 (MiB)\n",
      "[TensorRT] INFO: [MemUsageSnapshot] deserializeCudaEngine end: CPU 3431 MiB, GPU 2018 MiB\n",
      "[TensorRT] INFO: [MemUsageSnapshot] ExecutionContext creation begin: CPU 1589 MiB, GPU 2018 MiB\n",
      "[TensorRT] INFO: [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 1589, GPU 2026 (MiB)\n",
      "[TensorRT] INFO: [MemUsageSnapshot] ExecutionContext creation end: CPU 1890 MiB, GPU 3340 MiB\n",
      "../inference.py:142: DeprecationWarning: Use set_optimization_profile_async instead.\n",
      "  context.active_optimization_profile = selected_profile\n",
      "\n",
      "Passage: TensorRT is a high performance deep learning inference platform that delivers low latency and high throughput for appssuch as recommenders, speech and image/video on NVIDIA GPUs. It includes parsers to import models, and plugins to support novel opsand layers before applying optimizations for inference. Today NVIDIA is open-sourcing parsers and plugins in TensorRT so that the deeplearning community can customize and extend these components to take advantage of powerful TensorRT optimizations for your apps.\n",
      "\n",
      "Question: What is included in TensorRT?\n",
      "------------------------\n",
      "Running inference in 58.978 Sentences/Sec\n",
      "------------------------\n",
      "Answer: 'parsers to import models, and plugins'\n",
      "With probability: 30.461\n",
      "[TensorRT] INFO: [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 2085, GPU 4215 (MiB)\n"
     ]
    }
   ],
   "source": [
    "QUESTION=\"What is included in TensorRT?\"\n",
    "\n",
    "!python3 ../inference.py -e engines_$TRT_VERSION/bert_large_384.engine -s 384 -p $PASSAGE -q $QUESTION -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-simpson",
   "metadata": {},
   "source": [
    "## Validation on the SQuAD dev set\n",
    "Next, we will assess the accuracy of the TensorRT-optimized FP16 BERT model on the SQuAD dev set. \n",
    "\n",
    "There are two dominant metrics used by many question answering datasets, including SQuAD: exact match (EM) and F1 score. These scores are computed on individual question+answer pairs. When multiple correct answers are possible for a given question, the maximum score over all possible correct answers is computed. Overall EM and F1 scores are computed for a model by averaging over the individual example scores.\n",
    "\n",
    "### Exact Match\n",
    "\n",
    "This metric is as simple as it sounds. For each question+answer pair, if the characters of the model's prediction exactly match the characters of (one of) the True Answer(s), EM = 1, otherwise EM = 0. This is a strict all-or-nothing metric; being off by a single character results in a score of 0. When assessing against a negative example, if the model predicts any text at all, it automatically receives a 0 for that example.\n",
    "\n",
    "### F1\n",
    "\n",
    "F1 score is a common metric for classification problems, and widely used in QA. It is appropriate when we care equally about precision and recall. In this case, it's computed over the individual words in the prediction against those in the True Answer. The number of shared words between the prediction and the truth is the basis of the F1 score: precision is the ratio of the number of shared words to the total number of words in the prediction, and recall is the ratio of the number of shared words to the total number of words in the ground truth.\n",
    "\n",
    "For more info, see [reference](https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html#Metrics-for-QA).\n",
    "\n",
    "Herein, we verify that the TensorRT model achieves a state-of-the-art accuracy of 90% F1 score on the SQuAD development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "loose-musical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT] INFO: [MemUsageChange] Init CUDA: CPU +491, GPU +0, now: CPU 518, GPU 864 (MiB)\n",
      "[TensorRT] INFO: Loaded engine size: 1841 MB\n",
      "[TensorRT] INFO: [MemUsageSnapshot] deserializeCudaEngine begin: CPU 2360 MiB, GPU 864 MiB\n",
      "[TensorRT] INFO: [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 3433, GPU 2026 (MiB)\n",
      "[TensorRT] INFO: [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 3433, GPU 2018 (MiB)\n",
      "[TensorRT] INFO: [MemUsageSnapshot] deserializeCudaEngine end: CPU 3433 MiB, GPU 2018 MiB\n",
      "[TensorRT] INFO: [MemUsageSnapshot] ExecutionContext creation begin: CPU 1591 MiB, GPU 2018 MiB\n",
      "[TensorRT] INFO: [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 1591, GPU 2026 (MiB)\n",
      "[TensorRT] INFO: [MemUsageSnapshot] ExecutionContext creation end: CPU 1892 MiB, GPU 3340 MiB\n",
      "../inference.py:142: DeprecationWarning: Use set_optimization_profile_async instead.\n",
      "  context.active_optimization_profile = selected_profile\n",
      "\n",
      "Output dump to ./predictions-bert_large_384.json\n",
      "[TensorRT] INFO: [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 2136, GPU 4215 (MiB)\n",
      "&&&& PASSED TensorRT BERT Squad Accuracy matches reference.\n",
      "{\"exact_match\": 83.83159886471145, \"f1\": 90.95575279682572}\n"
     ]
    }
   ],
   "source": [
    "!python3 ../inference.py -e engines_$TRT_VERSION/bert_large_384.engine -s 384 -sq ./squad/dev-v1.1.json -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt -o ./predictions-bert_large_384.json\n",
    "!python3 ../squad/evaluate-v1.1.py  squad/dev-v1.1.json  ./predictions-bert_large_384.json 90\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-smile",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "\n",
    "## 4. Inference benchmarking\n",
    "BERT can be applied both for online and offline use cases. Online NLU applications, such as conversational AI,  place tight latency budgets during inference. Several models need to execute in a sequence in response to a single user query. When used as a service, the total time a customer experiences includes compute time as well as input and output network latency. Longer times lead to a sluggish performance and a poor customer experience.\n",
    "\n",
    "While the exact latency available for a single model can vary by application, several real-time applications need the language model to execute in under 10 ms. Using a Tesla T4 GPU, BERT optimized with TensorRT can perform inference in 2.2 ms for a QA task similar to available in SQuAD with batch size =1 and sequence length = 128. Using the TensorRT optimized sample, you can execute up to a batch size of 8 for BERT-base and even higher batch sizes for models with fewer Transformer layers within the 10 ms latency budget.  It took 40 ms to execute the same task with highly optimized code on a CPU-only platform for batch size = 1, while higher batch sizes did not run to completion and exit with errors.\n",
    "\n",
    "<img src=\"Figure-6-Compute-latency.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fe9312-7b20-4997-8f00-81b2d8d3ba9c",
   "metadata": {},
   "source": [
    "Next, we will perform a couple of inference benchmarks with different batch sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1\n",
    "!python3 ../perf.py -e ./engines_$TRT_VERSION/bert_large_384.engine -b $BATCH_SIZE -s 384 -i 100 -w 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "!python3 ../perf.py -e ./engines_$TRT_VERSION/bert_large_384.engine -b $BATCH_SIZE -s 384 -i 100 -w 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-museum",
   "metadata": {},
   "source": [
    "### BERT base model\n",
    "\n",
    "We repeat the same process with another BERT model, the BERT-base model (110M parameters) with sequence length 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ../scripts/download_model.sh base 128 v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../builder.py -m models/fine-tuned/bert_tf_ckpt_base_qa_squad2_amp_128_v19.03.1/model.ckpt -w 40000 -o engines_$TRT_VERSION/bert_base_128.engine -b $BATCH_SIZE -s 128 --fp16 -c models/fine-tuned/bert_tf_ckpt_base_qa_squad2_amp_128_v19.03.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../perf.py -e ./engines_$TRT_VERSION/bert_base_128.engine -b 1 -s 128 -i 100 -w 20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
