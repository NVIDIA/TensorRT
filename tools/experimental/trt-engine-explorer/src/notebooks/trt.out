&&&& RUNNING TensorRT.trtexec [TensorRT v8502] # /usr/src/tensorrt/bin/trtexec --int8 --onnx=yolo_pytorch_quantized.onnx --exportTimes=times.json --dumpProfile --exportProfile=profile.json --exportLayerInfo=layer_info.json --verbose
[10/31/2023-17:53:30] [I] === Model Options ===
[10/31/2023-17:53:30] [I] Format: ONNX
[10/31/2023-17:53:30] [I] Model: yolo_pytorch_quantized.onnx
[10/31/2023-17:53:30] [I] Output:
[10/31/2023-17:53:30] [I] === Build Options ===
[10/31/2023-17:53:30] [I] Max batch: explicit batch
[10/31/2023-17:53:30] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default
[10/31/2023-17:53:30] [I] minTiming: 1
[10/31/2023-17:53:30] [I] avgTiming: 8
[10/31/2023-17:53:30] [I] Precision: FP32+INT8
[10/31/2023-17:53:30] [I] LayerPrecisions: 
[10/31/2023-17:53:30] [I] Calibration: Dynamic
[10/31/2023-17:53:30] [I] Refit: Disabled
[10/31/2023-17:53:30] [I] Sparsity: Disabled
[10/31/2023-17:53:30] [I] Safe mode: Disabled
[10/31/2023-17:53:30] [I] DirectIO mode: Disabled
[10/31/2023-17:53:30] [I] Restricted mode: Disabled
[10/31/2023-17:53:30] [I] Build only: Disabled
[10/31/2023-17:53:30] [I] Save engine: 
[10/31/2023-17:53:30] [I] Load engine: 
[10/31/2023-17:53:30] [I] Profiling verbosity: 0
[10/31/2023-17:53:30] [I] Tactic sources: Using default tactic sources
[10/31/2023-17:53:30] [I] timingCacheMode: local
[10/31/2023-17:53:30] [I] timingCacheFile: 
[10/31/2023-17:53:30] [I] Heuristic: Disabled
[10/31/2023-17:53:30] [I] Preview Features: Use default preview flags.
[10/31/2023-17:53:30] [I] Input(s)s format: fp32:CHW
[10/31/2023-17:53:30] [I] Output(s)s format: fp32:CHW
[10/31/2023-17:53:30] [I] Input build shapes: model
[10/31/2023-17:53:30] [I] Input calibration shapes: model
[10/31/2023-17:53:30] [I] === System Options ===
[10/31/2023-17:53:30] [I] Device: 0
[10/31/2023-17:53:30] [I] DLACore: 
[10/31/2023-17:53:30] [I] Plugins:
[10/31/2023-17:53:30] [I] === Inference Options ===
[10/31/2023-17:53:30] [I] Batch: Explicit
[10/31/2023-17:53:30] [I] Input inference shapes: model
[10/31/2023-17:53:30] [I] Iterations: 10
[10/31/2023-17:53:30] [I] Duration: 3s (+ 200ms warm up)
[10/31/2023-17:53:30] [I] Sleep time: 0ms
[10/31/2023-17:53:30] [I] Idle time: 0ms
[10/31/2023-17:53:30] [I] Streams: 1
[10/31/2023-17:53:30] [I] ExposeDMA: Disabled
[10/31/2023-17:53:30] [I] Data transfers: Enabled
[10/31/2023-17:53:30] [I] Spin-wait: Disabled
[10/31/2023-17:53:30] [I] Multithreading: Disabled
[10/31/2023-17:53:30] [I] CUDA Graph: Disabled
[10/31/2023-17:53:30] [I] Separate profiling: Disabled
[10/31/2023-17:53:30] [I] Time Deserialize: Disabled
[10/31/2023-17:53:30] [I] Time Refit: Disabled
[10/31/2023-17:53:30] [I] NVTX verbosity: 0
[10/31/2023-17:53:30] [I] Persistent Cache Ratio: 0
[10/31/2023-17:53:30] [I] Inputs:
[10/31/2023-17:53:30] [I] === Reporting Options ===
[10/31/2023-17:53:30] [I] Verbose: Enabled
[10/31/2023-17:53:30] [I] Averages: 10 inferences
[10/31/2023-17:53:30] [I] Percentiles: 90,95,99
[10/31/2023-17:53:30] [I] Dump refittable layers:Disabled
[10/31/2023-17:53:30] [I] Dump output: Disabled
[10/31/2023-17:53:30] [I] Profile: Enabled
[10/31/2023-17:53:30] [I] Export timing to JSON file: times.json
[10/31/2023-17:53:30] [I] Export output to JSON file: 
[10/31/2023-17:53:30] [I] Export profile to JSON file: profile.json
[10/31/2023-17:53:30] [I] 
[10/31/2023-17:53:30] [I] === Device Information ===
[10/31/2023-17:53:30] [I] Selected Device: Orin
[10/31/2023-17:53:30] [I] Compute Capability: 8.7
[10/31/2023-17:53:30] [I] SMs: 4
[10/31/2023-17:53:30] [I] Compute Clock Rate: 0.765 GHz
[10/31/2023-17:53:30] [I] Device Global Memory: 7755 MiB
[10/31/2023-17:53:30] [I] Shared Memory per SM: 164 KiB
[10/31/2023-17:53:30] [I] Memory Bus Width: 256 bits (ECC disabled)
[10/31/2023-17:53:30] [I] Memory Clock Rate: 0.612 GHz
[10/31/2023-17:53:30] [I] 
[10/31/2023-17:53:30] [I] TensorRT version: 8.5.2
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::BatchedNMSDynamic_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::BatchedNMS_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::BatchTilePlugin_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::Clip_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::CoordConvAC version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::CropAndResizeDynamic version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::CropAndResize version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::DecodeBbox3DPlugin version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::DetectionLayer_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::EfficientNMS_Explicit_TF_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::EfficientNMS_Implicit_TF_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::EfficientNMS_ONNX_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::EfficientNMS_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::FlattenConcat_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::GenerateDetection_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::GridAnchor_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::GridAnchorRect_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::GroupNorm version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::InstanceNormalization_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::InstanceNormalization_TRT version 2
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::LayerNorm version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::LReLU_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::MultilevelCropAndResize_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::MultilevelProposeROI_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::MultiscaleDeformableAttnPlugin_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::NMSDynamic_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::NMS_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::Normalize_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::PillarScatterPlugin version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::PriorBox_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::ProposalDynamic version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::ProposalLayer_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::Proposal version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::PyramidROIAlign_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::Region_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::Reorg_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::ResizeNearest_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::ROIAlign_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::RPROI_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::ScatterND version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::SeqLen2Spatial version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::SpecialSlice_TRT version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::SplitGeLU version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::Split version 1
[10/31/2023-17:53:30] [V] [TRT] Registered plugin creator - ::VoxelGeneratorPlugin version 1
[10/31/2023-17:53:31] [I] [TRT] [MemUsageChange] Init CUDA: CPU +220, GPU +0, now: CPU 249, GPU 2734 (MiB)
[10/31/2023-17:53:31] [V] [TRT] Trying to load shared library libnvinfer_builder_resource.so.8.5.2
[10/31/2023-17:53:31] [V] [TRT] Loaded shared library libnvinfer_builder_resource.so.8.5.2
[10/31/2023-17:53:34] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +302, GPU +285, now: CPU 574, GPU 3040 (MiB)
[10/31/2023-17:53:34] [I] Start parsing network model
[10/31/2023-17:53:34] [I] [TRT] ----------------------------------------------------------------
[10/31/2023-17:53:34] [I] [TRT] Input filename:   yolo_pytorch_quantized.onnx
[10/31/2023-17:53:34] [I] [TRT] ONNX IR version:  0.0.7
[10/31/2023-17:53:34] [I] [TRT] Opset version:    13
[10/31/2023-17:53:34] [I] [TRT] Producer name:    pytorch
[10/31/2023-17:53:34] [I] [TRT] Producer version: 2.0.0
[10/31/2023-17:53:34] [I] [TRT] Domain:           
[10/31/2023-17:53:34] [I] [TRT] Model version:    0
[10/31/2023-17:53:34] [I] [TRT] Doc string:       
[10/31/2023-17:53:34] [I] [TRT] ----------------------------------------------------------------
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::BatchedNMSDynamic_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::BatchedNMS_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::BatchTilePlugin_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::Clip_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::CoordConvAC version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::CropAndResizeDynamic version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::CropAndResize version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::DecodeBbox3DPlugin version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::DetectionLayer_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::EfficientNMS_Explicit_TF_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::EfficientNMS_Implicit_TF_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::EfficientNMS_ONNX_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::EfficientNMS_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::FlattenConcat_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::GenerateDetection_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::GridAnchor_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::GridAnchorRect_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::GroupNorm version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::InstanceNormalization_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::InstanceNormalization_TRT version 2
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::LayerNorm version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::LReLU_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::MultilevelCropAndResize_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::MultilevelProposeROI_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::MultiscaleDeformableAttnPlugin_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::NMSDynamic_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::NMS_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::Normalize_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::PillarScatterPlugin version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::PriorBox_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::ProposalDynamic version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::ProposalLayer_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::Proposal version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::PyramidROIAlign_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::Region_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::Reorg_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::ResizeNearest_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::ROIAlign_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::RPROI_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::ScatterND version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::SeqLen2Spatial version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::SpecialSlice_TRT version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::SplitGeLU version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::Split version 1
[10/31/2023-17:53:34] [V] [TRT] Plugin creator already registered - ::VoxelGeneratorPlugin version 1
[10/31/2023-17:53:34] [V] [TRT] Adding network input: input with dtype: float32, dimensions: (1, 3, 2208, 3872)
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: input for ONNX tensor: input
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.bn1.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.bn1.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.bn1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.bn1.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer1.0.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer1.0.bn1.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer1.0.bn1.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer1.0.bn1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer1.0.bn1.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer1.0.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer1.0.bn2.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer1.0.bn2.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer1.0.bn2.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer1.0.bn2.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer1.1.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer1.1.bn1.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer1.1.bn1.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer1.1.bn1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer1.1.bn1.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer1.1.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer1.1.bn2.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer1.1.bn2.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer1.1.bn2.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer1.1.bn2.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.0.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.0.bn1.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.0.bn1.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.0.bn1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.0.bn1.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.0.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.0.bn2.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.0.bn2.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.0.bn2.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.0.bn2.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.0.downsample.0.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.0.downsample.1.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.0.downsample.1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.0.downsample.1.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.1.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.1.bn1.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.1.bn1.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.1.bn1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.1.bn1.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.1.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.1.bn2.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.1.bn2.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.1.bn2.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer2.1.bn2.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.0.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.0.bn1.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.0.bn1.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.0.bn1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.0.bn1.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.0.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.0.bn2.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.0.bn2.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.0.bn2.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.0.bn2.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.0.downsample.0.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.0.downsample.1.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.0.downsample.1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.0.downsample.1.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.1.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.1.bn1.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.1.bn1.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.1.bn1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.1.bn1.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.1.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.1.bn2.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.1.bn2.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.1.bn2.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer3.1.bn2.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.0.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.0.bn1.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.0.bn1.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.0.bn1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.0.bn1.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.0.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.0.bn2.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.0.bn2.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.0.bn2.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.0.bn2.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.0.downsample.0.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.0.downsample.1.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.0.downsample.1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.0.downsample.1.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.1.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.1.bn1.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.1.bn1.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.1.bn1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.1.bn1.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.1.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.1.bn2.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.1.bn2.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.1.bn2.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: backbone.layer4.1.bn2.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv1.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv1.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv1.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv1.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv1.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv2.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv2.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv2.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv2.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv2.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv3.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv3.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv3.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv3.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv3.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv4.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv4.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv4.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv4.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv4.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv5.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv5.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv5.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv5.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv5.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv6.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv6.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv6.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv6.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv6.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv7.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect1.conv7.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.conv1.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.conv1.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.conv1.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.conv1.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.conv1.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv1.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv1.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv1.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv1.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv1.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv2.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv2.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv2.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv2.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv2.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv3.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv3.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv3.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv3.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv3.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv4.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv4.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv4.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv4.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv4.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv5.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv5.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv5.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv5.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv5.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv6.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv6.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv6.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv6.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv6.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv7.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect2.conv7.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.conv2.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.conv2.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.conv2.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.conv2.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.conv2.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv1.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv1.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv1.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv1.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv1.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv2.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv2.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv2.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv2.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv2.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv3.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv3.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv3.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv3.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv3.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv4.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv4.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv4.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv4.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv4.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv5.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv5.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv5.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv5.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv5.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv6.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv6.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv6.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv6.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv6.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv7.weight
[10/31/2023-17:53:34] [V] [TRT] Importing initializer: neck.detect3.conv7.bias
[10/31/2023-17:53:34] [V] [TRT] Parsing node: Identity_0 [Identity]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.0.bn2.bias
[10/31/2023-17:53:34] [V] [TRT] Identity_0 [Identity] inputs: [backbone.layer4.0.bn2.bias -> (512)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer4.0.bn2.bias for ONNX node: backbone.layer4.0.bn2.bias
[10/31/2023-17:53:34] [V] [TRT] Registering layer: Identity_0 for ONNX node: Identity_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: backbone.layer4.0.downsample.1.bias for ONNX tensor: backbone.layer4.0.downsample.1.bias
[10/31/2023-17:53:34] [V] [TRT] Identity_0 [Identity] outputs: [backbone.layer4.0.downsample.1.bias -> (512)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: Identity_1 [Identity]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.0.bn2.bias
[10/31/2023-17:53:34] [V] [TRT] Identity_1 [Identity] inputs: [backbone.layer3.0.bn2.bias -> (256)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer3.0.bn2.bias for ONNX node: backbone.layer3.0.bn2.bias
[10/31/2023-17:53:34] [V] [TRT] Registering layer: Identity_1 for ONNX node: Identity_1
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: backbone.layer3.0.downsample.1.bias for ONNX tensor: backbone.layer3.0.downsample.1.bias
[10/31/2023-17:53:34] [V] [TRT] Identity_1 [Identity] outputs: [backbone.layer3.0.downsample.1.bias -> (256)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: Identity_2 [Identity]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.0.bn2.bias
[10/31/2023-17:53:34] [V] [TRT] Identity_2 [Identity] inputs: [backbone.layer2.0.bn2.bias -> (128)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer2.0.bn2.bias for ONNX node: backbone.layer2.0.bn2.bias
[10/31/2023-17:53:34] [V] [TRT] Registering layer: Identity_2 for ONNX node: Identity_2
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: backbone.layer2.0.downsample.1.bias for ONNX tensor: backbone.layer2.0.downsample.1.bias
[10/31/2023-17:53:34] [V] [TRT] Identity_2 [Identity] outputs: [backbone.layer2.0.downsample.1.bias -> (128)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/conv1/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/conv1/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/conv1/_input_quantizer/Constant [Constant] outputs: [/backbone/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/conv1/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/conv1/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/conv1/_input_quantizer/Constant_1 [Constant] outputs: [/backbone/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: input
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [input -> (1, 3, 2208, 3872)[FLOAT]], [/backbone/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/conv1/_input_quantizer/Constant_1_output_0 for ONNX node: /backbone/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/conv1/_input_quantizer/Constant_output_0 for ONNX node: /backbone/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/conv1/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/conv1/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/conv1/_input_quantizer/QuantizeLinear_output_0 -> (1, 3, 2208, 3872)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/conv1/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/conv1/_input_quantizer/QuantizeLinear_output_0 -> (1, 3, 2208, 3872)[FLOAT]], [/backbone/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/conv1/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/conv1/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/conv1/_input_quantizer/DequantizeLinear_output_0 -> (1, 3, 2208, 3872)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/conv1/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/conv1/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/conv1/_weight_quantizer/Constant [Constant] outputs: [/backbone/conv1/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/conv1/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/conv1/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/conv1/_weight_quantizer/Constant_1 [Constant] outputs: [/backbone/conv1/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [backbone.conv1.weight -> (64, 3, 7, 7)[FLOAT]], [/backbone/conv1/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], [/backbone/conv1/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.conv1.weight for ONNX node: backbone.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/conv1/_weight_quantizer/Constant_output_0 for ONNX node: /backbone/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/conv1/_weight_quantizer/Constant_1_output_0 for ONNX node: /backbone/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/conv1/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/conv1/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/conv1/_weight_quantizer/QuantizeLinear_output_0 -> (64, 3, 7, 7)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/conv1/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/conv1/_weight_quantizer/QuantizeLinear_output_0 -> (64, 3, 7, 7)[FLOAT]], [/backbone/conv1/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], [/backbone/conv1/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/conv1/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/conv1/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/conv1/_weight_quantizer/DequantizeLinear_output_0 -> (64, 3, 7, 7)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/conv1/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/conv1/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/conv1/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/conv1/Conv [Conv] inputs: [/backbone/conv1/_input_quantizer/DequantizeLinear_output_0 -> (1, 3, 2208, 3872)[FLOAT]], [/backbone/conv1/_weight_quantizer/DequantizeLinear_output_0 -> (64, 3, 7, 7)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/conv1/Conv for ONNX node: /backbone/conv1/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/conv1/Conv_output_0 for ONNX tensor: /backbone/conv1/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/conv1/Conv [Conv] outputs: [/backbone/conv1/Conv_output_0 -> (1, 64, 1104, 1936)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/bn1/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/conv1/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.bn1.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.bn1.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.bn1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.bn1.running_var
[10/31/2023-17:53:34] [V] [TRT] /backbone/bn1/BatchNormalization [BatchNormalization] inputs: [/backbone/conv1/Conv_output_0 -> (1, 64, 1104, 1936)[FLOAT]], [backbone.bn1.weight -> (64)[FLOAT]], [backbone.bn1.bias -> (64)[FLOAT]], [backbone.bn1.running_mean -> (64)[FLOAT]], [backbone.bn1.running_var -> (64)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/bn1/BatchNormalization for ONNX node: /backbone/bn1/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/bn1/BatchNormalization_output_0 for ONNX tensor: /backbone/bn1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/bn1/BatchNormalization [BatchNormalization] outputs: [/backbone/bn1/BatchNormalization_output_0 -> (1, 64, 1104, 1936)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/relu/Relu [Relu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/bn1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/relu/Relu [Relu] inputs: [/backbone/bn1/BatchNormalization_output_0 -> (1, 64, 1104, 1936)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/relu/Relu for ONNX node: /backbone/relu/Relu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/relu/Relu_output_0 for ONNX tensor: /backbone/relu/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/relu/Relu [Relu] outputs: [/backbone/relu/Relu_output_0 -> (1, 64, 1104, 1936)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/maxpool/MaxPool [MaxPool]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/relu/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/maxpool/MaxPool [MaxPool] inputs: [/backbone/relu/Relu_output_0 -> (1, 64, 1104, 1936)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/maxpool/MaxPool for ONNX node: /backbone/maxpool/MaxPool
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/maxpool/MaxPool_output_0 for ONNX tensor: /backbone/maxpool/MaxPool_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/maxpool/MaxPool [MaxPool] outputs: [/backbone/maxpool/MaxPool_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/conv1/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv1/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv1/_input_quantizer/Constant [Constant] outputs: [/backbone/layer1/layer1.0/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/conv1/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv1/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv1/_input_quantizer/Constant_1 [Constant] outputs: [/backbone/layer1/layer1.0/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/maxpool/MaxPool_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/backbone/maxpool/MaxPool_output_0 -> (1, 64, 552, 968)[FLOAT]], [/backbone/layer1/layer1.0/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer1/layer1.0/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.0/conv1/_input_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer1/layer1.0/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.0/conv1/_input_quantizer/Constant_output_0 for ONNX node: /backbone/layer1/layer1.0/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], [/backbone/layer1/layer1.0/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer1/layer1.0/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.0/conv1/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer1/layer1.0/conv1/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer1/layer1.0/conv1/_input_quantizer/DequantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/conv1/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv1/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv1/_weight_quantizer/Constant [Constant] outputs: [/backbone/layer1/layer1.0/conv1/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/conv1/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv1/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv1/_weight_quantizer/Constant_1 [Constant] outputs: [/backbone/layer1/layer1.0/conv1/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer1.0.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [backbone.layer1.0.conv1.weight -> (64, 64, 3, 3)[FLOAT]], [/backbone/layer1/layer1.0/conv1/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], [/backbone/layer1/layer1.0/conv1/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer1.0.conv1.weight for ONNX node: backbone.layer1.0.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.0/conv1/_weight_quantizer/Constant_output_0 for ONNX node: /backbone/layer1/layer1.0/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.0/conv1/_weight_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer1/layer1.0/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [/backbone/layer1/layer1.0/conv1/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], [/backbone/layer1/layer1.0/conv1/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.0/conv1/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer1/layer1.0/conv1/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer1/layer1.0/conv1/_weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/conv1/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv1/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv1/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv1/Conv [Conv] inputs: [/backbone/layer1/layer1.0/conv1/_input_quantizer/DequantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], [/backbone/layer1/layer1.0/conv1/_weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.0/conv1/Conv for ONNX node: /backbone/layer1/layer1.0/conv1/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.0/conv1/Conv_output_0 for ONNX tensor: /backbone/layer1/layer1.0/conv1/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv1/Conv [Conv] outputs: [/backbone/layer1/layer1.0/conv1/Conv_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/bn1/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv1/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer1.0.bn1.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer1.0.bn1.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer1.0.bn1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer1.0.bn1.running_var
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/bn1/BatchNormalization [BatchNormalization] inputs: [/backbone/layer1/layer1.0/conv1/Conv_output_0 -> (1, 64, 552, 968)[FLOAT]], [backbone.layer1.0.bn1.weight -> (64)[FLOAT]], [backbone.layer1.0.bn1.bias -> (64)[FLOAT]], [backbone.layer1.0.bn1.running_mean -> (64)[FLOAT]], [backbone.layer1.0.bn1.running_var -> (64)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.0/bn1/BatchNormalization for ONNX node: /backbone/layer1/layer1.0/bn1/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.0/bn1/BatchNormalization_output_0 for ONNX tensor: /backbone/layer1/layer1.0/bn1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/bn1/BatchNormalization [BatchNormalization] outputs: [/backbone/layer1/layer1.0/bn1/BatchNormalization_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/relu/Relu [Relu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/bn1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/relu/Relu [Relu] inputs: [/backbone/layer1/layer1.0/bn1/BatchNormalization_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.0/relu/Relu for ONNX node: /backbone/layer1/layer1.0/relu/Relu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.0/relu/Relu_output_0 for ONNX tensor: /backbone/layer1/layer1.0/relu/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/relu/Relu [Relu] outputs: [/backbone/layer1/layer1.0/relu/Relu_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/conv2/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv2/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv2/_input_quantizer/Constant [Constant] outputs: [/backbone/layer1/layer1.0/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/conv2/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv2/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv2/_input_quantizer/Constant_1 [Constant] outputs: [/backbone/layer1/layer1.0/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/relu/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/backbone/layer1/layer1.0/relu/Relu_output_0 -> (1, 64, 552, 968)[FLOAT]], [/backbone/layer1/layer1.0/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer1/layer1.0/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.0/conv2/_input_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer1/layer1.0/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.0/conv2/_input_quantizer/Constant_output_0 for ONNX node: /backbone/layer1/layer1.0/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.0/conv2/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer1/layer1.0/conv2/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer1/layer1.0/conv2/_input_quantizer/QuantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv2/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer1/layer1.0/conv2/_input_quantizer/QuantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], [/backbone/layer1/layer1.0/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer1/layer1.0/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.0/conv2/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer1/layer1.0/conv2/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer1/layer1.0/conv2/_input_quantizer/DequantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/conv2/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv2/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv2/_weight_quantizer/Constant [Constant] outputs: [/backbone/layer1/layer1.0/conv2/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/conv2/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv2/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv2/_weight_quantizer/Constant_1 [Constant] outputs: [/backbone/layer1/layer1.0/conv2/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer1.0.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [backbone.layer1.0.conv2.weight -> (64, 64, 3, 3)[FLOAT]], [/backbone/layer1/layer1.0/conv2/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], [/backbone/layer1/layer1.0/conv2/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer1.0.conv2.weight for ONNX node: backbone.layer1.0.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.0/conv2/_weight_quantizer/Constant_output_0 for ONNX node: /backbone/layer1/layer1.0/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.0/conv2/_weight_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer1/layer1.0/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [/backbone/layer1/layer1.0/conv2/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], [/backbone/layer1/layer1.0/conv2/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.0/conv2/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer1/layer1.0/conv2/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer1/layer1.0/conv2/_weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/conv2/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv2/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv2/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv2/Conv [Conv] inputs: [/backbone/layer1/layer1.0/conv2/_input_quantizer/DequantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], [/backbone/layer1/layer1.0/conv2/_weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.0/conv2/Conv for ONNX node: /backbone/layer1/layer1.0/conv2/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.0/conv2/Conv_output_0 for ONNX tensor: /backbone/layer1/layer1.0/conv2/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/conv2/Conv [Conv] outputs: [/backbone/layer1/layer1.0/conv2/Conv_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/bn2/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/conv2/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer1.0.bn2.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer1.0.bn2.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer1.0.bn2.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer1.0.bn2.running_var
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/bn2/BatchNormalization [BatchNormalization] inputs: [/backbone/layer1/layer1.0/conv2/Conv_output_0 -> (1, 64, 552, 968)[FLOAT]], [backbone.layer1.0.bn2.weight -> (64)[FLOAT]], [backbone.layer1.0.bn2.bias -> (64)[FLOAT]], [backbone.layer1.0.bn2.running_mean -> (64)[FLOAT]], [backbone.layer1.0.bn2.running_var -> (64)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.0/bn2/BatchNormalization for ONNX node: /backbone/layer1/layer1.0/bn2/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.0/bn2/BatchNormalization_output_0 for ONNX tensor: /backbone/layer1/layer1.0/bn2/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/bn2/BatchNormalization [BatchNormalization] outputs: [/backbone/layer1/layer1.0/bn2/BatchNormalization_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/Add [Add]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/bn2/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/maxpool/MaxPool_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/Add [Add] inputs: [/backbone/layer1/layer1.0/bn2/BatchNormalization_output_0 -> (1, 64, 552, 968)[FLOAT]], [/backbone/maxpool/MaxPool_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.0/Add for ONNX node: /backbone/layer1/layer1.0/Add
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.0/Add_output_0 for ONNX tensor: /backbone/layer1/layer1.0/Add_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/Add [Add] outputs: [/backbone/layer1/layer1.0/Add_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.0/relu_1/Relu [Relu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/Add_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/relu_1/Relu [Relu] inputs: [/backbone/layer1/layer1.0/Add_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.0/relu_1/Relu for ONNX node: /backbone/layer1/layer1.0/relu_1/Relu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.0/relu_1/Relu_output_0 for ONNX tensor: /backbone/layer1/layer1.0/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.0/relu_1/Relu [Relu] outputs: [/backbone/layer1/layer1.0/relu_1/Relu_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/conv1/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv1/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv1/_input_quantizer/Constant [Constant] outputs: [/backbone/layer1/layer1.1/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/conv1/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv1/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv1/_input_quantizer/Constant_1 [Constant] outputs: [/backbone/layer1/layer1.1/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/backbone/layer1/layer1.0/relu_1/Relu_output_0 -> (1, 64, 552, 968)[FLOAT]], [/backbone/layer1/layer1.1/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer1/layer1.1/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.1/conv1/_input_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer1/layer1.1/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.1/conv1/_input_quantizer/Constant_output_0 for ONNX node: /backbone/layer1/layer1.1/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], [/backbone/layer1/layer1.1/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer1/layer1.1/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.1/conv1/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer1/layer1.1/conv1/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer1/layer1.1/conv1/_input_quantizer/DequantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/conv1/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv1/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv1/_weight_quantizer/Constant [Constant] outputs: [/backbone/layer1/layer1.1/conv1/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/conv1/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv1/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv1/_weight_quantizer/Constant_1 [Constant] outputs: [/backbone/layer1/layer1.1/conv1/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer1.1.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [backbone.layer1.1.conv1.weight -> (64, 64, 3, 3)[FLOAT]], [/backbone/layer1/layer1.1/conv1/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], [/backbone/layer1/layer1.1/conv1/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer1.1.conv1.weight for ONNX node: backbone.layer1.1.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.1/conv1/_weight_quantizer/Constant_output_0 for ONNX node: /backbone/layer1/layer1.1/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.1/conv1/_weight_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer1/layer1.1/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [/backbone/layer1/layer1.1/conv1/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], [/backbone/layer1/layer1.1/conv1/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.1/conv1/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer1/layer1.1/conv1/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer1/layer1.1/conv1/_weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/conv1/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv1/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv1/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv1/Conv [Conv] inputs: [/backbone/layer1/layer1.1/conv1/_input_quantizer/DequantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], [/backbone/layer1/layer1.1/conv1/_weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.1/conv1/Conv for ONNX node: /backbone/layer1/layer1.1/conv1/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.1/conv1/Conv_output_0 for ONNX tensor: /backbone/layer1/layer1.1/conv1/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv1/Conv [Conv] outputs: [/backbone/layer1/layer1.1/conv1/Conv_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/bn1/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv1/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer1.1.bn1.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer1.1.bn1.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer1.1.bn1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer1.1.bn1.running_var
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/bn1/BatchNormalization [BatchNormalization] inputs: [/backbone/layer1/layer1.1/conv1/Conv_output_0 -> (1, 64, 552, 968)[FLOAT]], [backbone.layer1.1.bn1.weight -> (64)[FLOAT]], [backbone.layer1.1.bn1.bias -> (64)[FLOAT]], [backbone.layer1.1.bn1.running_mean -> (64)[FLOAT]], [backbone.layer1.1.bn1.running_var -> (64)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.1/bn1/BatchNormalization for ONNX node: /backbone/layer1/layer1.1/bn1/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.1/bn1/BatchNormalization_output_0 for ONNX tensor: /backbone/layer1/layer1.1/bn1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/bn1/BatchNormalization [BatchNormalization] outputs: [/backbone/layer1/layer1.1/bn1/BatchNormalization_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/relu/Relu [Relu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/bn1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/relu/Relu [Relu] inputs: [/backbone/layer1/layer1.1/bn1/BatchNormalization_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.1/relu/Relu for ONNX node: /backbone/layer1/layer1.1/relu/Relu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.1/relu/Relu_output_0 for ONNX tensor: /backbone/layer1/layer1.1/relu/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/relu/Relu [Relu] outputs: [/backbone/layer1/layer1.1/relu/Relu_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/conv2/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv2/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv2/_input_quantizer/Constant [Constant] outputs: [/backbone/layer1/layer1.1/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/conv2/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv2/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv2/_input_quantizer/Constant_1 [Constant] outputs: [/backbone/layer1/layer1.1/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/relu/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/backbone/layer1/layer1.1/relu/Relu_output_0 -> (1, 64, 552, 968)[FLOAT]], [/backbone/layer1/layer1.1/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer1/layer1.1/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.1/conv2/_input_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer1/layer1.1/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.1/conv2/_input_quantizer/Constant_output_0 for ONNX node: /backbone/layer1/layer1.1/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.1/conv2/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer1/layer1.1/conv2/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer1/layer1.1/conv2/_input_quantizer/QuantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv2/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer1/layer1.1/conv2/_input_quantizer/QuantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], [/backbone/layer1/layer1.1/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer1/layer1.1/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.1/conv2/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer1/layer1.1/conv2/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer1/layer1.1/conv2/_input_quantizer/DequantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/conv2/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv2/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv2/_weight_quantizer/Constant [Constant] outputs: [/backbone/layer1/layer1.1/conv2/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/conv2/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv2/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv2/_weight_quantizer/Constant_1 [Constant] outputs: [/backbone/layer1/layer1.1/conv2/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer1.1.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [backbone.layer1.1.conv2.weight -> (64, 64, 3, 3)[FLOAT]], [/backbone/layer1/layer1.1/conv2/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], [/backbone/layer1/layer1.1/conv2/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer1.1.conv2.weight for ONNX node: backbone.layer1.1.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.1/conv2/_weight_quantizer/Constant_output_0 for ONNX node: /backbone/layer1/layer1.1/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.1/conv2/_weight_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer1/layer1.1/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.1/conv2/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer1/layer1.1/conv2/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer1/layer1.1/conv2/_weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv2/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer1/layer1.1/conv2/_weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [/backbone/layer1/layer1.1/conv2/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], [/backbone/layer1/layer1.1/conv2/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.1/conv2/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer1/layer1.1/conv2/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer1/layer1.1/conv2/_weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/conv2/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv2/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv2/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv2/Conv [Conv] inputs: [/backbone/layer1/layer1.1/conv2/_input_quantizer/DequantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], [/backbone/layer1/layer1.1/conv2/_weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.1/conv2/Conv for ONNX node: /backbone/layer1/layer1.1/conv2/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.1/conv2/Conv_output_0 for ONNX tensor: /backbone/layer1/layer1.1/conv2/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/conv2/Conv [Conv] outputs: [/backbone/layer1/layer1.1/conv2/Conv_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/bn2/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/conv2/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer1.1.bn2.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer1.1.bn2.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer1.1.bn2.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer1.1.bn2.running_var
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/bn2/BatchNormalization [BatchNormalization] inputs: [/backbone/layer1/layer1.1/conv2/Conv_output_0 -> (1, 64, 552, 968)[FLOAT]], [backbone.layer1.1.bn2.weight -> (64)[FLOAT]], [backbone.layer1.1.bn2.bias -> (64)[FLOAT]], [backbone.layer1.1.bn2.running_mean -> (64)[FLOAT]], [backbone.layer1.1.bn2.running_var -> (64)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.1/bn2/BatchNormalization for ONNX node: /backbone/layer1/layer1.1/bn2/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.1/bn2/BatchNormalization_output_0 for ONNX tensor: /backbone/layer1/layer1.1/bn2/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/bn2/BatchNormalization [BatchNormalization] outputs: [/backbone/layer1/layer1.1/bn2/BatchNormalization_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/Add [Add]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/bn2/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.0/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/Add [Add] inputs: [/backbone/layer1/layer1.1/bn2/BatchNormalization_output_0 -> (1, 64, 552, 968)[FLOAT]], [/backbone/layer1/layer1.0/relu_1/Relu_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.1/Add for ONNX node: /backbone/layer1/layer1.1/Add
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.1/Add_output_0 for ONNX tensor: /backbone/layer1/layer1.1/Add_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/Add [Add] outputs: [/backbone/layer1/layer1.1/Add_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer1/layer1.1/relu_1/Relu [Relu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/Add_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/relu_1/Relu [Relu] inputs: [/backbone/layer1/layer1.1/Add_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer1/layer1.1/relu_1/Relu for ONNX node: /backbone/layer1/layer1.1/relu_1/Relu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer1/layer1.1/relu_1/Relu_output_0 for ONNX tensor: /backbone/layer1/layer1.1/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer1/layer1.1/relu_1/Relu [Relu] outputs: [/backbone/layer1/layer1.1/relu_1/Relu_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/conv1/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv1/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv1/_input_quantizer/Constant [Constant] outputs: [/backbone/layer2/layer2.0/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/conv1/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv1/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv1/_input_quantizer/Constant_1 [Constant] outputs: [/backbone/layer2/layer2.0/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/backbone/layer1/layer1.1/relu_1/Relu_output_0 -> (1, 64, 552, 968)[FLOAT]], [/backbone/layer2/layer2.0/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer2/layer2.0/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.0/conv1/_input_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer2/layer2.0/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.0/conv1/_input_quantizer/Constant_output_0 for ONNX node: /backbone/layer2/layer2.0/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.0/conv1/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer2/layer2.0/conv1/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer2/layer2.0/conv1/_input_quantizer/QuantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv1/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer2/layer2.0/conv1/_input_quantizer/QuantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], [/backbone/layer2/layer2.0/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer2/layer2.0/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.0/conv1/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer2/layer2.0/conv1/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer2/layer2.0/conv1/_input_quantizer/DequantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/conv1/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv1/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv1/_weight_quantizer/Constant [Constant] outputs: [/backbone/layer2/layer2.0/conv1/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/conv1/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv1/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv1/_weight_quantizer/Constant_1 [Constant] outputs: [/backbone/layer2/layer2.0/conv1/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.0.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [backbone.layer2.0.conv1.weight -> (128, 64, 3, 3)[FLOAT]], [/backbone/layer2/layer2.0/conv1/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/backbone/layer2/layer2.0/conv1/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer2.0.conv1.weight for ONNX node: backbone.layer2.0.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.0/conv1/_weight_quantizer/Constant_output_0 for ONNX node: /backbone/layer2/layer2.0/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.0/conv1/_weight_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer2/layer2.0/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], [/backbone/layer2/layer2.0/conv1/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/backbone/layer2/layer2.0/conv1/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.0/conv1/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer2/layer2.0/conv1/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer2/layer2.0/conv1/_weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/conv1/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv1/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv1/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv1/Conv [Conv] inputs: [/backbone/layer2/layer2.0/conv1/_input_quantizer/DequantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], [/backbone/layer2/layer2.0/conv1/_weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.0/conv1/Conv for ONNX node: /backbone/layer2/layer2.0/conv1/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.0/conv1/Conv_output_0 for ONNX tensor: /backbone/layer2/layer2.0/conv1/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv1/Conv [Conv] outputs: [/backbone/layer2/layer2.0/conv1/Conv_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/bn1/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv1/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.0.bn1.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.0.bn1.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.0.bn1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.0.bn1.running_var
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/bn1/BatchNormalization [BatchNormalization] inputs: [/backbone/layer2/layer2.0/conv1/Conv_output_0 -> (1, 128, 276, 484)[FLOAT]], [backbone.layer2.0.bn1.weight -> (128)[FLOAT]], [backbone.layer2.0.bn1.bias -> (128)[FLOAT]], [backbone.layer2.0.bn1.running_mean -> (128)[FLOAT]], [backbone.layer2.0.bn1.running_var -> (128)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.0/bn1/BatchNormalization for ONNX node: /backbone/layer2/layer2.0/bn1/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.0/bn1/BatchNormalization_output_0 for ONNX tensor: /backbone/layer2/layer2.0/bn1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/bn1/BatchNormalization [BatchNormalization] outputs: [/backbone/layer2/layer2.0/bn1/BatchNormalization_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/relu/Relu [Relu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/bn1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/relu/Relu [Relu] inputs: [/backbone/layer2/layer2.0/bn1/BatchNormalization_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.0/relu/Relu for ONNX node: /backbone/layer2/layer2.0/relu/Relu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.0/relu/Relu_output_0 for ONNX tensor: /backbone/layer2/layer2.0/relu/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/relu/Relu [Relu] outputs: [/backbone/layer2/layer2.0/relu/Relu_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/conv2/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv2/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv2/_input_quantizer/Constant [Constant] outputs: [/backbone/layer2/layer2.0/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/conv2/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv2/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv2/_input_quantizer/Constant_1 [Constant] outputs: [/backbone/layer2/layer2.0/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/relu/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/backbone/layer2/layer2.0/relu/Relu_output_0 -> (1, 128, 276, 484)[FLOAT]], [/backbone/layer2/layer2.0/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer2/layer2.0/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.0/conv2/_input_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer2/layer2.0/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.0/conv2/_input_quantizer/Constant_output_0 for ONNX node: /backbone/layer2/layer2.0/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.0/conv2/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer2/layer2.0/conv2/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer2/layer2.0/conv2/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv2/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer2/layer2.0/conv2/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], [/backbone/layer2/layer2.0/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer2/layer2.0/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.0/conv2/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer2/layer2.0/conv2/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer2/layer2.0/conv2/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/conv2/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv2/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv2/_weight_quantizer/Constant [Constant] outputs: [/backbone/layer2/layer2.0/conv2/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/conv2/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv2/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv2/_weight_quantizer/Constant_1 [Constant] outputs: [/backbone/layer2/layer2.0/conv2/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.0.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [backbone.layer2.0.conv2.weight -> (128, 128, 3, 3)[FLOAT]], [/backbone/layer2/layer2.0/conv2/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/backbone/layer2/layer2.0/conv2/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer2.0.conv2.weight for ONNX node: backbone.layer2.0.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.0/conv2/_weight_quantizer/Constant_output_0 for ONNX node: /backbone/layer2/layer2.0/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.0/conv2/_weight_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer2/layer2.0/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [/backbone/layer2/layer2.0/conv2/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/backbone/layer2/layer2.0/conv2/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.0/conv2/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer2/layer2.0/conv2/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer2/layer2.0/conv2/_weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/conv2/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv2/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv2/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv2/Conv [Conv] inputs: [/backbone/layer2/layer2.0/conv2/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], [/backbone/layer2/layer2.0/conv2/_weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.0/conv2/Conv for ONNX node: /backbone/layer2/layer2.0/conv2/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.0/conv2/Conv_output_0 for ONNX tensor: /backbone/layer2/layer2.0/conv2/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/conv2/Conv [Conv] outputs: [/backbone/layer2/layer2.0/conv2/Conv_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/bn2/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/conv2/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.0.bn2.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.0.bn2.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.0.bn2.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.0.bn2.running_var
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/bn2/BatchNormalization [BatchNormalization] inputs: [/backbone/layer2/layer2.0/conv2/Conv_output_0 -> (1, 128, 276, 484)[FLOAT]], [backbone.layer2.0.bn2.weight -> (128)[FLOAT]], [backbone.layer2.0.bn2.bias -> (128)[FLOAT]], [backbone.layer2.0.bn2.running_mean -> (128)[FLOAT]], [backbone.layer2.0.bn2.running_var -> (128)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.0/bn2/BatchNormalization for ONNX node: /backbone/layer2/layer2.0/bn2/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.0/bn2/BatchNormalization_output_0 for ONNX tensor: /backbone/layer2/layer2.0/bn2/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/bn2/BatchNormalization [BatchNormalization] outputs: [/backbone/layer2/layer2.0/bn2/BatchNormalization_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/Constant [Constant] outputs: [/backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/Constant_1 [Constant] outputs: [/backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer1/layer1.1/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/backbone/layer1/layer1.1/relu_1/Relu_output_0 -> (1, 64, 552, 968)[FLOAT]], [/backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/Constant_output_0 for ONNX node: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], [/backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/DequantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/Constant [Constant] outputs: [/backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/Constant_1 [Constant] outputs: [/backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.0.downsample.0.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [backbone.layer2.0.downsample.0.weight -> (128, 64, 1, 1)[FLOAT]], [/backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer2.0.downsample.0.weight for ONNX node: backbone.layer2.0.downsample.0.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/Constant_output_0 for ONNX node: /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 1, 1)[FLOAT]], [/backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/downsample/downsample.0/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/downsample/downsample.0/Conv [Conv] inputs: [/backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/DequantizeLinear_output_0 -> (1, 64, 552, 968)[FLOAT]], [/backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.0/downsample/downsample.0/Conv for ONNX node: /backbone/layer2/layer2.0/downsample/downsample.0/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 for ONNX tensor: /backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/downsample/downsample.0/Conv [Conv] outputs: [/backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/downsample/downsample.1/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.0.downsample.1.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.0.downsample.1.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.0.downsample.1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.0.downsample.1.running_var
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/downsample/downsample.1/BatchNormalization [BatchNormalization] inputs: [/backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> (1, 128, 276, 484)[FLOAT]], [backbone.layer2.0.downsample.1.weight -> (128)[FLOAT]], [backbone.layer2.0.downsample.1.bias -> (128)[FLOAT]], [backbone.layer2.0.downsample.1.running_mean -> (128)[FLOAT]], [backbone.layer2.0.downsample.1.running_var -> (128)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer2.0.downsample.1.weight for ONNX node: backbone.layer2.0.downsample.1.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer2.0.downsample.1.running_mean for ONNX node: backbone.layer2.0.downsample.1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer2.0.downsample.1.running_var for ONNX node: backbone.layer2.0.downsample.1.running_var
[10/31/2023-17:53:34] [V] [TRT] Original shape: (128,), unsqueezing to: (1, 128, 1, 1)
[10/31/2023-17:53:34] [V] [TRT] Original shape: (128,), unsqueezing to: (1, 128, 1, 1)
[10/31/2023-17:53:34] [V] [TRT] Original shape: (128,), unsqueezing to: (1, 128, 1, 1)
[10/31/2023-17:53:34] [V] [TRT] Original shape: (128,), unsqueezing to: (1, 128, 1, 1)
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.0/downsample/downsample.1/BatchNormalization for ONNX node: /backbone/layer2/layer2.0/downsample/downsample.1/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.0/downsample/downsample.1/BatchNormalization_output_0 for ONNX tensor: /backbone/layer2/layer2.0/downsample/downsample.1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/downsample/downsample.1/BatchNormalization [BatchNormalization] outputs: [/backbone/layer2/layer2.0/downsample/downsample.1/BatchNormalization_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/Add [Add]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/bn2/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/downsample/downsample.1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/Add [Add] inputs: [/backbone/layer2/layer2.0/bn2/BatchNormalization_output_0 -> (1, 128, 276, 484)[FLOAT]], [/backbone/layer2/layer2.0/downsample/downsample.1/BatchNormalization_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.0/Add for ONNX node: /backbone/layer2/layer2.0/Add
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.0/Add_output_0 for ONNX tensor: /backbone/layer2/layer2.0/Add_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/Add [Add] outputs: [/backbone/layer2/layer2.0/Add_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.0/relu_1/Relu [Relu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/Add_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/relu_1/Relu [Relu] inputs: [/backbone/layer2/layer2.0/Add_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.0/relu_1/Relu for ONNX node: /backbone/layer2/layer2.0/relu_1/Relu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.0/relu_1/Relu_output_0 for ONNX tensor: /backbone/layer2/layer2.0/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.0/relu_1/Relu [Relu] outputs: [/backbone/layer2/layer2.0/relu_1/Relu_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/conv1/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv1/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv1/_input_quantizer/Constant [Constant] outputs: [/backbone/layer2/layer2.1/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/conv1/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv1/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv1/_input_quantizer/Constant_1 [Constant] outputs: [/backbone/layer2/layer2.1/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/backbone/layer2/layer2.0/relu_1/Relu_output_0 -> (1, 128, 276, 484)[FLOAT]], [/backbone/layer2/layer2.1/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer2/layer2.1/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.1/conv1/_input_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer2/layer2.1/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.1/conv1/_input_quantizer/Constant_output_0 for ONNX node: /backbone/layer2/layer2.1/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], [/backbone/layer2/layer2.1/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer2/layer2.1/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.1/conv1/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer2/layer2.1/conv1/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer2/layer2.1/conv1/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/conv1/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv1/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv1/_weight_quantizer/Constant [Constant] outputs: [/backbone/layer2/layer2.1/conv1/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/conv1/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv1/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv1/_weight_quantizer/Constant_1 [Constant] outputs: [/backbone/layer2/layer2.1/conv1/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.1.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [backbone.layer2.1.conv1.weight -> (128, 128, 3, 3)[FLOAT]], [/backbone/layer2/layer2.1/conv1/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/backbone/layer2/layer2.1/conv1/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer2.1.conv1.weight for ONNX node: backbone.layer2.1.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.1/conv1/_weight_quantizer/Constant_output_0 for ONNX node: /backbone/layer2/layer2.1/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.1/conv1/_weight_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer2/layer2.1/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [/backbone/layer2/layer2.1/conv1/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/backbone/layer2/layer2.1/conv1/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.1/conv1/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer2/layer2.1/conv1/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer2/layer2.1/conv1/_weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/conv1/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv1/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv1/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv1/Conv [Conv] inputs: [/backbone/layer2/layer2.1/conv1/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], [/backbone/layer2/layer2.1/conv1/_weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.1/conv1/Conv for ONNX node: /backbone/layer2/layer2.1/conv1/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.1/conv1/Conv_output_0 for ONNX tensor: /backbone/layer2/layer2.1/conv1/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv1/Conv [Conv] outputs: [/backbone/layer2/layer2.1/conv1/Conv_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/bn1/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv1/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.1.bn1.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.1.bn1.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.1.bn1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.1.bn1.running_var
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/bn1/BatchNormalization [BatchNormalization] inputs: [/backbone/layer2/layer2.1/conv1/Conv_output_0 -> (1, 128, 276, 484)[FLOAT]], [backbone.layer2.1.bn1.weight -> (128)[FLOAT]], [backbone.layer2.1.bn1.bias -> (128)[FLOAT]], [backbone.layer2.1.bn1.running_mean -> (128)[FLOAT]], [backbone.layer2.1.bn1.running_var -> (128)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.1/bn1/BatchNormalization for ONNX node: /backbone/layer2/layer2.1/bn1/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.1/bn1/BatchNormalization_output_0 for ONNX tensor: /backbone/layer2/layer2.1/bn1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/bn1/BatchNormalization [BatchNormalization] outputs: [/backbone/layer2/layer2.1/bn1/BatchNormalization_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/relu/Relu [Relu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/bn1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/relu/Relu [Relu] inputs: [/backbone/layer2/layer2.1/bn1/BatchNormalization_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.1/relu/Relu for ONNX node: /backbone/layer2/layer2.1/relu/Relu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.1/relu/Relu_output_0 for ONNX tensor: /backbone/layer2/layer2.1/relu/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/relu/Relu [Relu] outputs: [/backbone/layer2/layer2.1/relu/Relu_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/conv2/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv2/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv2/_input_quantizer/Constant [Constant] outputs: [/backbone/layer2/layer2.1/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/conv2/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv2/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv2/_input_quantizer/Constant_1 [Constant] outputs: [/backbone/layer2/layer2.1/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/relu/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/backbone/layer2/layer2.1/relu/Relu_output_0 -> (1, 128, 276, 484)[FLOAT]], [/backbone/layer2/layer2.1/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer2/layer2.1/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.1/conv2/_input_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer2/layer2.1/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.1/conv2/_input_quantizer/Constant_output_0 for ONNX node: /backbone/layer2/layer2.1/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.1/conv2/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer2/layer2.1/conv2/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer2/layer2.1/conv2/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv2/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer2/layer2.1/conv2/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], [/backbone/layer2/layer2.1/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer2/layer2.1/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.1/conv2/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer2/layer2.1/conv2/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer2/layer2.1/conv2/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/conv2/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv2/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv2/_weight_quantizer/Constant [Constant] outputs: [/backbone/layer2/layer2.1/conv2/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/conv2/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv2/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv2/_weight_quantizer/Constant_1 [Constant] outputs: [/backbone/layer2/layer2.1/conv2/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.1.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [backbone.layer2.1.conv2.weight -> (128, 128, 3, 3)[FLOAT]], [/backbone/layer2/layer2.1/conv2/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/backbone/layer2/layer2.1/conv2/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer2.1.conv2.weight for ONNX node: backbone.layer2.1.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.1/conv2/_weight_quantizer/Constant_output_0 for ONNX node: /backbone/layer2/layer2.1/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.1/conv2/_weight_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer2/layer2.1/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.1/conv2/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer2/layer2.1/conv2/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer2/layer2.1/conv2/_weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv2/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer2/layer2.1/conv2/_weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [/backbone/layer2/layer2.1/conv2/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/backbone/layer2/layer2.1/conv2/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.1/conv2/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer2/layer2.1/conv2/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer2/layer2.1/conv2/_weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/conv2/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv2/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv2/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv2/Conv [Conv] inputs: [/backbone/layer2/layer2.1/conv2/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], [/backbone/layer2/layer2.1/conv2/_weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.1/conv2/Conv for ONNX node: /backbone/layer2/layer2.1/conv2/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.1/conv2/Conv_output_0 for ONNX tensor: /backbone/layer2/layer2.1/conv2/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/conv2/Conv [Conv] outputs: [/backbone/layer2/layer2.1/conv2/Conv_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/bn2/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/conv2/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.1.bn2.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.1.bn2.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.1.bn2.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer2.1.bn2.running_var
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/bn2/BatchNormalization [BatchNormalization] inputs: [/backbone/layer2/layer2.1/conv2/Conv_output_0 -> (1, 128, 276, 484)[FLOAT]], [backbone.layer2.1.bn2.weight -> (128)[FLOAT]], [backbone.layer2.1.bn2.bias -> (128)[FLOAT]], [backbone.layer2.1.bn2.running_mean -> (128)[FLOAT]], [backbone.layer2.1.bn2.running_var -> (128)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.1/bn2/BatchNormalization for ONNX node: /backbone/layer2/layer2.1/bn2/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.1/bn2/BatchNormalization_output_0 for ONNX tensor: /backbone/layer2/layer2.1/bn2/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/bn2/BatchNormalization [BatchNormalization] outputs: [/backbone/layer2/layer2.1/bn2/BatchNormalization_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/Add [Add]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/bn2/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.0/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/Add [Add] inputs: [/backbone/layer2/layer2.1/bn2/BatchNormalization_output_0 -> (1, 128, 276, 484)[FLOAT]], [/backbone/layer2/layer2.0/relu_1/Relu_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.1/Add for ONNX node: /backbone/layer2/layer2.1/Add
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.1/Add_output_0 for ONNX tensor: /backbone/layer2/layer2.1/Add_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/Add [Add] outputs: [/backbone/layer2/layer2.1/Add_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer2/layer2.1/relu_1/Relu [Relu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/Add_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/relu_1/Relu [Relu] inputs: [/backbone/layer2/layer2.1/Add_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer2/layer2.1/relu_1/Relu for ONNX node: /backbone/layer2/layer2.1/relu_1/Relu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer2/layer2.1/relu_1/Relu_output_0 for ONNX tensor: /backbone/layer2/layer2.1/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer2/layer2.1/relu_1/Relu [Relu] outputs: [/backbone/layer2/layer2.1/relu_1/Relu_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/conv1/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv1/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv1/_input_quantizer/Constant [Constant] outputs: [/backbone/layer3/layer3.0/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/conv1/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv1/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv1/_input_quantizer/Constant_1 [Constant] outputs: [/backbone/layer3/layer3.0/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/backbone/layer2/layer2.1/relu_1/Relu_output_0 -> (1, 128, 276, 484)[FLOAT]], [/backbone/layer3/layer3.0/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer3/layer3.0/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.0/conv1/_input_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer3/layer3.0/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.0/conv1/_input_quantizer/Constant_output_0 for ONNX node: /backbone/layer3/layer3.0/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.0/conv1/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer3/layer3.0/conv1/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer3/layer3.0/conv1/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv1/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer3/layer3.0/conv1/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], [/backbone/layer3/layer3.0/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer3/layer3.0/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.0/conv1/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer3/layer3.0/conv1/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer3/layer3.0/conv1/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/conv1/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv1/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv1/_weight_quantizer/Constant [Constant] outputs: [/backbone/layer3/layer3.0/conv1/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/conv1/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv1/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv1/_weight_quantizer/Constant_1 [Constant] outputs: [/backbone/layer3/layer3.0/conv1/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.0.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [backbone.layer3.0.conv1.weight -> (256, 128, 3, 3)[FLOAT]], [/backbone/layer3/layer3.0/conv1/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], [/backbone/layer3/layer3.0/conv1/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer3.0.conv1.weight for ONNX node: backbone.layer3.0.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.0/conv1/_weight_quantizer/Constant_output_0 for ONNX node: /backbone/layer3/layer3.0/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.0/conv1/_weight_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer3/layer3.0/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], [/backbone/layer3/layer3.0/conv1/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], [/backbone/layer3/layer3.0/conv1/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.0/conv1/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer3/layer3.0/conv1/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer3/layer3.0/conv1/_weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/conv1/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv1/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv1/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv1/Conv [Conv] inputs: [/backbone/layer3/layer3.0/conv1/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], [/backbone/layer3/layer3.0/conv1/_weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.0/conv1/Conv for ONNX node: /backbone/layer3/layer3.0/conv1/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.0/conv1/Conv_output_0 for ONNX tensor: /backbone/layer3/layer3.0/conv1/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv1/Conv [Conv] outputs: [/backbone/layer3/layer3.0/conv1/Conv_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/bn1/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv1/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.0.bn1.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.0.bn1.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.0.bn1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.0.bn1.running_var
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/bn1/BatchNormalization [BatchNormalization] inputs: [/backbone/layer3/layer3.0/conv1/Conv_output_0 -> (1, 256, 138, 242)[FLOAT]], [backbone.layer3.0.bn1.weight -> (256)[FLOAT]], [backbone.layer3.0.bn1.bias -> (256)[FLOAT]], [backbone.layer3.0.bn1.running_mean -> (256)[FLOAT]], [backbone.layer3.0.bn1.running_var -> (256)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.0/bn1/BatchNormalization for ONNX node: /backbone/layer3/layer3.0/bn1/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.0/bn1/BatchNormalization_output_0 for ONNX tensor: /backbone/layer3/layer3.0/bn1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/bn1/BatchNormalization [BatchNormalization] outputs: [/backbone/layer3/layer3.0/bn1/BatchNormalization_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/relu/Relu [Relu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/bn1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/relu/Relu [Relu] inputs: [/backbone/layer3/layer3.0/bn1/BatchNormalization_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.0/relu/Relu for ONNX node: /backbone/layer3/layer3.0/relu/Relu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.0/relu/Relu_output_0 for ONNX tensor: /backbone/layer3/layer3.0/relu/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/relu/Relu [Relu] outputs: [/backbone/layer3/layer3.0/relu/Relu_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/conv2/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv2/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv2/_input_quantizer/Constant [Constant] outputs: [/backbone/layer3/layer3.0/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/conv2/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv2/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv2/_input_quantizer/Constant_1 [Constant] outputs: [/backbone/layer3/layer3.0/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/relu/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/backbone/layer3/layer3.0/relu/Relu_output_0 -> (1, 256, 138, 242)[FLOAT]], [/backbone/layer3/layer3.0/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer3/layer3.0/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.0/conv2/_input_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer3/layer3.0/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.0/conv2/_input_quantizer/Constant_output_0 for ONNX node: /backbone/layer3/layer3.0/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.0/conv2/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer3/layer3.0/conv2/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer3/layer3.0/conv2/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv2/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer3/layer3.0/conv2/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], [/backbone/layer3/layer3.0/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer3/layer3.0/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.0/conv2/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer3/layer3.0/conv2/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer3/layer3.0/conv2/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/conv2/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv2/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv2/_weight_quantizer/Constant [Constant] outputs: [/backbone/layer3/layer3.0/conv2/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/conv2/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv2/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv2/_weight_quantizer/Constant_1 [Constant] outputs: [/backbone/layer3/layer3.0/conv2/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.0.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [backbone.layer3.0.conv2.weight -> (256, 256, 3, 3)[FLOAT]], [/backbone/layer3/layer3.0/conv2/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], [/backbone/layer3/layer3.0/conv2/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer3.0.conv2.weight for ONNX node: backbone.layer3.0.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.0/conv2/_weight_quantizer/Constant_output_0 for ONNX node: /backbone/layer3/layer3.0/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.0/conv2/_weight_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer3/layer3.0/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [/backbone/layer3/layer3.0/conv2/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], [/backbone/layer3/layer3.0/conv2/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.0/conv2/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer3/layer3.0/conv2/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer3/layer3.0/conv2/_weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/conv2/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv2/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv2/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv2/Conv [Conv] inputs: [/backbone/layer3/layer3.0/conv2/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], [/backbone/layer3/layer3.0/conv2/_weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.0/conv2/Conv for ONNX node: /backbone/layer3/layer3.0/conv2/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.0/conv2/Conv_output_0 for ONNX tensor: /backbone/layer3/layer3.0/conv2/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/conv2/Conv [Conv] outputs: [/backbone/layer3/layer3.0/conv2/Conv_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/bn2/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/conv2/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.0.bn2.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.0.bn2.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.0.bn2.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.0.bn2.running_var
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/bn2/BatchNormalization [BatchNormalization] inputs: [/backbone/layer3/layer3.0/conv2/Conv_output_0 -> (1, 256, 138, 242)[FLOAT]], [backbone.layer3.0.bn2.weight -> (256)[FLOAT]], [backbone.layer3.0.bn2.bias -> (256)[FLOAT]], [backbone.layer3.0.bn2.running_mean -> (256)[FLOAT]], [backbone.layer3.0.bn2.running_var -> (256)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.0/bn2/BatchNormalization for ONNX node: /backbone/layer3/layer3.0/bn2/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.0/bn2/BatchNormalization_output_0 for ONNX tensor: /backbone/layer3/layer3.0/bn2/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/bn2/BatchNormalization [BatchNormalization] outputs: [/backbone/layer3/layer3.0/bn2/BatchNormalization_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/Constant [Constant] outputs: [/backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/Constant_1 [Constant] outputs: [/backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/backbone/layer2/layer2.1/relu_1/Relu_output_0 -> (1, 128, 276, 484)[FLOAT]], [/backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/Constant_output_0 for ONNX node: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], [/backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/Constant [Constant] outputs: [/backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/Constant_1 [Constant] outputs: [/backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.0.downsample.0.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [backbone.layer3.0.downsample.0.weight -> (256, 128, 1, 1)[FLOAT]], [/backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], [/backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer3.0.downsample.0.weight for ONNX node: backbone.layer3.0.downsample.0.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/Constant_output_0 for ONNX node: /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [/backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], [/backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/downsample/downsample.0/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/downsample/downsample.0/Conv [Conv] inputs: [/backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], [/backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.0/downsample/downsample.0/Conv for ONNX node: /backbone/layer3/layer3.0/downsample/downsample.0/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 for ONNX tensor: /backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/downsample/downsample.0/Conv [Conv] outputs: [/backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/downsample/downsample.1/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.0.downsample.1.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.0.downsample.1.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.0.downsample.1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.0.downsample.1.running_var
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/downsample/downsample.1/BatchNormalization [BatchNormalization] inputs: [/backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> (1, 256, 138, 242)[FLOAT]], [backbone.layer3.0.downsample.1.weight -> (256)[FLOAT]], [backbone.layer3.0.downsample.1.bias -> (256)[FLOAT]], [backbone.layer3.0.downsample.1.running_mean -> (256)[FLOAT]], [backbone.layer3.0.downsample.1.running_var -> (256)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer3.0.downsample.1.weight for ONNX node: backbone.layer3.0.downsample.1.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer3.0.downsample.1.running_mean for ONNX node: backbone.layer3.0.downsample.1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer3.0.downsample.1.running_var for ONNX node: backbone.layer3.0.downsample.1.running_var
[10/31/2023-17:53:34] [V] [TRT] Original shape: (256,), unsqueezing to: (1, 256, 1, 1)
[10/31/2023-17:53:34] [V] [TRT] Original shape: (256,), unsqueezing to: (1, 256, 1, 1)
[10/31/2023-17:53:34] [V] [TRT] Original shape: (256,), unsqueezing to: (1, 256, 1, 1)
[10/31/2023-17:53:34] [V] [TRT] Original shape: (256,), unsqueezing to: (1, 256, 1, 1)
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.0/downsample/downsample.1/BatchNormalization for ONNX node: /backbone/layer3/layer3.0/downsample/downsample.1/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.0/downsample/downsample.1/BatchNormalization_output_0 for ONNX tensor: /backbone/layer3/layer3.0/downsample/downsample.1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/downsample/downsample.1/BatchNormalization [BatchNormalization] outputs: [/backbone/layer3/layer3.0/downsample/downsample.1/BatchNormalization_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/Add [Add]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/bn2/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/downsample/downsample.1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/Add [Add] inputs: [/backbone/layer3/layer3.0/bn2/BatchNormalization_output_0 -> (1, 256, 138, 242)[FLOAT]], [/backbone/layer3/layer3.0/downsample/downsample.1/BatchNormalization_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.0/Add for ONNX node: /backbone/layer3/layer3.0/Add
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.0/Add_output_0 for ONNX tensor: /backbone/layer3/layer3.0/Add_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/Add [Add] outputs: [/backbone/layer3/layer3.0/Add_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.0/relu_1/Relu [Relu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/Add_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/relu_1/Relu [Relu] inputs: [/backbone/layer3/layer3.0/Add_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.0/relu_1/Relu for ONNX node: /backbone/layer3/layer3.0/relu_1/Relu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.0/relu_1/Relu_output_0 for ONNX tensor: /backbone/layer3/layer3.0/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.0/relu_1/Relu [Relu] outputs: [/backbone/layer3/layer3.0/relu_1/Relu_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/conv1/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv1/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv1/_input_quantizer/Constant [Constant] outputs: [/backbone/layer3/layer3.1/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/conv1/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv1/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv1/_input_quantizer/Constant_1 [Constant] outputs: [/backbone/layer3/layer3.1/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/backbone/layer3/layer3.0/relu_1/Relu_output_0 -> (1, 256, 138, 242)[FLOAT]], [/backbone/layer3/layer3.1/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer3/layer3.1/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.1/conv1/_input_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer3/layer3.1/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.1/conv1/_input_quantizer/Constant_output_0 for ONNX node: /backbone/layer3/layer3.1/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], [/backbone/layer3/layer3.1/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer3/layer3.1/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.1/conv1/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer3/layer3.1/conv1/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer3/layer3.1/conv1/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/conv1/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv1/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv1/_weight_quantizer/Constant [Constant] outputs: [/backbone/layer3/layer3.1/conv1/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/conv1/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv1/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv1/_weight_quantizer/Constant_1 [Constant] outputs: [/backbone/layer3/layer3.1/conv1/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.1.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [backbone.layer3.1.conv1.weight -> (256, 256, 3, 3)[FLOAT]], [/backbone/layer3/layer3.1/conv1/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], [/backbone/layer3/layer3.1/conv1/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer3.1.conv1.weight for ONNX node: backbone.layer3.1.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.1/conv1/_weight_quantizer/Constant_output_0 for ONNX node: /backbone/layer3/layer3.1/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.1/conv1/_weight_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer3/layer3.1/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [/backbone/layer3/layer3.1/conv1/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], [/backbone/layer3/layer3.1/conv1/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.1/conv1/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer3/layer3.1/conv1/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer3/layer3.1/conv1/_weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/conv1/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv1/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv1/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv1/Conv [Conv] inputs: [/backbone/layer3/layer3.1/conv1/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], [/backbone/layer3/layer3.1/conv1/_weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.1/conv1/Conv for ONNX node: /backbone/layer3/layer3.1/conv1/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.1/conv1/Conv_output_0 for ONNX tensor: /backbone/layer3/layer3.1/conv1/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv1/Conv [Conv] outputs: [/backbone/layer3/layer3.1/conv1/Conv_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/bn1/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv1/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.1.bn1.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.1.bn1.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.1.bn1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.1.bn1.running_var
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/bn1/BatchNormalization [BatchNormalization] inputs: [/backbone/layer3/layer3.1/conv1/Conv_output_0 -> (1, 256, 138, 242)[FLOAT]], [backbone.layer3.1.bn1.weight -> (256)[FLOAT]], [backbone.layer3.1.bn1.bias -> (256)[FLOAT]], [backbone.layer3.1.bn1.running_mean -> (256)[FLOAT]], [backbone.layer3.1.bn1.running_var -> (256)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.1/bn1/BatchNormalization for ONNX node: /backbone/layer3/layer3.1/bn1/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.1/bn1/BatchNormalization_output_0 for ONNX tensor: /backbone/layer3/layer3.1/bn1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/bn1/BatchNormalization [BatchNormalization] outputs: [/backbone/layer3/layer3.1/bn1/BatchNormalization_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/relu/Relu [Relu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/bn1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/relu/Relu [Relu] inputs: [/backbone/layer3/layer3.1/bn1/BatchNormalization_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.1/relu/Relu for ONNX node: /backbone/layer3/layer3.1/relu/Relu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.1/relu/Relu_output_0 for ONNX tensor: /backbone/layer3/layer3.1/relu/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/relu/Relu [Relu] outputs: [/backbone/layer3/layer3.1/relu/Relu_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/conv2/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv2/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv2/_input_quantizer/Constant [Constant] outputs: [/backbone/layer3/layer3.1/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/conv2/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv2/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv2/_input_quantizer/Constant_1 [Constant] outputs: [/backbone/layer3/layer3.1/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/relu/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/backbone/layer3/layer3.1/relu/Relu_output_0 -> (1, 256, 138, 242)[FLOAT]], [/backbone/layer3/layer3.1/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer3/layer3.1/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.1/conv2/_input_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer3/layer3.1/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.1/conv2/_input_quantizer/Constant_output_0 for ONNX node: /backbone/layer3/layer3.1/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.1/conv2/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer3/layer3.1/conv2/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer3/layer3.1/conv2/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv2/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer3/layer3.1/conv2/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], [/backbone/layer3/layer3.1/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer3/layer3.1/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.1/conv2/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer3/layer3.1/conv2/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer3/layer3.1/conv2/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/conv2/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv2/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv2/_weight_quantizer/Constant [Constant] outputs: [/backbone/layer3/layer3.1/conv2/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/conv2/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv2/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv2/_weight_quantizer/Constant_1 [Constant] outputs: [/backbone/layer3/layer3.1/conv2/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.1.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [backbone.layer3.1.conv2.weight -> (256, 256, 3, 3)[FLOAT]], [/backbone/layer3/layer3.1/conv2/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], [/backbone/layer3/layer3.1/conv2/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer3.1.conv2.weight for ONNX node: backbone.layer3.1.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.1/conv2/_weight_quantizer/Constant_output_0 for ONNX node: /backbone/layer3/layer3.1/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.1/conv2/_weight_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer3/layer3.1/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.1/conv2/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer3/layer3.1/conv2/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer3/layer3.1/conv2/_weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv2/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer3/layer3.1/conv2/_weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [/backbone/layer3/layer3.1/conv2/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], [/backbone/layer3/layer3.1/conv2/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.1/conv2/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer3/layer3.1/conv2/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer3/layer3.1/conv2/_weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/conv2/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv2/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv2/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv2/Conv [Conv] inputs: [/backbone/layer3/layer3.1/conv2/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], [/backbone/layer3/layer3.1/conv2/_weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.1/conv2/Conv for ONNX node: /backbone/layer3/layer3.1/conv2/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.1/conv2/Conv_output_0 for ONNX tensor: /backbone/layer3/layer3.1/conv2/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/conv2/Conv [Conv] outputs: [/backbone/layer3/layer3.1/conv2/Conv_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/bn2/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/conv2/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.1.bn2.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.1.bn2.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.1.bn2.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer3.1.bn2.running_var
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/bn2/BatchNormalization [BatchNormalization] inputs: [/backbone/layer3/layer3.1/conv2/Conv_output_0 -> (1, 256, 138, 242)[FLOAT]], [backbone.layer3.1.bn2.weight -> (256)[FLOAT]], [backbone.layer3.1.bn2.bias -> (256)[FLOAT]], [backbone.layer3.1.bn2.running_mean -> (256)[FLOAT]], [backbone.layer3.1.bn2.running_var -> (256)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.1/bn2/BatchNormalization for ONNX node: /backbone/layer3/layer3.1/bn2/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.1/bn2/BatchNormalization_output_0 for ONNX tensor: /backbone/layer3/layer3.1/bn2/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/bn2/BatchNormalization [BatchNormalization] outputs: [/backbone/layer3/layer3.1/bn2/BatchNormalization_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/Add [Add]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/bn2/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.0/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/Add [Add] inputs: [/backbone/layer3/layer3.1/bn2/BatchNormalization_output_0 -> (1, 256, 138, 242)[FLOAT]], [/backbone/layer3/layer3.0/relu_1/Relu_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.1/Add for ONNX node: /backbone/layer3/layer3.1/Add
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.1/Add_output_0 for ONNX tensor: /backbone/layer3/layer3.1/Add_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/Add [Add] outputs: [/backbone/layer3/layer3.1/Add_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer3/layer3.1/relu_1/Relu [Relu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/Add_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/relu_1/Relu [Relu] inputs: [/backbone/layer3/layer3.1/Add_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer3/layer3.1/relu_1/Relu for ONNX node: /backbone/layer3/layer3.1/relu_1/Relu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer3/layer3.1/relu_1/Relu_output_0 for ONNX tensor: /backbone/layer3/layer3.1/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer3/layer3.1/relu_1/Relu [Relu] outputs: [/backbone/layer3/layer3.1/relu_1/Relu_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/conv1/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv1/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv1/_input_quantizer/Constant [Constant] outputs: [/backbone/layer4/layer4.0/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/conv1/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv1/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv1/_input_quantizer/Constant_1 [Constant] outputs: [/backbone/layer4/layer4.0/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/backbone/layer3/layer3.1/relu_1/Relu_output_0 -> (1, 256, 138, 242)[FLOAT]], [/backbone/layer4/layer4.0/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer4/layer4.0/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.0/conv1/_input_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer4/layer4.0/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.0/conv1/_input_quantizer/Constant_output_0 for ONNX node: /backbone/layer4/layer4.0/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.0/conv1/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer4/layer4.0/conv1/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer4/layer4.0/conv1/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv1/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer4/layer4.0/conv1/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], [/backbone/layer4/layer4.0/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer4/layer4.0/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.0/conv1/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer4/layer4.0/conv1/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer4/layer4.0/conv1/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/conv1/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv1/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv1/_weight_quantizer/Constant [Constant] outputs: [/backbone/layer4/layer4.0/conv1/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/conv1/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv1/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv1/_weight_quantizer/Constant_1 [Constant] outputs: [/backbone/layer4/layer4.0/conv1/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.0.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [backbone.layer4.0.conv1.weight -> (512, 256, 3, 3)[FLOAT]], [/backbone/layer4/layer4.0/conv1/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], [/backbone/layer4/layer4.0/conv1/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer4.0.conv1.weight for ONNX node: backbone.layer4.0.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.0/conv1/_weight_quantizer/Constant_output_0 for ONNX node: /backbone/layer4/layer4.0/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.0/conv1/_weight_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer4/layer4.0/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], [/backbone/layer4/layer4.0/conv1/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], [/backbone/layer4/layer4.0/conv1/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.0/conv1/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer4/layer4.0/conv1/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer4/layer4.0/conv1/_weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/conv1/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv1/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv1/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv1/Conv [Conv] inputs: [/backbone/layer4/layer4.0/conv1/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], [/backbone/layer4/layer4.0/conv1/_weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.0/conv1/Conv for ONNX node: /backbone/layer4/layer4.0/conv1/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.0/conv1/Conv_output_0 for ONNX tensor: /backbone/layer4/layer4.0/conv1/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv1/Conv [Conv] outputs: [/backbone/layer4/layer4.0/conv1/Conv_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/bn1/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv1/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.0.bn1.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.0.bn1.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.0.bn1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.0.bn1.running_var
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/bn1/BatchNormalization [BatchNormalization] inputs: [/backbone/layer4/layer4.0/conv1/Conv_output_0 -> (1, 512, 69, 121)[FLOAT]], [backbone.layer4.0.bn1.weight -> (512)[FLOAT]], [backbone.layer4.0.bn1.bias -> (512)[FLOAT]], [backbone.layer4.0.bn1.running_mean -> (512)[FLOAT]], [backbone.layer4.0.bn1.running_var -> (512)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.0/bn1/BatchNormalization for ONNX node: /backbone/layer4/layer4.0/bn1/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.0/bn1/BatchNormalization_output_0 for ONNX tensor: /backbone/layer4/layer4.0/bn1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/bn1/BatchNormalization [BatchNormalization] outputs: [/backbone/layer4/layer4.0/bn1/BatchNormalization_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/relu/Relu [Relu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/bn1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/relu/Relu [Relu] inputs: [/backbone/layer4/layer4.0/bn1/BatchNormalization_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.0/relu/Relu for ONNX node: /backbone/layer4/layer4.0/relu/Relu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.0/relu/Relu_output_0 for ONNX tensor: /backbone/layer4/layer4.0/relu/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/relu/Relu [Relu] outputs: [/backbone/layer4/layer4.0/relu/Relu_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/conv2/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv2/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv2/_input_quantizer/Constant [Constant] outputs: [/backbone/layer4/layer4.0/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/conv2/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv2/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv2/_input_quantizer/Constant_1 [Constant] outputs: [/backbone/layer4/layer4.0/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/relu/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/backbone/layer4/layer4.0/relu/Relu_output_0 -> (1, 512, 69, 121)[FLOAT]], [/backbone/layer4/layer4.0/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer4/layer4.0/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.0/conv2/_input_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer4/layer4.0/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.0/conv2/_input_quantizer/Constant_output_0 for ONNX node: /backbone/layer4/layer4.0/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.0/conv2/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer4/layer4.0/conv2/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer4/layer4.0/conv2/_input_quantizer/QuantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv2/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer4/layer4.0/conv2/_input_quantizer/QuantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], [/backbone/layer4/layer4.0/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer4/layer4.0/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.0/conv2/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer4/layer4.0/conv2/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer4/layer4.0/conv2/_input_quantizer/DequantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/conv2/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv2/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv2/_weight_quantizer/Constant [Constant] outputs: [/backbone/layer4/layer4.0/conv2/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/conv2/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv2/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv2/_weight_quantizer/Constant_1 [Constant] outputs: [/backbone/layer4/layer4.0/conv2/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.0.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [backbone.layer4.0.conv2.weight -> (512, 512, 3, 3)[FLOAT]], [/backbone/layer4/layer4.0/conv2/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], [/backbone/layer4/layer4.0/conv2/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer4.0.conv2.weight for ONNX node: backbone.layer4.0.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.0/conv2/_weight_quantizer/Constant_output_0 for ONNX node: /backbone/layer4/layer4.0/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.0/conv2/_weight_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer4/layer4.0/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], [/backbone/layer4/layer4.0/conv2/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], [/backbone/layer4/layer4.0/conv2/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.0/conv2/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer4/layer4.0/conv2/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer4/layer4.0/conv2/_weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/conv2/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv2/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv2/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv2/Conv [Conv] inputs: [/backbone/layer4/layer4.0/conv2/_input_quantizer/DequantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], [/backbone/layer4/layer4.0/conv2/_weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.0/conv2/Conv for ONNX node: /backbone/layer4/layer4.0/conv2/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.0/conv2/Conv_output_0 for ONNX tensor: /backbone/layer4/layer4.0/conv2/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/conv2/Conv [Conv] outputs: [/backbone/layer4/layer4.0/conv2/Conv_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/bn2/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/conv2/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.0.bn2.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.0.bn2.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.0.bn2.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.0.bn2.running_var
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/bn2/BatchNormalization [BatchNormalization] inputs: [/backbone/layer4/layer4.0/conv2/Conv_output_0 -> (1, 512, 69, 121)[FLOAT]], [backbone.layer4.0.bn2.weight -> (512)[FLOAT]], [backbone.layer4.0.bn2.bias -> (512)[FLOAT]], [backbone.layer4.0.bn2.running_mean -> (512)[FLOAT]], [backbone.layer4.0.bn2.running_var -> (512)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.0/bn2/BatchNormalization for ONNX node: /backbone/layer4/layer4.0/bn2/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.0/bn2/BatchNormalization_output_0 for ONNX tensor: /backbone/layer4/layer4.0/bn2/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/bn2/BatchNormalization [BatchNormalization] outputs: [/backbone/layer4/layer4.0/bn2/BatchNormalization_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/Constant [Constant] outputs: [/backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/Constant_1 [Constant] outputs: [/backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/backbone/layer3/layer3.1/relu_1/Relu_output_0 -> (1, 256, 138, 242)[FLOAT]], [/backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/Constant_output_0 for ONNX node: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], [/backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/Constant [Constant] outputs: [/backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/Constant_1 [Constant] outputs: [/backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.0.downsample.0.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [backbone.layer4.0.downsample.0.weight -> (512, 256, 1, 1)[FLOAT]], [/backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], [/backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer4.0.downsample.0.weight for ONNX node: backbone.layer4.0.downsample.0.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/Constant_output_0 for ONNX node: /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 1, 1)[FLOAT]], [/backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], [/backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/downsample/downsample.0/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/downsample/downsample.0/Conv [Conv] inputs: [/backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], [/backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.0/downsample/downsample.0/Conv for ONNX node: /backbone/layer4/layer4.0/downsample/downsample.0/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 for ONNX tensor: /backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/downsample/downsample.0/Conv [Conv] outputs: [/backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/downsample/downsample.1/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.0.downsample.1.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.0.downsample.1.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.0.downsample.1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.0.downsample.1.running_var
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/downsample/downsample.1/BatchNormalization [BatchNormalization] inputs: [/backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> (1, 512, 69, 121)[FLOAT]], [backbone.layer4.0.downsample.1.weight -> (512)[FLOAT]], [backbone.layer4.0.downsample.1.bias -> (512)[FLOAT]], [backbone.layer4.0.downsample.1.running_mean -> (512)[FLOAT]], [backbone.layer4.0.downsample.1.running_var -> (512)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer4.0.downsample.1.weight for ONNX node: backbone.layer4.0.downsample.1.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer4.0.downsample.1.running_mean for ONNX node: backbone.layer4.0.downsample.1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer4.0.downsample.1.running_var for ONNX node: backbone.layer4.0.downsample.1.running_var
[10/31/2023-17:53:34] [V] [TRT] Original shape: (512,), unsqueezing to: (1, 512, 1, 1)
[10/31/2023-17:53:34] [V] [TRT] Original shape: (512,), unsqueezing to: (1, 512, 1, 1)
[10/31/2023-17:53:34] [V] [TRT] Original shape: (512,), unsqueezing to: (1, 512, 1, 1)
[10/31/2023-17:53:34] [V] [TRT] Original shape: (512,), unsqueezing to: (1, 512, 1, 1)
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.0/downsample/downsample.1/BatchNormalization for ONNX node: /backbone/layer4/layer4.0/downsample/downsample.1/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.0/downsample/downsample.1/BatchNormalization_output_0 for ONNX tensor: /backbone/layer4/layer4.0/downsample/downsample.1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/downsample/downsample.1/BatchNormalization [BatchNormalization] outputs: [/backbone/layer4/layer4.0/downsample/downsample.1/BatchNormalization_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/Add [Add]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/bn2/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/downsample/downsample.1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/Add [Add] inputs: [/backbone/layer4/layer4.0/bn2/BatchNormalization_output_0 -> (1, 512, 69, 121)[FLOAT]], [/backbone/layer4/layer4.0/downsample/downsample.1/BatchNormalization_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.0/Add for ONNX node: /backbone/layer4/layer4.0/Add
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.0/Add_output_0 for ONNX tensor: /backbone/layer4/layer4.0/Add_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/Add [Add] outputs: [/backbone/layer4/layer4.0/Add_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.0/relu_1/Relu [Relu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/Add_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/relu_1/Relu [Relu] inputs: [/backbone/layer4/layer4.0/Add_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.0/relu_1/Relu for ONNX node: /backbone/layer4/layer4.0/relu_1/Relu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.0/relu_1/Relu_output_0 for ONNX tensor: /backbone/layer4/layer4.0/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.0/relu_1/Relu [Relu] outputs: [/backbone/layer4/layer4.0/relu_1/Relu_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/conv1/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv1/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv1/_input_quantizer/Constant [Constant] outputs: [/backbone/layer4/layer4.1/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/conv1/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv1/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv1/_input_quantizer/Constant_1 [Constant] outputs: [/backbone/layer4/layer4.1/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/backbone/layer4/layer4.0/relu_1/Relu_output_0 -> (1, 512, 69, 121)[FLOAT]], [/backbone/layer4/layer4.1/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer4/layer4.1/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.1/conv1/_input_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer4/layer4.1/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.1/conv1/_input_quantizer/Constant_output_0 for ONNX node: /backbone/layer4/layer4.1/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], [/backbone/layer4/layer4.1/conv1/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer4/layer4.1/conv1/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.1/conv1/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer4/layer4.1/conv1/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv1/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer4/layer4.1/conv1/_input_quantizer/DequantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/conv1/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv1/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv1/_weight_quantizer/Constant [Constant] outputs: [/backbone/layer4/layer4.1/conv1/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/conv1/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv1/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv1/_weight_quantizer/Constant_1 [Constant] outputs: [/backbone/layer4/layer4.1/conv1/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.1.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [backbone.layer4.1.conv1.weight -> (512, 512, 3, 3)[FLOAT]], [/backbone/layer4/layer4.1/conv1/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], [/backbone/layer4/layer4.1/conv1/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer4.1.conv1.weight for ONNX node: backbone.layer4.1.conv1.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.1/conv1/_weight_quantizer/Constant_output_0 for ONNX node: /backbone/layer4/layer4.1/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.1/conv1/_weight_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer4/layer4.1/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], [/backbone/layer4/layer4.1/conv1/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], [/backbone/layer4/layer4.1/conv1/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.1/conv1/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer4/layer4.1/conv1/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv1/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer4/layer4.1/conv1/_weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/conv1/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv1/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv1/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv1/Conv [Conv] inputs: [/backbone/layer4/layer4.1/conv1/_input_quantizer/DequantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], [/backbone/layer4/layer4.1/conv1/_weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.1/conv1/Conv for ONNX node: /backbone/layer4/layer4.1/conv1/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.1/conv1/Conv_output_0 for ONNX tensor: /backbone/layer4/layer4.1/conv1/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv1/Conv [Conv] outputs: [/backbone/layer4/layer4.1/conv1/Conv_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/bn1/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv1/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.1.bn1.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.1.bn1.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.1.bn1.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.1.bn1.running_var
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/bn1/BatchNormalization [BatchNormalization] inputs: [/backbone/layer4/layer4.1/conv1/Conv_output_0 -> (1, 512, 69, 121)[FLOAT]], [backbone.layer4.1.bn1.weight -> (512)[FLOAT]], [backbone.layer4.1.bn1.bias -> (512)[FLOAT]], [backbone.layer4.1.bn1.running_mean -> (512)[FLOAT]], [backbone.layer4.1.bn1.running_var -> (512)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.1/bn1/BatchNormalization for ONNX node: /backbone/layer4/layer4.1/bn1/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.1/bn1/BatchNormalization_output_0 for ONNX tensor: /backbone/layer4/layer4.1/bn1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/bn1/BatchNormalization [BatchNormalization] outputs: [/backbone/layer4/layer4.1/bn1/BatchNormalization_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/relu/Relu [Relu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/bn1/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/relu/Relu [Relu] inputs: [/backbone/layer4/layer4.1/bn1/BatchNormalization_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.1/relu/Relu for ONNX node: /backbone/layer4/layer4.1/relu/Relu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.1/relu/Relu_output_0 for ONNX tensor: /backbone/layer4/layer4.1/relu/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/relu/Relu [Relu] outputs: [/backbone/layer4/layer4.1/relu/Relu_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/conv2/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv2/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv2/_input_quantizer/Constant [Constant] outputs: [/backbone/layer4/layer4.1/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/conv2/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv2/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv2/_input_quantizer/Constant_1 [Constant] outputs: [/backbone/layer4/layer4.1/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/relu/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/backbone/layer4/layer4.1/relu/Relu_output_0 -> (1, 512, 69, 121)[FLOAT]], [/backbone/layer4/layer4.1/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer4/layer4.1/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.1/conv2/_input_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer4/layer4.1/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.1/conv2/_input_quantizer/Constant_output_0 for ONNX node: /backbone/layer4/layer4.1/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.1/conv2/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer4/layer4.1/conv2/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv2/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer4/layer4.1/conv2/_input_quantizer/QuantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv2/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer4/layer4.1/conv2/_input_quantizer/QuantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], [/backbone/layer4/layer4.1/conv2/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/backbone/layer4/layer4.1/conv2/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.1/conv2/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer4/layer4.1/conv2/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv2/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer4/layer4.1/conv2/_input_quantizer/DequantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/conv2/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv2/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv2/_weight_quantizer/Constant [Constant] outputs: [/backbone/layer4/layer4.1/conv2/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/conv2/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv2/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv2/_weight_quantizer/Constant_1 [Constant] outputs: [/backbone/layer4/layer4.1/conv2/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.1.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [backbone.layer4.1.conv2.weight -> (512, 512, 3, 3)[FLOAT]], [/backbone/layer4/layer4.1/conv2/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], [/backbone/layer4/layer4.1/conv2/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: backbone.layer4.1.conv2.weight for ONNX node: backbone.layer4.1.conv2.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.1/conv2/_weight_quantizer/Constant_output_0 for ONNX node: /backbone/layer4/layer4.1/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.1/conv2/_weight_quantizer/Constant_1_output_0 for ONNX node: /backbone/layer4/layer4.1/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.1/conv2/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /backbone/layer4/layer4.1/conv2/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv2/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/backbone/layer4/layer4.1/conv2/_weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv2/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/backbone/layer4/layer4.1/conv2/_weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], [/backbone/layer4/layer4.1/conv2/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], [/backbone/layer4/layer4.1/conv2/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.1/conv2/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /backbone/layer4/layer4.1/conv2/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv2/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/backbone/layer4/layer4.1/conv2/_weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/conv2/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv2/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv2/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv2/Conv [Conv] inputs: [/backbone/layer4/layer4.1/conv2/_input_quantizer/DequantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], [/backbone/layer4/layer4.1/conv2/_weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.1/conv2/Conv for ONNX node: /backbone/layer4/layer4.1/conv2/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.1/conv2/Conv_output_0 for ONNX tensor: /backbone/layer4/layer4.1/conv2/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/conv2/Conv [Conv] outputs: [/backbone/layer4/layer4.1/conv2/Conv_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/bn2/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/conv2/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.1.bn2.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.1.bn2.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.1.bn2.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: backbone.layer4.1.bn2.running_var
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/bn2/BatchNormalization [BatchNormalization] inputs: [/backbone/layer4/layer4.1/conv2/Conv_output_0 -> (1, 512, 69, 121)[FLOAT]], [backbone.layer4.1.bn2.weight -> (512)[FLOAT]], [backbone.layer4.1.bn2.bias -> (512)[FLOAT]], [backbone.layer4.1.bn2.running_mean -> (512)[FLOAT]], [backbone.layer4.1.bn2.running_var -> (512)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.1/bn2/BatchNormalization for ONNX node: /backbone/layer4/layer4.1/bn2/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.1/bn2/BatchNormalization_output_0 for ONNX tensor: /backbone/layer4/layer4.1/bn2/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/bn2/BatchNormalization [BatchNormalization] outputs: [/backbone/layer4/layer4.1/bn2/BatchNormalization_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/Add [Add]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/bn2/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.0/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/Add [Add] inputs: [/backbone/layer4/layer4.1/bn2/BatchNormalization_output_0 -> (1, 512, 69, 121)[FLOAT]], [/backbone/layer4/layer4.0/relu_1/Relu_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.1/Add for ONNX node: /backbone/layer4/layer4.1/Add
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.1/Add_output_0 for ONNX tensor: /backbone/layer4/layer4.1/Add_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/Add [Add] outputs: [/backbone/layer4/layer4.1/Add_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /backbone/layer4/layer4.1/relu_1/Relu [Relu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/Add_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/relu_1/Relu [Relu] inputs: [/backbone/layer4/layer4.1/Add_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /backbone/layer4/layer4.1/relu_1/Relu for ONNX node: /backbone/layer4/layer4.1/relu_1/Relu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /backbone/layer4/layer4.1/relu_1/Relu_output_0 for ONNX tensor: /backbone/layer4/layer4.1/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /backbone/layer4/layer4.1/relu_1/Relu [Relu] outputs: [/backbone/layer4/layer4.1/relu_1/Relu_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv1/conv/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv1/conv/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv1/conv/_input_quantizer/Constant [Constant] outputs: [/neck/detect1/conv1/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv1/conv/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv1/conv/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv1/conv/_input_quantizer/Constant_1 [Constant] outputs: [/neck/detect1/conv1/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv1/conv/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer4/layer4.1/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv1/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv1/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv1/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/backbone/layer4/layer4.1/relu_1/Relu_output_0 -> (1, 512, 69, 121)[FLOAT]], [/neck/detect1/conv1/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect1/conv1/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv1/conv/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/detect1/conv1/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv1/conv/_input_quantizer/Constant_output_0 for ONNX node: /neck/detect1/conv1/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv1/conv/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv1/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv1/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect1/conv1/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv1/conv/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv1/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv1/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv1/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv1/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect1/conv1/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], [/neck/detect1/conv1/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect1/conv1/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv1/conv/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv1/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv1/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect1/conv1/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv1/conv/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv1/conv/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv1/conv/_weight_quantizer/Constant [Constant] outputs: [/neck/detect1/conv1/conv/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv1/conv/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv1/conv/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv1/conv/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/detect1/conv1/conv/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv1.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv1/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv1/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.detect1.conv1.conv.weight -> (256, 512, 1, 1)[FLOAT]], [/neck/detect1/conv1/conv/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], [/neck/detect1/conv1/conv/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: neck.detect1.conv1.conv.weight for ONNX node: neck.detect1.conv1.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv1/conv/_weight_quantizer/Constant_output_0 for ONNX node: /neck/detect1/conv1/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv1/conv/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/detect1/conv1/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear_output_0 -> (256, 512, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv1/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv1/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv1/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv1/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear_output_0 -> (256, 512, 1, 1)[FLOAT]], [/neck/detect1/conv1/conv/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], [/neck/detect1/conv1/conv/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv1/conv/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv1/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv1/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect1/conv1/conv/_weight_quantizer/DequantizeLinear_output_0 -> (256, 512, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv1/conv/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv1/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv1/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv1/conv/Conv [Conv] inputs: [/neck/detect1/conv1/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], [/neck/detect1/conv1/conv/_weight_quantizer/DequantizeLinear_output_0 -> (256, 512, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv1/conv/Conv for ONNX node: /neck/detect1/conv1/conv/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv1/conv/Conv_output_0 for ONNX tensor: /neck/detect1/conv1/conv/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv1/conv/Conv [Conv] outputs: [/neck/detect1/conv1/conv/Conv_output_0 -> (1, 256, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv1/bn/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv1/conv/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv1.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv1.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv1.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv1.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv1/bn/BatchNormalization [BatchNormalization] inputs: [/neck/detect1/conv1/conv/Conv_output_0 -> (1, 256, 69, 121)[FLOAT]], [neck.detect1.conv1.bn.weight -> (256)[FLOAT]], [neck.detect1.conv1.bn.bias -> (256)[FLOAT]], [neck.detect1.conv1.bn.running_mean -> (256)[FLOAT]], [neck.detect1.conv1.bn.running_var -> (256)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv1/bn/BatchNormalization for ONNX node: /neck/detect1/conv1/bn/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv1/bn/BatchNormalization_output_0 for ONNX tensor: /neck/detect1/conv1/bn/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv1/bn/BatchNormalization [BatchNormalization] outputs: [/neck/detect1/conv1/bn/BatchNormalization_output_0 -> (1, 256, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv1/relu/LeakyRelu [LeakyRelu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv1/bn/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv1/relu/LeakyRelu [LeakyRelu] inputs: [/neck/detect1/conv1/bn/BatchNormalization_output_0 -> (1, 256, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv1/relu/LeakyRelu for ONNX node: /neck/detect1/conv1/relu/LeakyRelu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv1/relu/LeakyRelu_output_0 for ONNX tensor: /neck/detect1/conv1/relu/LeakyRelu_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv1/relu/LeakyRelu [LeakyRelu] outputs: [/neck/detect1/conv1/relu/LeakyRelu_output_0 -> (1, 256, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv2/conv/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv2/conv/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv2/conv/_input_quantizer/Constant [Constant] outputs: [/neck/detect1/conv2/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv2/conv/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv2/conv/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv2/conv/_input_quantizer/Constant_1 [Constant] outputs: [/neck/detect1/conv2/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv2/conv/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv1/relu/LeakyRelu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv2/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv2/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv2/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/neck/detect1/conv1/relu/LeakyRelu_output_0 -> (1, 256, 69, 121)[FLOAT]], [/neck/detect1/conv2/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect1/conv2/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv2/conv/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/detect1/conv2/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv2/conv/_input_quantizer/Constant_output_0 for ONNX node: /neck/detect1/conv2/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv2/conv/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv2/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv2/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect1/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv2/conv/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv2/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv2/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv2/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv2/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect1/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 69, 121)[FLOAT]], [/neck/detect1/conv2/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect1/conv2/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv2/conv/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv2/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv2/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect1/conv2/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv2/conv/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv2/conv/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv2/conv/_weight_quantizer/Constant [Constant] outputs: [/neck/detect1/conv2/conv/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv2/conv/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv2/conv/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv2/conv/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/detect1/conv2/conv/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv2.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv2/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv2/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.detect1.conv2.conv.weight -> (512, 256, 3, 3)[FLOAT]], [/neck/detect1/conv2/conv/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], [/neck/detect1/conv2/conv/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: neck.detect1.conv2.conv.weight for ONNX node: neck.detect1.conv2.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv2/conv/_weight_quantizer/Constant_output_0 for ONNX node: /neck/detect1/conv2/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv2/conv/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/detect1/conv2/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv2/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv2/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv2/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv2/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], [/neck/detect1/conv2/conv/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], [/neck/detect1/conv2/conv/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv2/conv/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv2/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv2/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect1/conv2/conv/_weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv2/conv/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv2/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv2/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv2/conv/Conv [Conv] inputs: [/neck/detect1/conv2/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 69, 121)[FLOAT]], [/neck/detect1/conv2/conv/_weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv2/conv/Conv for ONNX node: /neck/detect1/conv2/conv/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv2/conv/Conv_output_0 for ONNX tensor: /neck/detect1/conv2/conv/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv2/conv/Conv [Conv] outputs: [/neck/detect1/conv2/conv/Conv_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv2/bn/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv2/conv/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv2.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv2.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv2.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv2.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv2/bn/BatchNormalization [BatchNormalization] inputs: [/neck/detect1/conv2/conv/Conv_output_0 -> (1, 512, 69, 121)[FLOAT]], [neck.detect1.conv2.bn.weight -> (512)[FLOAT]], [neck.detect1.conv2.bn.bias -> (512)[FLOAT]], [neck.detect1.conv2.bn.running_mean -> (512)[FLOAT]], [neck.detect1.conv2.bn.running_var -> (512)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv2/bn/BatchNormalization for ONNX node: /neck/detect1/conv2/bn/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv2/bn/BatchNormalization_output_0 for ONNX tensor: /neck/detect1/conv2/bn/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv2/bn/BatchNormalization [BatchNormalization] outputs: [/neck/detect1/conv2/bn/BatchNormalization_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv2/relu/LeakyRelu [LeakyRelu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv2/bn/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv2/relu/LeakyRelu [LeakyRelu] inputs: [/neck/detect1/conv2/bn/BatchNormalization_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv2/relu/LeakyRelu for ONNX node: /neck/detect1/conv2/relu/LeakyRelu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv2/relu/LeakyRelu_output_0 for ONNX tensor: /neck/detect1/conv2/relu/LeakyRelu_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv2/relu/LeakyRelu [LeakyRelu] outputs: [/neck/detect1/conv2/relu/LeakyRelu_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv3/conv/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv3/conv/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv3/conv/_input_quantizer/Constant [Constant] outputs: [/neck/detect1/conv3/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv3/conv/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv3/conv/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv3/conv/_input_quantizer/Constant_1 [Constant] outputs: [/neck/detect1/conv3/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv3/conv/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv2/relu/LeakyRelu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv3/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv3/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv3/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/neck/detect1/conv2/relu/LeakyRelu_output_0 -> (1, 512, 69, 121)[FLOAT]], [/neck/detect1/conv3/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect1/conv3/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv3/conv/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/detect1/conv3/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv3/conv/_input_quantizer/Constant_output_0 for ONNX node: /neck/detect1/conv3/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv3/conv/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv3/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv3/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect1/conv3/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv3/conv/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv3/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv3/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv3/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv3/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect1/conv3/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], [/neck/detect1/conv3/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect1/conv3/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv3/conv/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv3/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv3/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect1/conv3/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv3/conv/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv3/conv/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv3/conv/_weight_quantizer/Constant [Constant] outputs: [/neck/detect1/conv3/conv/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv3/conv/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv3/conv/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv3/conv/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/detect1/conv3/conv/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv3.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv3/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv3/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.detect1.conv3.conv.weight -> (256, 512, 1, 1)[FLOAT]], [/neck/detect1/conv3/conv/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], [/neck/detect1/conv3/conv/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: neck.detect1.conv3.conv.weight for ONNX node: neck.detect1.conv3.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv3/conv/_weight_quantizer/Constant_output_0 for ONNX node: /neck/detect1/conv3/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv3/conv/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/detect1/conv3/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear_output_0 -> (256, 512, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv3/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv3/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv3/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv3/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear_output_0 -> (256, 512, 1, 1)[FLOAT]], [/neck/detect1/conv3/conv/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], [/neck/detect1/conv3/conv/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv3/conv/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv3/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv3/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect1/conv3/conv/_weight_quantizer/DequantizeLinear_output_0 -> (256, 512, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv3/conv/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv3/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv3/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv3/conv/Conv [Conv] inputs: [/neck/detect1/conv3/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], [/neck/detect1/conv3/conv/_weight_quantizer/DequantizeLinear_output_0 -> (256, 512, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv3/conv/Conv for ONNX node: /neck/detect1/conv3/conv/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv3/conv/Conv_output_0 for ONNX tensor: /neck/detect1/conv3/conv/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv3/conv/Conv [Conv] outputs: [/neck/detect1/conv3/conv/Conv_output_0 -> (1, 256, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv3/bn/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv3/conv/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv3.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv3.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv3.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv3.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv3/bn/BatchNormalization [BatchNormalization] inputs: [/neck/detect1/conv3/conv/Conv_output_0 -> (1, 256, 69, 121)[FLOAT]], [neck.detect1.conv3.bn.weight -> (256)[FLOAT]], [neck.detect1.conv3.bn.bias -> (256)[FLOAT]], [neck.detect1.conv3.bn.running_mean -> (256)[FLOAT]], [neck.detect1.conv3.bn.running_var -> (256)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv3/bn/BatchNormalization for ONNX node: /neck/detect1/conv3/bn/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv3/bn/BatchNormalization_output_0 for ONNX tensor: /neck/detect1/conv3/bn/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv3/bn/BatchNormalization [BatchNormalization] outputs: [/neck/detect1/conv3/bn/BatchNormalization_output_0 -> (1, 256, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv3/relu/LeakyRelu [LeakyRelu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv3/bn/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv3/relu/LeakyRelu [LeakyRelu] inputs: [/neck/detect1/conv3/bn/BatchNormalization_output_0 -> (1, 256, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv3/relu/LeakyRelu for ONNX node: /neck/detect1/conv3/relu/LeakyRelu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv3/relu/LeakyRelu_output_0 for ONNX tensor: /neck/detect1/conv3/relu/LeakyRelu_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv3/relu/LeakyRelu [LeakyRelu] outputs: [/neck/detect1/conv3/relu/LeakyRelu_output_0 -> (1, 256, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv4/conv/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv4/conv/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv4/conv/_input_quantizer/Constant [Constant] outputs: [/neck/detect1/conv4/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv4/conv/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv4/conv/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv4/conv/_input_quantizer/Constant_1 [Constant] outputs: [/neck/detect1/conv4/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv4/conv/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv3/relu/LeakyRelu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv4/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv4/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv4/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/neck/detect1/conv3/relu/LeakyRelu_output_0 -> (1, 256, 69, 121)[FLOAT]], [/neck/detect1/conv4/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect1/conv4/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv4/conv/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/detect1/conv4/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv4/conv/_input_quantizer/Constant_output_0 for ONNX node: /neck/detect1/conv4/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv4/conv/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv4/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv4/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect1/conv4/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv4/conv/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv4/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv4/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv4/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv4/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect1/conv4/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 69, 121)[FLOAT]], [/neck/detect1/conv4/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect1/conv4/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv4/conv/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv4/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv4/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect1/conv4/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv4/conv/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv4/conv/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv4/conv/_weight_quantizer/Constant [Constant] outputs: [/neck/detect1/conv4/conv/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv4/conv/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv4/conv/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv4/conv/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/detect1/conv4/conv/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv4.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv4/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv4/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.detect1.conv4.conv.weight -> (512, 256, 3, 3)[FLOAT]], [/neck/detect1/conv4/conv/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], [/neck/detect1/conv4/conv/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: neck.detect1.conv4.conv.weight for ONNX node: neck.detect1.conv4.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv4/conv/_weight_quantizer/Constant_output_0 for ONNX node: /neck/detect1/conv4/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv4/conv/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/detect1/conv4/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv4/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv4/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv4/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv4/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], [/neck/detect1/conv4/conv/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], [/neck/detect1/conv4/conv/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv4/conv/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv4/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv4/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect1/conv4/conv/_weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv4/conv/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv4/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv4/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv4/conv/Conv [Conv] inputs: [/neck/detect1/conv4/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 69, 121)[FLOAT]], [/neck/detect1/conv4/conv/_weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv4/conv/Conv for ONNX node: /neck/detect1/conv4/conv/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv4/conv/Conv_output_0 for ONNX tensor: /neck/detect1/conv4/conv/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv4/conv/Conv [Conv] outputs: [/neck/detect1/conv4/conv/Conv_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv4/bn/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv4/conv/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv4.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv4.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv4.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv4.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv4/bn/BatchNormalization [BatchNormalization] inputs: [/neck/detect1/conv4/conv/Conv_output_0 -> (1, 512, 69, 121)[FLOAT]], [neck.detect1.conv4.bn.weight -> (512)[FLOAT]], [neck.detect1.conv4.bn.bias -> (512)[FLOAT]], [neck.detect1.conv4.bn.running_mean -> (512)[FLOAT]], [neck.detect1.conv4.bn.running_var -> (512)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv4/bn/BatchNormalization for ONNX node: /neck/detect1/conv4/bn/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv4/bn/BatchNormalization_output_0 for ONNX tensor: /neck/detect1/conv4/bn/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv4/bn/BatchNormalization [BatchNormalization] outputs: [/neck/detect1/conv4/bn/BatchNormalization_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv4/relu/LeakyRelu [LeakyRelu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv4/bn/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv4/relu/LeakyRelu [LeakyRelu] inputs: [/neck/detect1/conv4/bn/BatchNormalization_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv4/relu/LeakyRelu for ONNX node: /neck/detect1/conv4/relu/LeakyRelu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv4/relu/LeakyRelu_output_0 for ONNX tensor: /neck/detect1/conv4/relu/LeakyRelu_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv4/relu/LeakyRelu [LeakyRelu] outputs: [/neck/detect1/conv4/relu/LeakyRelu_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv5/conv/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv5/conv/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv5/conv/_input_quantizer/Constant [Constant] outputs: [/neck/detect1/conv5/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv5/conv/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv5/conv/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv5/conv/_input_quantizer/Constant_1 [Constant] outputs: [/neck/detect1/conv5/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv5/conv/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv4/relu/LeakyRelu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv5/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv5/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv5/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/neck/detect1/conv4/relu/LeakyRelu_output_0 -> (1, 512, 69, 121)[FLOAT]], [/neck/detect1/conv5/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect1/conv5/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv5/conv/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/detect1/conv5/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv5/conv/_input_quantizer/Constant_output_0 for ONNX node: /neck/detect1/conv5/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv5/conv/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv5/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv5/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect1/conv5/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv5/conv/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv5/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv5/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv5/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv5/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect1/conv5/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], [/neck/detect1/conv5/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect1/conv5/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv5/conv/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv5/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv5/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect1/conv5/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv5/conv/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv5/conv/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv5/conv/_weight_quantizer/Constant [Constant] outputs: [/neck/detect1/conv5/conv/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv5/conv/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv5/conv/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv5/conv/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/detect1/conv5/conv/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv5.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv5/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv5/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.detect1.conv5.conv.weight -> (256, 512, 1, 1)[FLOAT]], [/neck/detect1/conv5/conv/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], [/neck/detect1/conv5/conv/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: neck.detect1.conv5.conv.weight for ONNX node: neck.detect1.conv5.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv5/conv/_weight_quantizer/Constant_output_0 for ONNX node: /neck/detect1/conv5/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv5/conv/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/detect1/conv5/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear_output_0 -> (256, 512, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv5/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv5/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv5/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv5/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear_output_0 -> (256, 512, 1, 1)[FLOAT]], [/neck/detect1/conv5/conv/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], [/neck/detect1/conv5/conv/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv5/conv/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv5/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv5/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect1/conv5/conv/_weight_quantizer/DequantizeLinear_output_0 -> (256, 512, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv5/conv/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv5/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv5/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv5/conv/Conv [Conv] inputs: [/neck/detect1/conv5/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], [/neck/detect1/conv5/conv/_weight_quantizer/DequantizeLinear_output_0 -> (256, 512, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv5/conv/Conv for ONNX node: /neck/detect1/conv5/conv/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv5/conv/Conv_output_0 for ONNX tensor: /neck/detect1/conv5/conv/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv5/conv/Conv [Conv] outputs: [/neck/detect1/conv5/conv/Conv_output_0 -> (1, 256, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv5/bn/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv5/conv/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv5.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv5.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv5.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv5.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv5/bn/BatchNormalization [BatchNormalization] inputs: [/neck/detect1/conv5/conv/Conv_output_0 -> (1, 256, 69, 121)[FLOAT]], [neck.detect1.conv5.bn.weight -> (256)[FLOAT]], [neck.detect1.conv5.bn.bias -> (256)[FLOAT]], [neck.detect1.conv5.bn.running_mean -> (256)[FLOAT]], [neck.detect1.conv5.bn.running_var -> (256)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv5/bn/BatchNormalization for ONNX node: /neck/detect1/conv5/bn/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv5/bn/BatchNormalization_output_0 for ONNX tensor: /neck/detect1/conv5/bn/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv5/bn/BatchNormalization [BatchNormalization] outputs: [/neck/detect1/conv5/bn/BatchNormalization_output_0 -> (1, 256, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv5/relu/LeakyRelu [LeakyRelu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv5/bn/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv5/relu/LeakyRelu [LeakyRelu] inputs: [/neck/detect1/conv5/bn/BatchNormalization_output_0 -> (1, 256, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv5/relu/LeakyRelu for ONNX node: /neck/detect1/conv5/relu/LeakyRelu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv5/relu/LeakyRelu_output_0 for ONNX tensor: /neck/detect1/conv5/relu/LeakyRelu_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv5/relu/LeakyRelu [LeakyRelu] outputs: [/neck/detect1/conv5/relu/LeakyRelu_output_0 -> (1, 256, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv6/conv/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv6/conv/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv6/conv/_input_quantizer/Constant [Constant] outputs: [/neck/detect1/conv6/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv6/conv/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv6/conv/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv6/conv/_input_quantizer/Constant_1 [Constant] outputs: [/neck/detect1/conv6/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv6/conv/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv5/relu/LeakyRelu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv6/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv6/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv6/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/neck/detect1/conv5/relu/LeakyRelu_output_0 -> (1, 256, 69, 121)[FLOAT]], [/neck/detect1/conv6/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect1/conv6/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv6/conv/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/detect1/conv6/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv6/conv/_input_quantizer/Constant_output_0 for ONNX node: /neck/detect1/conv6/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv6/conv/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv6/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv6/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect1/conv6/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv6/conv/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv6/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv6/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv6/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv6/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect1/conv6/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 69, 121)[FLOAT]], [/neck/detect1/conv6/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect1/conv6/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv6/conv/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv6/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv6/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect1/conv6/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv6/conv/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv6/conv/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv6/conv/_weight_quantizer/Constant [Constant] outputs: [/neck/detect1/conv6/conv/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv6/conv/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv6/conv/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv6/conv/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/detect1/conv6/conv/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv6.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv6/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv6/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.detect1.conv6.conv.weight -> (512, 256, 3, 3)[FLOAT]], [/neck/detect1/conv6/conv/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], [/neck/detect1/conv6/conv/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: neck.detect1.conv6.conv.weight for ONNX node: neck.detect1.conv6.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv6/conv/_weight_quantizer/Constant_output_0 for ONNX node: /neck/detect1/conv6/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv6/conv/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/detect1/conv6/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv6/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv6/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv6/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv6/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], [/neck/detect1/conv6/conv/_weight_quantizer/Constant_output_0 -> (512)[FLOAT]], [/neck/detect1/conv6/conv/_weight_quantizer/Constant_1_output_0 -> (512)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv6/conv/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv6/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv6/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect1/conv6/conv/_weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv6/conv/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv6/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv6/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv6/conv/Conv [Conv] inputs: [/neck/detect1/conv6/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 69, 121)[FLOAT]], [/neck/detect1/conv6/conv/_weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv6/conv/Conv for ONNX node: /neck/detect1/conv6/conv/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv6/conv/Conv_output_0 for ONNX tensor: /neck/detect1/conv6/conv/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv6/conv/Conv [Conv] outputs: [/neck/detect1/conv6/conv/Conv_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv6/bn/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv6/conv/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv6.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv6.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv6.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv6.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv6/bn/BatchNormalization [BatchNormalization] inputs: [/neck/detect1/conv6/conv/Conv_output_0 -> (1, 512, 69, 121)[FLOAT]], [neck.detect1.conv6.bn.weight -> (512)[FLOAT]], [neck.detect1.conv6.bn.bias -> (512)[FLOAT]], [neck.detect1.conv6.bn.running_mean -> (512)[FLOAT]], [neck.detect1.conv6.bn.running_var -> (512)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv6/bn/BatchNormalization for ONNX node: /neck/detect1/conv6/bn/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv6/bn/BatchNormalization_output_0 for ONNX tensor: /neck/detect1/conv6/bn/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv6/bn/BatchNormalization [BatchNormalization] outputs: [/neck/detect1/conv6/bn/BatchNormalization_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv6/relu/LeakyRelu [LeakyRelu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv6/bn/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv6/relu/LeakyRelu [LeakyRelu] inputs: [/neck/detect1/conv6/bn/BatchNormalization_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv6/relu/LeakyRelu for ONNX node: /neck/detect1/conv6/relu/LeakyRelu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv6/relu/LeakyRelu_output_0 for ONNX tensor: /neck/detect1/conv6/relu/LeakyRelu_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv6/relu/LeakyRelu [LeakyRelu] outputs: [/neck/detect1/conv6/relu/LeakyRelu_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv7/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv7/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv7/_input_quantizer/Constant [Constant] outputs: [/neck/detect1/conv7/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv7/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv7/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv7/_input_quantizer/Constant_1 [Constant] outputs: [/neck/detect1/conv7/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv7/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv6/relu/LeakyRelu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv7/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv7/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv7/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/neck/detect1/conv6/relu/LeakyRelu_output_0 -> (1, 512, 69, 121)[FLOAT]], [/neck/detect1/conv7/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect1/conv7/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv7/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/detect1/conv7/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv7/_input_quantizer/Constant_output_0 for ONNX node: /neck/detect1/conv7/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv7/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv7/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv7/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect1/conv7/_input_quantizer/QuantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv7/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv7/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv7/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv7/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv7/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect1/conv7/_input_quantizer/QuantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], [/neck/detect1/conv7/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect1/conv7/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv7/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv7/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv7/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect1/conv7/_input_quantizer/DequantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv7/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv7/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv7/_weight_quantizer/Constant [Constant] outputs: [/neck/detect1/conv7/_weight_quantizer/Constant_output_0 -> (63)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv7/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv7/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv7/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/detect1/conv7/_weight_quantizer/Constant_1_output_0 -> (63)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv7/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv7.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv7/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv7/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv7/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.detect1.conv7.weight -> (63, 512, 1, 1)[FLOAT]], [/neck/detect1/conv7/_weight_quantizer/Constant_output_0 -> (63)[FLOAT]], [/neck/detect1/conv7/_weight_quantizer/Constant_1_output_0 -> (63)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: neck.detect1.conv7.weight for ONNX node: neck.detect1.conv7.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv7/_weight_quantizer/Constant_output_0 for ONNX node: /neck/detect1/conv7/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv7/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/detect1/conv7/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv7/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv7/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv7/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect1/conv7/_weight_quantizer/QuantizeLinear_output_0 -> (63, 512, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv7/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv7/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv7/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv7/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv7/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect1/conv7/_weight_quantizer/QuantizeLinear_output_0 -> (63, 512, 1, 1)[FLOAT]], [/neck/detect1/conv7/_weight_quantizer/Constant_output_0 -> (63)[FLOAT]], [/neck/detect1/conv7/_weight_quantizer/Constant_1_output_0 -> (63)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv7/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect1/conv7/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv7/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect1/conv7/_weight_quantizer/DequantizeLinear_output_0 -> (63, 512, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect1/conv7/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv7/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv7/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect1.conv7.bias
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv7/Conv [Conv] inputs: [/neck/detect1/conv7/_input_quantizer/DequantizeLinear_output_0 -> (1, 512, 69, 121)[FLOAT]], [/neck/detect1/conv7/_weight_quantizer/DequantizeLinear_output_0 -> (63, 512, 1, 1)[FLOAT]], [neck.detect1.conv7.bias -> (63)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect1/conv7/Conv for ONNX node: /neck/detect1/conv7/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect1/conv7/Conv_output_0 for ONNX tensor: /neck/detect1/conv7/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect1/conv7/Conv [Conv] outputs: [/neck/detect1/conv7/Conv_output_0 -> (1, 63, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/conv1/conv/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/conv1/conv/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/conv1/conv/_input_quantizer/Constant [Constant] outputs: [/neck/conv1/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/conv1/conv/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/conv1/conv/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/conv1/conv/_input_quantizer/Constant_1 [Constant] outputs: [/neck/conv1/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/conv1/conv/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect1/conv5/relu/LeakyRelu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/conv1/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/conv1/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/conv1/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/neck/detect1/conv5/relu/LeakyRelu_output_0 -> (1, 256, 69, 121)[FLOAT]], [/neck/conv1/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/conv1/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/conv1/conv/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/conv1/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/conv1/conv/_input_quantizer/Constant_output_0 for ONNX node: /neck/conv1/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/conv1/conv/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/conv1/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/conv1/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/conv1/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/conv1/conv/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/conv1/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/conv1/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/conv1/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/conv1/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/conv1/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 69, 121)[FLOAT]], [/neck/conv1/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/conv1/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/conv1/conv/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/conv1/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/conv1/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/conv1/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/conv1/conv/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/conv1/conv/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/conv1/conv/_weight_quantizer/Constant [Constant] outputs: [/neck/conv1/conv/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/conv1/conv/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/conv1/conv/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/conv1/conv/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/conv1/conv/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/conv1/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.conv1.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/conv1/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/conv1/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/conv1/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.conv1.conv.weight -> (128, 256, 1, 1)[FLOAT]], [/neck/conv1/conv/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/neck/conv1/conv/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: neck.conv1.conv.weight for ONNX node: neck.conv1.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/conv1/conv/_weight_quantizer/Constant_output_0 for ONNX node: /neck/conv1/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/conv1/conv/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/conv1/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/conv1/conv/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/conv1/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/conv1/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/conv1/conv/_weight_quantizer/QuantizeLinear_output_0 -> (128, 256, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/conv1/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/conv1/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/conv1/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/conv1/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/conv1/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/conv1/conv/_weight_quantizer/QuantizeLinear_output_0 -> (128, 256, 1, 1)[FLOAT]], [/neck/conv1/conv/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/neck/conv1/conv/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/conv1/conv/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/conv1/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/conv1/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/conv1/conv/_weight_quantizer/DequantizeLinear_output_0 -> (128, 256, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/conv1/conv/Conv [Conv]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/conv1/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/conv1/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/conv1/conv/Conv [Conv] inputs: [/neck/conv1/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 69, 121)[FLOAT]], [/neck/conv1/conv/_weight_quantizer/DequantizeLinear_output_0 -> (128, 256, 1, 1)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/conv1/conv/Conv for ONNX node: /neck/conv1/conv/Conv
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/conv1/conv/Conv_output_0 for ONNX tensor: /neck/conv1/conv/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/conv1/conv/Conv [Conv] outputs: [/neck/conv1/conv/Conv_output_0 -> (1, 128, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/conv1/bn/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/conv1/conv/Conv_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.conv1.bn.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.conv1.bn.bias
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.conv1.bn.running_mean
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.conv1.bn.running_var
[10/31/2023-17:53:34] [V] [TRT] /neck/conv1/bn/BatchNormalization [BatchNormalization] inputs: [/neck/conv1/conv/Conv_output_0 -> (1, 128, 69, 121)[FLOAT]], [neck.conv1.bn.weight -> (128)[FLOAT]], [neck.conv1.bn.bias -> (128)[FLOAT]], [neck.conv1.bn.running_mean -> (128)[FLOAT]], [neck.conv1.bn.running_var -> (128)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/conv1/bn/BatchNormalization for ONNX node: /neck/conv1/bn/BatchNormalization
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/conv1/bn/BatchNormalization_output_0 for ONNX tensor: /neck/conv1/bn/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/conv1/bn/BatchNormalization [BatchNormalization] outputs: [/neck/conv1/bn/BatchNormalization_output_0 -> (1, 128, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/conv1/relu/LeakyRelu [LeakyRelu]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/conv1/bn/BatchNormalization_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/conv1/relu/LeakyRelu [LeakyRelu] inputs: [/neck/conv1/bn/BatchNormalization_output_0 -> (1, 128, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/conv1/relu/LeakyRelu for ONNX node: /neck/conv1/relu/LeakyRelu
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/conv1/relu/LeakyRelu_output_0 for ONNX tensor: /neck/conv1/relu/LeakyRelu_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/conv1/relu/LeakyRelu [LeakyRelu] outputs: [/neck/conv1/relu/LeakyRelu_output_0 -> (1, 128, 69, 121)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/Constant [Constant] outputs: [/neck/Constant_output_0 -> (4)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/Resize [Resize]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/conv1/relu/LeakyRelu_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/Resize [Resize] inputs: [/neck/conv1/relu/LeakyRelu_output_0 -> (1, 128, 69, 121)[FLOAT]], [optional input, not set], [/neck/Constant_output_0 -> (4)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/Resize for ONNX node: /neck/Resize
[10/31/2023-17:53:34] [V] [TRT] Running resize layer with: 
Transformation mode: asymmetric
Resize mode: nearest

[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/Resize_output_0 for ONNX tensor: /neck/Resize_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/Resize [Resize] outputs: [/neck/Resize_output_0 -> (1, 128, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/Concat [Concat]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/Resize_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /backbone/layer3/layer3.1/relu_1/Relu_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/Concat [Concat] inputs: [/neck/Resize_output_0 -> (1, 128, 138, 242)[FLOAT]], [/backbone/layer3/layer3.1/relu_1/Relu_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/Concat for ONNX node: /neck/Concat
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/Concat_output_0 for ONNX tensor: /neck/Concat_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/Concat [Concat] outputs: [/neck/Concat_output_0 -> (1, 384, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect2/conv1/conv/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect2/conv1/conv/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect2/conv1/conv/_input_quantizer/Constant [Constant] outputs: [/neck/detect2/conv1/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect2/conv1/conv/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect2/conv1/conv/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect2/conv1/conv/_input_quantizer/Constant_1 [Constant] outputs: [/neck/detect2/conv1/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/Concat_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect2/conv1/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect2/conv1/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/neck/Concat_output_0 -> (1, 384, 138, 242)[FLOAT]], [/neck/detect2/conv1/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect2/conv1/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect2/conv1/conv/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/detect2/conv1/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect2/conv1/conv/_input_quantizer/Constant_output_0 for ONNX node: /neck/detect2/conv1/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 384, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect2/conv1/conv/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect2/conv1/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect2/conv1/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect2/conv1/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 384, 138, 242)[FLOAT]], [/neck/detect2/conv1/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect2/conv1/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering tensor: /neck/detect2/conv1/conv/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv1/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect2/conv1/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect2/conv1/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 384, 138, 242)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect2/conv1/conv/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect2/conv1/conv/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect2/conv1/conv/_weight_quantizer/Constant [Constant] outputs: [/neck/detect2/conv1/conv/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect2/conv1/conv/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:34] [V] [TRT] /neck/detect2/conv1/conv/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:34] [V] [TRT] /neck/detect2/conv1/conv/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/detect2/conv1/conv/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Parsing node: /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:34] [V] [TRT] Searching for input: neck.detect2.conv1.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect2/conv1/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Searching for input: /neck/detect2/conv1/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:34] [V] [TRT] /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.detect2.conv1.conv.weight -> (128, 384, 1, 1)[FLOAT]], [/neck/detect2/conv1/conv/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/neck/detect2/conv1/conv/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:34] [V] [TRT] Registering layer: neck.detect2.conv1.conv.weight for ONNX node: neck.detect2.conv1.conv.weight
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect2/conv1/conv/_weight_quantizer/Constant_output_0 for ONNX node: /neck/detect2/conv1/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:34] [V] [TRT] Registering layer: /neck/detect2/conv1/conv/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/detect2/conv1/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear_output_0 -> (128, 384, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv1/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv1/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv1/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv1/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear_output_0 -> (128, 384, 1, 1)[FLOAT]], [/neck/detect2/conv1/conv/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/neck/detect2/conv1/conv/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv1/conv/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv1/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv1/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect2/conv1/conv/_weight_quantizer/DequantizeLinear_output_0 -> (128, 384, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv1/conv/Conv [Conv]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv1/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv1/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv1/conv/Conv [Conv] inputs: [/neck/detect2/conv1/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 384, 138, 242)[FLOAT]], [/neck/detect2/conv1/conv/_weight_quantizer/DequantizeLinear_output_0 -> (128, 384, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv1/conv/Conv for ONNX node: /neck/detect2/conv1/conv/Conv
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv1/conv/Conv_output_0 for ONNX tensor: /neck/detect2/conv1/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv1/conv/Conv [Conv] outputs: [/neck/detect2/conv1/conv/Conv_output_0 -> (1, 128, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv1/bn/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv1/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv1.bn.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv1.bn.bias
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv1.bn.running_mean
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv1.bn.running_var
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv1/bn/BatchNormalization [BatchNormalization] inputs: [/neck/detect2/conv1/conv/Conv_output_0 -> (1, 128, 138, 242)[FLOAT]], [neck.detect2.conv1.bn.weight -> (128)[FLOAT]], [neck.detect2.conv1.bn.bias -> (128)[FLOAT]], [neck.detect2.conv1.bn.running_mean -> (128)[FLOAT]], [neck.detect2.conv1.bn.running_var -> (128)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv1/bn/BatchNormalization for ONNX node: /neck/detect2/conv1/bn/BatchNormalization
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv1/bn/BatchNormalization_output_0 for ONNX tensor: /neck/detect2/conv1/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv1/bn/BatchNormalization [BatchNormalization] outputs: [/neck/detect2/conv1/bn/BatchNormalization_output_0 -> (1, 128, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv1/relu/LeakyRelu [LeakyRelu]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv1/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv1/relu/LeakyRelu [LeakyRelu] inputs: [/neck/detect2/conv1/bn/BatchNormalization_output_0 -> (1, 128, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv1/relu/LeakyRelu for ONNX node: /neck/detect2/conv1/relu/LeakyRelu
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv1/relu/LeakyRelu_output_0 for ONNX tensor: /neck/detect2/conv1/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv1/relu/LeakyRelu [LeakyRelu] outputs: [/neck/detect2/conv1/relu/LeakyRelu_output_0 -> (1, 128, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv2/conv/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv2/conv/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv2/conv/_input_quantizer/Constant [Constant] outputs: [/neck/detect2/conv2/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv2/conv/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv2/conv/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv2/conv/_input_quantizer/Constant_1 [Constant] outputs: [/neck/detect2/conv2/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv2/conv/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv1/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv2/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv2/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv2/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/neck/detect2/conv1/relu/LeakyRelu_output_0 -> (1, 128, 138, 242)[FLOAT]], [/neck/detect2/conv2/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect2/conv2/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv2/conv/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/detect2/conv2/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv2/conv/_input_quantizer/Constant_output_0 for ONNX node: /neck/detect2/conv2/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv2/conv/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv2/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv2/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect2/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv2/conv/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv2/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv2/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv2/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv2/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect2/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 138, 242)[FLOAT]], [/neck/detect2/conv2/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect2/conv2/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv2/conv/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv2/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv2/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect2/conv2/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv2/conv/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv2/conv/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv2/conv/_weight_quantizer/Constant [Constant] outputs: [/neck/detect2/conv2/conv/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv2/conv/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv2/conv/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv2/conv/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/detect2/conv2/conv/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv2.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv2/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv2/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.detect2.conv2.conv.weight -> (256, 128, 3, 3)[FLOAT]], [/neck/detect2/conv2/conv/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], [/neck/detect2/conv2/conv/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: neck.detect2.conv2.conv.weight for ONNX node: neck.detect2.conv2.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv2/conv/_weight_quantizer/Constant_output_0 for ONNX node: /neck/detect2/conv2/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv2/conv/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/detect2/conv2/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv2/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv2/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv2/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv2/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], [/neck/detect2/conv2/conv/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], [/neck/detect2/conv2/conv/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv2/conv/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv2/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv2/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect2/conv2/conv/_weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv2/conv/Conv [Conv]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv2/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv2/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv2/conv/Conv [Conv] inputs: [/neck/detect2/conv2/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 138, 242)[FLOAT]], [/neck/detect2/conv2/conv/_weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv2/conv/Conv for ONNX node: /neck/detect2/conv2/conv/Conv
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv2/conv/Conv_output_0 for ONNX tensor: /neck/detect2/conv2/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv2/conv/Conv [Conv] outputs: [/neck/detect2/conv2/conv/Conv_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv2/bn/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv2/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv2.bn.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv2.bn.bias
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv2.bn.running_mean
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv2.bn.running_var
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv2/bn/BatchNormalization [BatchNormalization] inputs: [/neck/detect2/conv2/conv/Conv_output_0 -> (1, 256, 138, 242)[FLOAT]], [neck.detect2.conv2.bn.weight -> (256)[FLOAT]], [neck.detect2.conv2.bn.bias -> (256)[FLOAT]], [neck.detect2.conv2.bn.running_mean -> (256)[FLOAT]], [neck.detect2.conv2.bn.running_var -> (256)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv2/bn/BatchNormalization for ONNX node: /neck/detect2/conv2/bn/BatchNormalization
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv2/bn/BatchNormalization_output_0 for ONNX tensor: /neck/detect2/conv2/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv2/bn/BatchNormalization [BatchNormalization] outputs: [/neck/detect2/conv2/bn/BatchNormalization_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv2/relu/LeakyRelu [LeakyRelu]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv2/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv2/relu/LeakyRelu [LeakyRelu] inputs: [/neck/detect2/conv2/bn/BatchNormalization_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv2/relu/LeakyRelu for ONNX node: /neck/detect2/conv2/relu/LeakyRelu
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv2/relu/LeakyRelu_output_0 for ONNX tensor: /neck/detect2/conv2/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv2/relu/LeakyRelu [LeakyRelu] outputs: [/neck/detect2/conv2/relu/LeakyRelu_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv3/conv/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv3/conv/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv3/conv/_input_quantizer/Constant [Constant] outputs: [/neck/detect2/conv3/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv3/conv/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv3/conv/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv3/conv/_input_quantizer/Constant_1 [Constant] outputs: [/neck/detect2/conv3/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv3/conv/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv2/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv3/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv3/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv3/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/neck/detect2/conv2/relu/LeakyRelu_output_0 -> (1, 256, 138, 242)[FLOAT]], [/neck/detect2/conv3/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect2/conv3/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv3/conv/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/detect2/conv3/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv3/conv/_input_quantizer/Constant_output_0 for ONNX node: /neck/detect2/conv3/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv3/conv/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv3/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv3/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect2/conv3/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv3/conv/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv3/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv3/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv3/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv3/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect2/conv3/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], [/neck/detect2/conv3/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect2/conv3/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv3/conv/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv3/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv3/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect2/conv3/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv3/conv/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv3/conv/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv3/conv/_weight_quantizer/Constant [Constant] outputs: [/neck/detect2/conv3/conv/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv3/conv/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv3/conv/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv3/conv/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/detect2/conv3/conv/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv3.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv3/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv3/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.detect2.conv3.conv.weight -> (128, 256, 1, 1)[FLOAT]], [/neck/detect2/conv3/conv/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/neck/detect2/conv3/conv/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: neck.detect2.conv3.conv.weight for ONNX node: neck.detect2.conv3.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv3/conv/_weight_quantizer/Constant_output_0 for ONNX node: /neck/detect2/conv3/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv3/conv/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/detect2/conv3/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear_output_0 -> (128, 256, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv3/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv3/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv3/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv3/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear_output_0 -> (128, 256, 1, 1)[FLOAT]], [/neck/detect2/conv3/conv/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/neck/detect2/conv3/conv/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv3/conv/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv3/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv3/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect2/conv3/conv/_weight_quantizer/DequantizeLinear_output_0 -> (128, 256, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv3/conv/Conv [Conv]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv3/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv3/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv3/conv/Conv [Conv] inputs: [/neck/detect2/conv3/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], [/neck/detect2/conv3/conv/_weight_quantizer/DequantizeLinear_output_0 -> (128, 256, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv3/conv/Conv for ONNX node: /neck/detect2/conv3/conv/Conv
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv3/conv/Conv_output_0 for ONNX tensor: /neck/detect2/conv3/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv3/conv/Conv [Conv] outputs: [/neck/detect2/conv3/conv/Conv_output_0 -> (1, 128, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv3/bn/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv3/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv3.bn.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv3.bn.bias
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv3.bn.running_mean
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv3.bn.running_var
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv3/bn/BatchNormalization [BatchNormalization] inputs: [/neck/detect2/conv3/conv/Conv_output_0 -> (1, 128, 138, 242)[FLOAT]], [neck.detect2.conv3.bn.weight -> (128)[FLOAT]], [neck.detect2.conv3.bn.bias -> (128)[FLOAT]], [neck.detect2.conv3.bn.running_mean -> (128)[FLOAT]], [neck.detect2.conv3.bn.running_var -> (128)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv3/bn/BatchNormalization for ONNX node: /neck/detect2/conv3/bn/BatchNormalization
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv3/bn/BatchNormalization_output_0 for ONNX tensor: /neck/detect2/conv3/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv3/bn/BatchNormalization [BatchNormalization] outputs: [/neck/detect2/conv3/bn/BatchNormalization_output_0 -> (1, 128, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv3/relu/LeakyRelu [LeakyRelu]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv3/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv3/relu/LeakyRelu [LeakyRelu] inputs: [/neck/detect2/conv3/bn/BatchNormalization_output_0 -> (1, 128, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv3/relu/LeakyRelu for ONNX node: /neck/detect2/conv3/relu/LeakyRelu
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv3/relu/LeakyRelu_output_0 for ONNX tensor: /neck/detect2/conv3/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv3/relu/LeakyRelu [LeakyRelu] outputs: [/neck/detect2/conv3/relu/LeakyRelu_output_0 -> (1, 128, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv4/conv/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv4/conv/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv4/conv/_input_quantizer/Constant [Constant] outputs: [/neck/detect2/conv4/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv4/conv/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv4/conv/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv4/conv/_input_quantizer/Constant_1 [Constant] outputs: [/neck/detect2/conv4/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv4/conv/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv3/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv4/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv4/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv4/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/neck/detect2/conv3/relu/LeakyRelu_output_0 -> (1, 128, 138, 242)[FLOAT]], [/neck/detect2/conv4/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect2/conv4/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv4/conv/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/detect2/conv4/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv4/conv/_input_quantizer/Constant_output_0 for ONNX node: /neck/detect2/conv4/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv4/conv/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv4/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv4/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect2/conv4/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv4/conv/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv4/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv4/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv4/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv4/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect2/conv4/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 138, 242)[FLOAT]], [/neck/detect2/conv4/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect2/conv4/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv4/conv/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv4/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv4/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect2/conv4/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv4/conv/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv4/conv/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv4/conv/_weight_quantizer/Constant [Constant] outputs: [/neck/detect2/conv4/conv/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv4/conv/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv4/conv/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv4/conv/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/detect2/conv4/conv/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv4.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv4/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv4/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.detect2.conv4.conv.weight -> (256, 128, 3, 3)[FLOAT]], [/neck/detect2/conv4/conv/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], [/neck/detect2/conv4/conv/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: neck.detect2.conv4.conv.weight for ONNX node: neck.detect2.conv4.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv4/conv/_weight_quantizer/Constant_output_0 for ONNX node: /neck/detect2/conv4/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv4/conv/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/detect2/conv4/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv4/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv4/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv4/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv4/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], [/neck/detect2/conv4/conv/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], [/neck/detect2/conv4/conv/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv4/conv/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv4/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv4/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect2/conv4/conv/_weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv4/conv/Conv [Conv]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv4/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv4/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv4/conv/Conv [Conv] inputs: [/neck/detect2/conv4/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 138, 242)[FLOAT]], [/neck/detect2/conv4/conv/_weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv4/conv/Conv for ONNX node: /neck/detect2/conv4/conv/Conv
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv4/conv/Conv_output_0 for ONNX tensor: /neck/detect2/conv4/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv4/conv/Conv [Conv] outputs: [/neck/detect2/conv4/conv/Conv_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv4/bn/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv4/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv4.bn.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv4.bn.bias
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv4.bn.running_mean
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv4.bn.running_var
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv4/bn/BatchNormalization [BatchNormalization] inputs: [/neck/detect2/conv4/conv/Conv_output_0 -> (1, 256, 138, 242)[FLOAT]], [neck.detect2.conv4.bn.weight -> (256)[FLOAT]], [neck.detect2.conv4.bn.bias -> (256)[FLOAT]], [neck.detect2.conv4.bn.running_mean -> (256)[FLOAT]], [neck.detect2.conv4.bn.running_var -> (256)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv4/bn/BatchNormalization for ONNX node: /neck/detect2/conv4/bn/BatchNormalization
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv4/bn/BatchNormalization_output_0 for ONNX tensor: /neck/detect2/conv4/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv4/bn/BatchNormalization [BatchNormalization] outputs: [/neck/detect2/conv4/bn/BatchNormalization_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv4/relu/LeakyRelu [LeakyRelu]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv4/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv4/relu/LeakyRelu [LeakyRelu] inputs: [/neck/detect2/conv4/bn/BatchNormalization_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv4/relu/LeakyRelu for ONNX node: /neck/detect2/conv4/relu/LeakyRelu
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv4/relu/LeakyRelu_output_0 for ONNX tensor: /neck/detect2/conv4/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv4/relu/LeakyRelu [LeakyRelu] outputs: [/neck/detect2/conv4/relu/LeakyRelu_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv5/conv/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv5/conv/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv5/conv/_input_quantizer/Constant [Constant] outputs: [/neck/detect2/conv5/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv5/conv/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv5/conv/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv5/conv/_input_quantizer/Constant_1 [Constant] outputs: [/neck/detect2/conv5/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv5/conv/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv4/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv5/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv5/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv5/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/neck/detect2/conv4/relu/LeakyRelu_output_0 -> (1, 256, 138, 242)[FLOAT]], [/neck/detect2/conv5/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect2/conv5/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv5/conv/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/detect2/conv5/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv5/conv/_input_quantizer/Constant_output_0 for ONNX node: /neck/detect2/conv5/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv5/conv/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv5/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv5/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect2/conv5/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv5/conv/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv5/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv5/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv5/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv5/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect2/conv5/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], [/neck/detect2/conv5/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect2/conv5/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv5/conv/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv5/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv5/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect2/conv5/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv5/conv/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv5/conv/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv5/conv/_weight_quantizer/Constant [Constant] outputs: [/neck/detect2/conv5/conv/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv5/conv/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv5/conv/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv5/conv/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/detect2/conv5/conv/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv5.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv5/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv5/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.detect2.conv5.conv.weight -> (128, 256, 1, 1)[FLOAT]], [/neck/detect2/conv5/conv/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/neck/detect2/conv5/conv/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: neck.detect2.conv5.conv.weight for ONNX node: neck.detect2.conv5.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv5/conv/_weight_quantizer/Constant_output_0 for ONNX node: /neck/detect2/conv5/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv5/conv/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/detect2/conv5/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear_output_0 -> (128, 256, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv5/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv5/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv5/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv5/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear_output_0 -> (128, 256, 1, 1)[FLOAT]], [/neck/detect2/conv5/conv/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/neck/detect2/conv5/conv/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv5/conv/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv5/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv5/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect2/conv5/conv/_weight_quantizer/DequantizeLinear_output_0 -> (128, 256, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv5/conv/Conv [Conv]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv5/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv5/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv5/conv/Conv [Conv] inputs: [/neck/detect2/conv5/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], [/neck/detect2/conv5/conv/_weight_quantizer/DequantizeLinear_output_0 -> (128, 256, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv5/conv/Conv for ONNX node: /neck/detect2/conv5/conv/Conv
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv5/conv/Conv_output_0 for ONNX tensor: /neck/detect2/conv5/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv5/conv/Conv [Conv] outputs: [/neck/detect2/conv5/conv/Conv_output_0 -> (1, 128, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv5/bn/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv5/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv5.bn.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv5.bn.bias
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv5.bn.running_mean
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv5.bn.running_var
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv5/bn/BatchNormalization [BatchNormalization] inputs: [/neck/detect2/conv5/conv/Conv_output_0 -> (1, 128, 138, 242)[FLOAT]], [neck.detect2.conv5.bn.weight -> (128)[FLOAT]], [neck.detect2.conv5.bn.bias -> (128)[FLOAT]], [neck.detect2.conv5.bn.running_mean -> (128)[FLOAT]], [neck.detect2.conv5.bn.running_var -> (128)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv5/bn/BatchNormalization for ONNX node: /neck/detect2/conv5/bn/BatchNormalization
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv5/bn/BatchNormalization_output_0 for ONNX tensor: /neck/detect2/conv5/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv5/bn/BatchNormalization [BatchNormalization] outputs: [/neck/detect2/conv5/bn/BatchNormalization_output_0 -> (1, 128, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv5/relu/LeakyRelu [LeakyRelu]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv5/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv5/relu/LeakyRelu [LeakyRelu] inputs: [/neck/detect2/conv5/bn/BatchNormalization_output_0 -> (1, 128, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv5/relu/LeakyRelu for ONNX node: /neck/detect2/conv5/relu/LeakyRelu
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv5/relu/LeakyRelu_output_0 for ONNX tensor: /neck/detect2/conv5/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv5/relu/LeakyRelu [LeakyRelu] outputs: [/neck/detect2/conv5/relu/LeakyRelu_output_0 -> (1, 128, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv6/conv/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv6/conv/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv6/conv/_input_quantizer/Constant [Constant] outputs: [/neck/detect2/conv6/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv6/conv/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv6/conv/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv6/conv/_input_quantizer/Constant_1 [Constant] outputs: [/neck/detect2/conv6/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv6/conv/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv5/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv6/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv6/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv6/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/neck/detect2/conv5/relu/LeakyRelu_output_0 -> (1, 128, 138, 242)[FLOAT]], [/neck/detect2/conv6/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect2/conv6/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv6/conv/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/detect2/conv6/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv6/conv/_input_quantizer/Constant_output_0 for ONNX node: /neck/detect2/conv6/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv6/conv/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv6/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv6/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect2/conv6/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv6/conv/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv6/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv6/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv6/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv6/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect2/conv6/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 138, 242)[FLOAT]], [/neck/detect2/conv6/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect2/conv6/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv6/conv/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv6/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv6/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect2/conv6/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv6/conv/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv6/conv/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv6/conv/_weight_quantizer/Constant [Constant] outputs: [/neck/detect2/conv6/conv/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv6/conv/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv6/conv/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv6/conv/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/detect2/conv6/conv/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv6.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv6/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv6/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.detect2.conv6.conv.weight -> (256, 128, 3, 3)[FLOAT]], [/neck/detect2/conv6/conv/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], [/neck/detect2/conv6/conv/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: neck.detect2.conv6.conv.weight for ONNX node: neck.detect2.conv6.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv6/conv/_weight_quantizer/Constant_output_0 for ONNX node: /neck/detect2/conv6/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv6/conv/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/detect2/conv6/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv6/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv6/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv6/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv6/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], [/neck/detect2/conv6/conv/_weight_quantizer/Constant_output_0 -> (256)[FLOAT]], [/neck/detect2/conv6/conv/_weight_quantizer/Constant_1_output_0 -> (256)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv6/conv/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv6/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv6/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect2/conv6/conv/_weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv6/conv/Conv [Conv]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv6/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv6/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv6/conv/Conv [Conv] inputs: [/neck/detect2/conv6/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 138, 242)[FLOAT]], [/neck/detect2/conv6/conv/_weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv6/conv/Conv for ONNX node: /neck/detect2/conv6/conv/Conv
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv6/conv/Conv_output_0 for ONNX tensor: /neck/detect2/conv6/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv6/conv/Conv [Conv] outputs: [/neck/detect2/conv6/conv/Conv_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv6/bn/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv6/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv6.bn.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv6.bn.bias
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv6.bn.running_mean
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv6.bn.running_var
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv6/bn/BatchNormalization [BatchNormalization] inputs: [/neck/detect2/conv6/conv/Conv_output_0 -> (1, 256, 138, 242)[FLOAT]], [neck.detect2.conv6.bn.weight -> (256)[FLOAT]], [neck.detect2.conv6.bn.bias -> (256)[FLOAT]], [neck.detect2.conv6.bn.running_mean -> (256)[FLOAT]], [neck.detect2.conv6.bn.running_var -> (256)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv6/bn/BatchNormalization for ONNX node: /neck/detect2/conv6/bn/BatchNormalization
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv6/bn/BatchNormalization_output_0 for ONNX tensor: /neck/detect2/conv6/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv6/bn/BatchNormalization [BatchNormalization] outputs: [/neck/detect2/conv6/bn/BatchNormalization_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv6/relu/LeakyRelu [LeakyRelu]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv6/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv6/relu/LeakyRelu [LeakyRelu] inputs: [/neck/detect2/conv6/bn/BatchNormalization_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv6/relu/LeakyRelu for ONNX node: /neck/detect2/conv6/relu/LeakyRelu
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv6/relu/LeakyRelu_output_0 for ONNX tensor: /neck/detect2/conv6/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv6/relu/LeakyRelu [LeakyRelu] outputs: [/neck/detect2/conv6/relu/LeakyRelu_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv7/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv7/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv7/_input_quantizer/Constant [Constant] outputs: [/neck/detect2/conv7/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv7/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv7/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv7/_input_quantizer/Constant_1 [Constant] outputs: [/neck/detect2/conv7/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv7/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv6/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv7/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv7/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv7/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/neck/detect2/conv6/relu/LeakyRelu_output_0 -> (1, 256, 138, 242)[FLOAT]], [/neck/detect2/conv7/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect2/conv7/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv7/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/detect2/conv7/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv7/_input_quantizer/Constant_output_0 for ONNX node: /neck/detect2/conv7/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv7/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv7/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv7/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect2/conv7/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv7/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv7/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv7/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv7/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv7/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect2/conv7/_input_quantizer/QuantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], [/neck/detect2/conv7/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect2/conv7/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv7/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv7/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv7/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect2/conv7/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv7/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv7/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv7/_weight_quantizer/Constant [Constant] outputs: [/neck/detect2/conv7/_weight_quantizer/Constant_output_0 -> (63)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv7/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv7/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv7/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/detect2/conv7/_weight_quantizer/Constant_1_output_0 -> (63)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv7/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv7.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv7/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv7/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv7/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.detect2.conv7.weight -> (63, 256, 1, 1)[FLOAT]], [/neck/detect2/conv7/_weight_quantizer/Constant_output_0 -> (63)[FLOAT]], [/neck/detect2/conv7/_weight_quantizer/Constant_1_output_0 -> (63)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: neck.detect2.conv7.weight for ONNX node: neck.detect2.conv7.weight
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv7/_weight_quantizer/Constant_output_0 for ONNX node: /neck/detect2/conv7/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv7/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/detect2/conv7/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv7/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv7/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv7/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect2/conv7/_weight_quantizer/QuantizeLinear_output_0 -> (63, 256, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv7/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv7/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv7/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv7/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv7/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect2/conv7/_weight_quantizer/QuantizeLinear_output_0 -> (63, 256, 1, 1)[FLOAT]], [/neck/detect2/conv7/_weight_quantizer/Constant_output_0 -> (63)[FLOAT]], [/neck/detect2/conv7/_weight_quantizer/Constant_1_output_0 -> (63)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv7/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect2/conv7/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv7/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect2/conv7/_weight_quantizer/DequantizeLinear_output_0 -> (63, 256, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect2/conv7/Conv [Conv]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv7/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv7/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect2.conv7.bias
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv7/Conv [Conv] inputs: [/neck/detect2/conv7/_input_quantizer/DequantizeLinear_output_0 -> (1, 256, 138, 242)[FLOAT]], [/neck/detect2/conv7/_weight_quantizer/DequantizeLinear_output_0 -> (63, 256, 1, 1)[FLOAT]], [neck.detect2.conv7.bias -> (63)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect2/conv7/Conv for ONNX node: /neck/detect2/conv7/Conv
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect2/conv7/Conv_output_0 for ONNX tensor: /neck/detect2/conv7/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect2/conv7/Conv [Conv] outputs: [/neck/detect2/conv7/Conv_output_0 -> (1, 63, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/conv2/conv/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/conv2/conv/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/conv2/conv/_input_quantizer/Constant [Constant] outputs: [/neck/conv2/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/conv2/conv/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/conv2/conv/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/conv2/conv/_input_quantizer/Constant_1 [Constant] outputs: [/neck/conv2/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/conv2/conv/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv5/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/conv2/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/conv2/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/conv2/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/neck/detect2/conv5/relu/LeakyRelu_output_0 -> (1, 128, 138, 242)[FLOAT]], [/neck/conv2/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/conv2/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/conv2/conv/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/conv2/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/conv2/conv/_input_quantizer/Constant_output_0 for ONNX node: /neck/conv2/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/conv2/conv/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/conv2/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/conv2/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/conv2/conv/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/conv2/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/conv2/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/conv2/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/conv2/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 138, 242)[FLOAT]], [/neck/conv2/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/conv2/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/conv2/conv/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/conv2/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/conv2/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/conv2/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/conv2/conv/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/conv2/conv/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/conv2/conv/_weight_quantizer/Constant [Constant] outputs: [/neck/conv2/conv/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/conv2/conv/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/conv2/conv/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/conv2/conv/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/conv2/conv/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/conv2/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.conv2.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/conv2/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/conv2/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/conv2/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.conv2.conv.weight -> (64, 128, 1, 1)[FLOAT]], [/neck/conv2/conv/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], [/neck/conv2/conv/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: neck.conv2.conv.weight for ONNX node: neck.conv2.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/conv2/conv/_weight_quantizer/Constant_output_0 for ONNX node: /neck/conv2/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/conv2/conv/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/conv2/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/conv2/conv/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/conv2/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/conv2/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/conv2/conv/_weight_quantizer/QuantizeLinear_output_0 -> (64, 128, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/conv2/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/conv2/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/conv2/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/conv2/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/conv2/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/conv2/conv/_weight_quantizer/QuantizeLinear_output_0 -> (64, 128, 1, 1)[FLOAT]], [/neck/conv2/conv/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], [/neck/conv2/conv/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/conv2/conv/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/conv2/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/conv2/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/conv2/conv/_weight_quantizer/DequantizeLinear_output_0 -> (64, 128, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/conv2/conv/Conv [Conv]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/conv2/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/conv2/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/conv2/conv/Conv [Conv] inputs: [/neck/conv2/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 138, 242)[FLOAT]], [/neck/conv2/conv/_weight_quantizer/DequantizeLinear_output_0 -> (64, 128, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/conv2/conv/Conv for ONNX node: /neck/conv2/conv/Conv
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/conv2/conv/Conv_output_0 for ONNX tensor: /neck/conv2/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/conv2/conv/Conv [Conv] outputs: [/neck/conv2/conv/Conv_output_0 -> (1, 64, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/conv2/bn/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/conv2/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.conv2.bn.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.conv2.bn.bias
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.conv2.bn.running_mean
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.conv2.bn.running_var
[10/31/2023-17:53:35] [V] [TRT] /neck/conv2/bn/BatchNormalization [BatchNormalization] inputs: [/neck/conv2/conv/Conv_output_0 -> (1, 64, 138, 242)[FLOAT]], [neck.conv2.bn.weight -> (64)[FLOAT]], [neck.conv2.bn.bias -> (64)[FLOAT]], [neck.conv2.bn.running_mean -> (64)[FLOAT]], [neck.conv2.bn.running_var -> (64)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/conv2/bn/BatchNormalization for ONNX node: /neck/conv2/bn/BatchNormalization
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/conv2/bn/BatchNormalization_output_0 for ONNX tensor: /neck/conv2/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/conv2/bn/BatchNormalization [BatchNormalization] outputs: [/neck/conv2/bn/BatchNormalization_output_0 -> (1, 64, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/conv2/relu/LeakyRelu [LeakyRelu]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/conv2/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/conv2/relu/LeakyRelu [LeakyRelu] inputs: [/neck/conv2/bn/BatchNormalization_output_0 -> (1, 64, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/conv2/relu/LeakyRelu for ONNX node: /neck/conv2/relu/LeakyRelu
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/conv2/relu/LeakyRelu_output_0 for ONNX tensor: /neck/conv2/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/conv2/relu/LeakyRelu [LeakyRelu] outputs: [/neck/conv2/relu/LeakyRelu_output_0 -> (1, 64, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/Constant_1 [Constant] outputs: [/neck/Constant_1_output_0 -> (4)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/Resize_1 [Resize]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/conv2/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/Resize_1 [Resize] inputs: [/neck/conv2/relu/LeakyRelu_output_0 -> (1, 64, 138, 242)[FLOAT]], [optional input, not set], [/neck/Constant_1_output_0 -> (4)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/Resize_1 for ONNX node: /neck/Resize_1
[10/31/2023-17:53:35] [V] [TRT] Running resize layer with: 
Transformation mode: asymmetric
Resize mode: nearest

[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/Resize_1_output_0 for ONNX tensor: /neck/Resize_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/Resize_1 [Resize] outputs: [/neck/Resize_1_output_0 -> (1, 64, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/Concat_1 [Concat]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/Resize_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /backbone/layer2/layer2.1/relu_1/Relu_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/Concat_1 [Concat] inputs: [/neck/Resize_1_output_0 -> (1, 64, 276, 484)[FLOAT]], [/backbone/layer2/layer2.1/relu_1/Relu_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/Concat_1 for ONNX node: /neck/Concat_1
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/Concat_1_output_0 for ONNX tensor: /neck/Concat_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/Concat_1 [Concat] outputs: [/neck/Concat_1_output_0 -> (1, 192, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv1/conv/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv1/conv/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv1/conv/_input_quantizer/Constant [Constant] outputs: [/neck/detect3/conv1/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv1/conv/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv1/conv/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv1/conv/_input_quantizer/Constant_1 [Constant] outputs: [/neck/detect3/conv1/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/Concat_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv1/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv1/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/neck/Concat_1_output_0 -> (1, 192, 276, 484)[FLOAT]], [/neck/detect3/conv1/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect3/conv1/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv1/conv/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/detect3/conv1/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv1/conv/_input_quantizer/Constant_output_0 for ONNX node: /neck/detect3/conv1/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 192, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv1/conv/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv1/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv1/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv1/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 192, 276, 484)[FLOAT]], [/neck/detect3/conv1/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect3/conv1/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv1/conv/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv1/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv1/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect3/conv1/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 192, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv1/conv/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv1/conv/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv1/conv/_weight_quantizer/Constant [Constant] outputs: [/neck/detect3/conv1/conv/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv1/conv/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv1/conv/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv1/conv/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/detect3/conv1/conv/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv1.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv1/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv1/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.detect3.conv1.conv.weight -> (64, 192, 1, 1)[FLOAT]], [/neck/detect3/conv1/conv/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], [/neck/detect3/conv1/conv/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: neck.detect3.conv1.conv.weight for ONNX node: neck.detect3.conv1.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv1/conv/_weight_quantizer/Constant_output_0 for ONNX node: /neck/detect3/conv1/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv1/conv/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/detect3/conv1/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear_output_0 -> (64, 192, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv1/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv1/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv1/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv1/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear_output_0 -> (64, 192, 1, 1)[FLOAT]], [/neck/detect3/conv1/conv/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], [/neck/detect3/conv1/conv/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv1/conv/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv1/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv1/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect3/conv1/conv/_weight_quantizer/DequantizeLinear_output_0 -> (64, 192, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv1/conv/Conv [Conv]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv1/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv1/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv1/conv/Conv [Conv] inputs: [/neck/detect3/conv1/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 192, 276, 484)[FLOAT]], [/neck/detect3/conv1/conv/_weight_quantizer/DequantizeLinear_output_0 -> (64, 192, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv1/conv/Conv for ONNX node: /neck/detect3/conv1/conv/Conv
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv1/conv/Conv_output_0 for ONNX tensor: /neck/detect3/conv1/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv1/conv/Conv [Conv] outputs: [/neck/detect3/conv1/conv/Conv_output_0 -> (1, 64, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv1/bn/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv1/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv1.bn.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv1.bn.bias
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv1.bn.running_mean
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv1.bn.running_var
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv1/bn/BatchNormalization [BatchNormalization] inputs: [/neck/detect3/conv1/conv/Conv_output_0 -> (1, 64, 276, 484)[FLOAT]], [neck.detect3.conv1.bn.weight -> (64)[FLOAT]], [neck.detect3.conv1.bn.bias -> (64)[FLOAT]], [neck.detect3.conv1.bn.running_mean -> (64)[FLOAT]], [neck.detect3.conv1.bn.running_var -> (64)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv1/bn/BatchNormalization for ONNX node: /neck/detect3/conv1/bn/BatchNormalization
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv1/bn/BatchNormalization_output_0 for ONNX tensor: /neck/detect3/conv1/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv1/bn/BatchNormalization [BatchNormalization] outputs: [/neck/detect3/conv1/bn/BatchNormalization_output_0 -> (1, 64, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv1/relu/LeakyRelu [LeakyRelu]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv1/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv1/relu/LeakyRelu [LeakyRelu] inputs: [/neck/detect3/conv1/bn/BatchNormalization_output_0 -> (1, 64, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv1/relu/LeakyRelu for ONNX node: /neck/detect3/conv1/relu/LeakyRelu
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv1/relu/LeakyRelu_output_0 for ONNX tensor: /neck/detect3/conv1/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv1/relu/LeakyRelu [LeakyRelu] outputs: [/neck/detect3/conv1/relu/LeakyRelu_output_0 -> (1, 64, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv2/conv/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv2/conv/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv2/conv/_input_quantizer/Constant [Constant] outputs: [/neck/detect3/conv2/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv2/conv/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv2/conv/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv2/conv/_input_quantizer/Constant_1 [Constant] outputs: [/neck/detect3/conv2/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv2/conv/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv1/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv2/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv2/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv2/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/neck/detect3/conv1/relu/LeakyRelu_output_0 -> (1, 64, 276, 484)[FLOAT]], [/neck/detect3/conv2/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect3/conv2/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv2/conv/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/detect3/conv2/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv2/conv/_input_quantizer/Constant_output_0 for ONNX node: /neck/detect3/conv2/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv2/conv/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv2/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv2/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect3/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 64, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv2/conv/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv2/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv2/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv2/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv2/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect3/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 64, 276, 484)[FLOAT]], [/neck/detect3/conv2/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect3/conv2/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv2/conv/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv2/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv2/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect3/conv2/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 64, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv2/conv/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv2/conv/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv2/conv/_weight_quantizer/Constant [Constant] outputs: [/neck/detect3/conv2/conv/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv2/conv/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv2/conv/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv2/conv/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/detect3/conv2/conv/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv2.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv2/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv2/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.detect3.conv2.conv.weight -> (128, 64, 3, 3)[FLOAT]], [/neck/detect3/conv2/conv/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/neck/detect3/conv2/conv/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: neck.detect3.conv2.conv.weight for ONNX node: neck.detect3.conv2.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv2/conv/_weight_quantizer/Constant_output_0 for ONNX node: /neck/detect3/conv2/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv2/conv/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/detect3/conv2/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv2/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv2/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv2/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv2/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], [/neck/detect3/conv2/conv/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/neck/detect3/conv2/conv/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv2/conv/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv2/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv2/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect3/conv2/conv/_weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv2/conv/Conv [Conv]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv2/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv2/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv2/conv/Conv [Conv] inputs: [/neck/detect3/conv2/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 64, 276, 484)[FLOAT]], [/neck/detect3/conv2/conv/_weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv2/conv/Conv for ONNX node: /neck/detect3/conv2/conv/Conv
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv2/conv/Conv_output_0 for ONNX tensor: /neck/detect3/conv2/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv2/conv/Conv [Conv] outputs: [/neck/detect3/conv2/conv/Conv_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv2/bn/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv2/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv2.bn.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv2.bn.bias
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv2.bn.running_mean
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv2.bn.running_var
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv2/bn/BatchNormalization [BatchNormalization] inputs: [/neck/detect3/conv2/conv/Conv_output_0 -> (1, 128, 276, 484)[FLOAT]], [neck.detect3.conv2.bn.weight -> (128)[FLOAT]], [neck.detect3.conv2.bn.bias -> (128)[FLOAT]], [neck.detect3.conv2.bn.running_mean -> (128)[FLOAT]], [neck.detect3.conv2.bn.running_var -> (128)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv2/bn/BatchNormalization for ONNX node: /neck/detect3/conv2/bn/BatchNormalization
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv2/bn/BatchNormalization_output_0 for ONNX tensor: /neck/detect3/conv2/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv2/bn/BatchNormalization [BatchNormalization] outputs: [/neck/detect3/conv2/bn/BatchNormalization_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv2/relu/LeakyRelu [LeakyRelu]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv2/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv2/relu/LeakyRelu [LeakyRelu] inputs: [/neck/detect3/conv2/bn/BatchNormalization_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv2/relu/LeakyRelu for ONNX node: /neck/detect3/conv2/relu/LeakyRelu
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv2/relu/LeakyRelu_output_0 for ONNX tensor: /neck/detect3/conv2/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv2/relu/LeakyRelu [LeakyRelu] outputs: [/neck/detect3/conv2/relu/LeakyRelu_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv3/conv/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv3/conv/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv3/conv/_input_quantizer/Constant [Constant] outputs: [/neck/detect3/conv3/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv3/conv/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv3/conv/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv3/conv/_input_quantizer/Constant_1 [Constant] outputs: [/neck/detect3/conv3/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv3/conv/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv2/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv3/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv3/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv3/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/neck/detect3/conv2/relu/LeakyRelu_output_0 -> (1, 128, 276, 484)[FLOAT]], [/neck/detect3/conv3/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect3/conv3/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv3/conv/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/detect3/conv3/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv3/conv/_input_quantizer/Constant_output_0 for ONNX node: /neck/detect3/conv3/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv3/conv/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv3/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv3/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect3/conv3/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv3/conv/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv3/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv3/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv3/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv3/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect3/conv3/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], [/neck/detect3/conv3/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect3/conv3/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv3/conv/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv3/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv3/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect3/conv3/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv3/conv/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv3/conv/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv3/conv/_weight_quantizer/Constant [Constant] outputs: [/neck/detect3/conv3/conv/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv3/conv/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv3/conv/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv3/conv/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/detect3/conv3/conv/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv3.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv3/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv3/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.detect3.conv3.conv.weight -> (64, 128, 1, 1)[FLOAT]], [/neck/detect3/conv3/conv/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], [/neck/detect3/conv3/conv/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: neck.detect3.conv3.conv.weight for ONNX node: neck.detect3.conv3.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv3/conv/_weight_quantizer/Constant_output_0 for ONNX node: /neck/detect3/conv3/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv3/conv/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/detect3/conv3/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear_output_0 -> (64, 128, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv3/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv3/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv3/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv3/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear_output_0 -> (64, 128, 1, 1)[FLOAT]], [/neck/detect3/conv3/conv/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], [/neck/detect3/conv3/conv/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv3/conv/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv3/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv3/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect3/conv3/conv/_weight_quantizer/DequantizeLinear_output_0 -> (64, 128, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv3/conv/Conv [Conv]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv3/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv3/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv3/conv/Conv [Conv] inputs: [/neck/detect3/conv3/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], [/neck/detect3/conv3/conv/_weight_quantizer/DequantizeLinear_output_0 -> (64, 128, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv3/conv/Conv for ONNX node: /neck/detect3/conv3/conv/Conv
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv3/conv/Conv_output_0 for ONNX tensor: /neck/detect3/conv3/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv3/conv/Conv [Conv] outputs: [/neck/detect3/conv3/conv/Conv_output_0 -> (1, 64, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv3/bn/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv3/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv3.bn.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv3.bn.bias
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv3.bn.running_mean
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv3.bn.running_var
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv3/bn/BatchNormalization [BatchNormalization] inputs: [/neck/detect3/conv3/conv/Conv_output_0 -> (1, 64, 276, 484)[FLOAT]], [neck.detect3.conv3.bn.weight -> (64)[FLOAT]], [neck.detect3.conv3.bn.bias -> (64)[FLOAT]], [neck.detect3.conv3.bn.running_mean -> (64)[FLOAT]], [neck.detect3.conv3.bn.running_var -> (64)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv3/bn/BatchNormalization for ONNX node: /neck/detect3/conv3/bn/BatchNormalization
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv3/bn/BatchNormalization_output_0 for ONNX tensor: /neck/detect3/conv3/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv3/bn/BatchNormalization [BatchNormalization] outputs: [/neck/detect3/conv3/bn/BatchNormalization_output_0 -> (1, 64, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv3/relu/LeakyRelu [LeakyRelu]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv3/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv3/relu/LeakyRelu [LeakyRelu] inputs: [/neck/detect3/conv3/bn/BatchNormalization_output_0 -> (1, 64, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv3/relu/LeakyRelu for ONNX node: /neck/detect3/conv3/relu/LeakyRelu
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv3/relu/LeakyRelu_output_0 for ONNX tensor: /neck/detect3/conv3/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv3/relu/LeakyRelu [LeakyRelu] outputs: [/neck/detect3/conv3/relu/LeakyRelu_output_0 -> (1, 64, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv4/conv/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv4/conv/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv4/conv/_input_quantizer/Constant [Constant] outputs: [/neck/detect3/conv4/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv4/conv/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv4/conv/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv4/conv/_input_quantizer/Constant_1 [Constant] outputs: [/neck/detect3/conv4/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv4/conv/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv3/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv4/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv4/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv4/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/neck/detect3/conv3/relu/LeakyRelu_output_0 -> (1, 64, 276, 484)[FLOAT]], [/neck/detect3/conv4/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect3/conv4/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv4/conv/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/detect3/conv4/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv4/conv/_input_quantizer/Constant_output_0 for ONNX node: /neck/detect3/conv4/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv4/conv/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv4/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv4/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect3/conv4/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 64, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv4/conv/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv4/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv4/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv4/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv4/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect3/conv4/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 64, 276, 484)[FLOAT]], [/neck/detect3/conv4/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect3/conv4/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv4/conv/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv4/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv4/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect3/conv4/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 64, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv4/conv/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv4/conv/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv4/conv/_weight_quantizer/Constant [Constant] outputs: [/neck/detect3/conv4/conv/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv4/conv/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv4/conv/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv4/conv/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/detect3/conv4/conv/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv4.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv4/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv4/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.detect3.conv4.conv.weight -> (128, 64, 3, 3)[FLOAT]], [/neck/detect3/conv4/conv/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/neck/detect3/conv4/conv/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: neck.detect3.conv4.conv.weight for ONNX node: neck.detect3.conv4.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv4/conv/_weight_quantizer/Constant_output_0 for ONNX node: /neck/detect3/conv4/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv4/conv/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/detect3/conv4/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv4/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv4/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv4/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv4/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], [/neck/detect3/conv4/conv/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/neck/detect3/conv4/conv/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv4/conv/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv4/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv4/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect3/conv4/conv/_weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv4/conv/Conv [Conv]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv4/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv4/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv4/conv/Conv [Conv] inputs: [/neck/detect3/conv4/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 64, 276, 484)[FLOAT]], [/neck/detect3/conv4/conv/_weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv4/conv/Conv for ONNX node: /neck/detect3/conv4/conv/Conv
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv4/conv/Conv_output_0 for ONNX tensor: /neck/detect3/conv4/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv4/conv/Conv [Conv] outputs: [/neck/detect3/conv4/conv/Conv_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv4/bn/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv4/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv4.bn.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv4.bn.bias
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv4.bn.running_mean
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv4.bn.running_var
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv4/bn/BatchNormalization [BatchNormalization] inputs: [/neck/detect3/conv4/conv/Conv_output_0 -> (1, 128, 276, 484)[FLOAT]], [neck.detect3.conv4.bn.weight -> (128)[FLOAT]], [neck.detect3.conv4.bn.bias -> (128)[FLOAT]], [neck.detect3.conv4.bn.running_mean -> (128)[FLOAT]], [neck.detect3.conv4.bn.running_var -> (128)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv4/bn/BatchNormalization for ONNX node: /neck/detect3/conv4/bn/BatchNormalization
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv4/bn/BatchNormalization_output_0 for ONNX tensor: /neck/detect3/conv4/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv4/bn/BatchNormalization [BatchNormalization] outputs: [/neck/detect3/conv4/bn/BatchNormalization_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv4/relu/LeakyRelu [LeakyRelu]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv4/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv4/relu/LeakyRelu [LeakyRelu] inputs: [/neck/detect3/conv4/bn/BatchNormalization_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv4/relu/LeakyRelu for ONNX node: /neck/detect3/conv4/relu/LeakyRelu
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv4/relu/LeakyRelu_output_0 for ONNX tensor: /neck/detect3/conv4/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv4/relu/LeakyRelu [LeakyRelu] outputs: [/neck/detect3/conv4/relu/LeakyRelu_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv5/conv/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv5/conv/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv5/conv/_input_quantizer/Constant [Constant] outputs: [/neck/detect3/conv5/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv5/conv/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv5/conv/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv5/conv/_input_quantizer/Constant_1 [Constant] outputs: [/neck/detect3/conv5/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv5/conv/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv4/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv5/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv5/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv5/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/neck/detect3/conv4/relu/LeakyRelu_output_0 -> (1, 128, 276, 484)[FLOAT]], [/neck/detect3/conv5/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect3/conv5/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv5/conv/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/detect3/conv5/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv5/conv/_input_quantizer/Constant_output_0 for ONNX node: /neck/detect3/conv5/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv5/conv/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv5/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv5/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect3/conv5/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv5/conv/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv5/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv5/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv5/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv5/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect3/conv5/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], [/neck/detect3/conv5/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect3/conv5/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv5/conv/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv5/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv5/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect3/conv5/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv5/conv/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv5/conv/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv5/conv/_weight_quantizer/Constant [Constant] outputs: [/neck/detect3/conv5/conv/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv5/conv/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv5/conv/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv5/conv/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/detect3/conv5/conv/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv5.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv5/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv5/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.detect3.conv5.conv.weight -> (64, 128, 1, 1)[FLOAT]], [/neck/detect3/conv5/conv/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], [/neck/detect3/conv5/conv/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: neck.detect3.conv5.conv.weight for ONNX node: neck.detect3.conv5.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv5/conv/_weight_quantizer/Constant_output_0 for ONNX node: /neck/detect3/conv5/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv5/conv/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/detect3/conv5/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear_output_0 -> (64, 128, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv5/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv5/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv5/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv5/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear_output_0 -> (64, 128, 1, 1)[FLOAT]], [/neck/detect3/conv5/conv/_weight_quantizer/Constant_output_0 -> (64)[FLOAT]], [/neck/detect3/conv5/conv/_weight_quantizer/Constant_1_output_0 -> (64)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv5/conv/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv5/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv5/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect3/conv5/conv/_weight_quantizer/DequantizeLinear_output_0 -> (64, 128, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv5/conv/Conv [Conv]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv5/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv5/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv5/conv/Conv [Conv] inputs: [/neck/detect3/conv5/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], [/neck/detect3/conv5/conv/_weight_quantizer/DequantizeLinear_output_0 -> (64, 128, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv5/conv/Conv for ONNX node: /neck/detect3/conv5/conv/Conv
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv5/conv/Conv_output_0 for ONNX tensor: /neck/detect3/conv5/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv5/conv/Conv [Conv] outputs: [/neck/detect3/conv5/conv/Conv_output_0 -> (1, 64, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv5/bn/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv5/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv5.bn.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv5.bn.bias
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv5.bn.running_mean
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv5.bn.running_var
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv5/bn/BatchNormalization [BatchNormalization] inputs: [/neck/detect3/conv5/conv/Conv_output_0 -> (1, 64, 276, 484)[FLOAT]], [neck.detect3.conv5.bn.weight -> (64)[FLOAT]], [neck.detect3.conv5.bn.bias -> (64)[FLOAT]], [neck.detect3.conv5.bn.running_mean -> (64)[FLOAT]], [neck.detect3.conv5.bn.running_var -> (64)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv5/bn/BatchNormalization for ONNX node: /neck/detect3/conv5/bn/BatchNormalization
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv5/bn/BatchNormalization_output_0 for ONNX tensor: /neck/detect3/conv5/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv5/bn/BatchNormalization [BatchNormalization] outputs: [/neck/detect3/conv5/bn/BatchNormalization_output_0 -> (1, 64, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv5/relu/LeakyRelu [LeakyRelu]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv5/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv5/relu/LeakyRelu [LeakyRelu] inputs: [/neck/detect3/conv5/bn/BatchNormalization_output_0 -> (1, 64, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv5/relu/LeakyRelu for ONNX node: /neck/detect3/conv5/relu/LeakyRelu
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv5/relu/LeakyRelu_output_0 for ONNX tensor: /neck/detect3/conv5/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv5/relu/LeakyRelu [LeakyRelu] outputs: [/neck/detect3/conv5/relu/LeakyRelu_output_0 -> (1, 64, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv6/conv/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv6/conv/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv6/conv/_input_quantizer/Constant [Constant] outputs: [/neck/detect3/conv6/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv6/conv/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv6/conv/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv6/conv/_input_quantizer/Constant_1 [Constant] outputs: [/neck/detect3/conv6/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv6/conv/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv5/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv6/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv6/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv6/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/neck/detect3/conv5/relu/LeakyRelu_output_0 -> (1, 64, 276, 484)[FLOAT]], [/neck/detect3/conv6/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect3/conv6/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv6/conv/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/detect3/conv6/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv6/conv/_input_quantizer/Constant_output_0 for ONNX node: /neck/detect3/conv6/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv6/conv/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv6/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv6/conv/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect3/conv6/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 64, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv6/conv/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv6/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv6/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv6/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv6/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect3/conv6/conv/_input_quantizer/QuantizeLinear_output_0 -> (1, 64, 276, 484)[FLOAT]], [/neck/detect3/conv6/conv/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect3/conv6/conv/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv6/conv/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv6/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv6/conv/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect3/conv6/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 64, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv6/conv/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv6/conv/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv6/conv/_weight_quantizer/Constant [Constant] outputs: [/neck/detect3/conv6/conv/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv6/conv/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv6/conv/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv6/conv/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/detect3/conv6/conv/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv6.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv6/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv6/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.detect3.conv6.conv.weight -> (128, 64, 3, 3)[FLOAT]], [/neck/detect3/conv6/conv/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/neck/detect3/conv6/conv/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: neck.detect3.conv6.conv.weight for ONNX node: neck.detect3.conv6.conv.weight
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv6/conv/_weight_quantizer/Constant_output_0 for ONNX node: /neck/detect3/conv6/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv6/conv/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/detect3/conv6/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv6/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv6/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv6/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv6/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], [/neck/detect3/conv6/conv/_weight_quantizer/Constant_output_0 -> (128)[FLOAT]], [/neck/detect3/conv6/conv/_weight_quantizer/Constant_1_output_0 -> (128)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv6/conv/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv6/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv6/conv/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect3/conv6/conv/_weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv6/conv/Conv [Conv]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv6/conv/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv6/conv/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv6/conv/Conv [Conv] inputs: [/neck/detect3/conv6/conv/_input_quantizer/DequantizeLinear_output_0 -> (1, 64, 276, 484)[FLOAT]], [/neck/detect3/conv6/conv/_weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv6/conv/Conv for ONNX node: /neck/detect3/conv6/conv/Conv
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv6/conv/Conv_output_0 for ONNX tensor: /neck/detect3/conv6/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv6/conv/Conv [Conv] outputs: [/neck/detect3/conv6/conv/Conv_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv6/bn/BatchNormalization [BatchNormalization]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv6/conv/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv6.bn.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv6.bn.bias
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv6.bn.running_mean
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv6.bn.running_var
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv6/bn/BatchNormalization [BatchNormalization] inputs: [/neck/detect3/conv6/conv/Conv_output_0 -> (1, 128, 276, 484)[FLOAT]], [neck.detect3.conv6.bn.weight -> (128)[FLOAT]], [neck.detect3.conv6.bn.bias -> (128)[FLOAT]], [neck.detect3.conv6.bn.running_mean -> (128)[FLOAT]], [neck.detect3.conv6.bn.running_var -> (128)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv6/bn/BatchNormalization for ONNX node: /neck/detect3/conv6/bn/BatchNormalization
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv6/bn/BatchNormalization_output_0 for ONNX tensor: /neck/detect3/conv6/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv6/bn/BatchNormalization [BatchNormalization] outputs: [/neck/detect3/conv6/bn/BatchNormalization_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv6/relu/LeakyRelu [LeakyRelu]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv6/bn/BatchNormalization_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv6/relu/LeakyRelu [LeakyRelu] inputs: [/neck/detect3/conv6/bn/BatchNormalization_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv6/relu/LeakyRelu for ONNX node: /neck/detect3/conv6/relu/LeakyRelu
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv6/relu/LeakyRelu_output_0 for ONNX tensor: /neck/detect3/conv6/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv6/relu/LeakyRelu [LeakyRelu] outputs: [/neck/detect3/conv6/relu/LeakyRelu_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv7/_input_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv7/_input_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv7/_input_quantizer/Constant [Constant] outputs: [/neck/detect3/conv7/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv7/_input_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv7/_input_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv7/_input_quantizer/Constant_1 [Constant] outputs: [/neck/detect3/conv7/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv7/_input_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv6/relu/LeakyRelu_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv7/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv7/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv7/_input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/neck/detect3/conv6/relu/LeakyRelu_output_0 -> (1, 128, 276, 484)[FLOAT]], [/neck/detect3/conv7/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect3/conv7/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv7/_input_quantizer/Constant_1_output_0 for ONNX node: /neck/detect3/conv7/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv7/_input_quantizer/Constant_output_0 for ONNX node: /neck/detect3/conv7/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv7/_input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv7/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv7/_input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect3/conv7/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv7/_input_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv7/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv7/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv7/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv7/_input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect3/conv7/_input_quantizer/QuantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], [/neck/detect3/conv7/_input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/neck/detect3/conv7/_input_quantizer/Constant_output_0 -> ()[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv7/_input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv7/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv7/_input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect3/conv7/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv7/_weight_quantizer/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv7/_weight_quantizer/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv7/_weight_quantizer/Constant [Constant] outputs: [/neck/detect3/conv7/_weight_quantizer/Constant_output_0 -> (63)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv7/_weight_quantizer/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv7/_weight_quantizer/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv7/_weight_quantizer/Constant_1 [Constant] outputs: [/neck/detect3/conv7/_weight_quantizer/Constant_1_output_0 -> (63)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv7/_weight_quantizer/QuantizeLinear [QuantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv7.weight
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv7/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv7/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv7/_weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [neck.detect3.conv7.weight -> (63, 128, 1, 1)[FLOAT]], [/neck/detect3/conv7/_weight_quantizer/Constant_output_0 -> (63)[FLOAT]], [/neck/detect3/conv7/_weight_quantizer/Constant_1_output_0 -> (63)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: neck.detect3.conv7.weight for ONNX node: neck.detect3.conv7.weight
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv7/_weight_quantizer/Constant_output_0 for ONNX node: /neck/detect3/conv7/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv7/_weight_quantizer/Constant_1_output_0 for ONNX node: /neck/detect3/conv7/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv7/_weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv7/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv7/_weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/neck/detect3/conv7/_weight_quantizer/QuantizeLinear_output_0 -> (63, 128, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv7/_weight_quantizer/DequantizeLinear [DequantizeLinear]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv7/_weight_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv7/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv7/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv7/_weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/neck/detect3/conv7/_weight_quantizer/QuantizeLinear_output_0 -> (63, 128, 1, 1)[FLOAT]], [/neck/detect3/conv7/_weight_quantizer/Constant_output_0 -> (63)[FLOAT]], [/neck/detect3/conv7/_weight_quantizer/Constant_1_output_0 -> (63)[INT8]], 
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv7/_weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /neck/detect3/conv7/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv7/_weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/neck/detect3/conv7/_weight_quantizer/DequantizeLinear_output_0 -> (63, 128, 1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /neck/detect3/conv7/Conv [Conv]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv7/_input_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv7/_weight_quantizer/DequantizeLinear_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: neck.detect3.conv7.bias
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv7/Conv [Conv] inputs: [/neck/detect3/conv7/_input_quantizer/DequantizeLinear_output_0 -> (1, 128, 276, 484)[FLOAT]], [/neck/detect3/conv7/_weight_quantizer/DequantizeLinear_output_0 -> (63, 128, 1, 1)[FLOAT]], [neck.detect3.conv7.bias -> (63)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /neck/detect3/conv7/Conv for ONNX node: /neck/detect3/conv7/Conv
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /neck/detect3/conv7/Conv_output_0 for ONNX tensor: /neck/detect3/conv7/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] /neck/detect3/conv7/Conv [Conv] outputs: [/neck/detect3/conv7/Conv_output_0 -> (1, 63, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant [Constant] outputs: [/head/Constant_output_0 -> ()[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Transpose [Transpose]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect1/conv7/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Transpose [Transpose] inputs: [/neck/detect1/conv7/Conv_output_0 -> (1, 63, 69, 121)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Transpose for ONNX node: /head/Transpose
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Transpose_output_0 for ONNX tensor: /head/Transpose_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Transpose [Transpose] outputs: [/head/Transpose_output_0 -> (1, 69, 121, 63)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_1 [Constant] outputs: [/head/Constant_1_output_0 -> (5)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Reshape [Reshape]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Transpose_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Reshape [Reshape] inputs: [/head/Transpose_output_0 -> (1, 69, 121, 63)[FLOAT]], [/head/Constant_1_output_0 -> (5)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Reshape for ONNX node: /head/Reshape
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Reshape_output_0 for ONNX tensor: /head/Reshape_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Reshape [Reshape] outputs: [/head/Reshape_output_0 -> (1, 69, 121, 3, 21)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_2 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_2 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_2 [Constant] outputs: [/head/Constant_2_output_0 -> ()[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_3 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_3 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_3 [Constant] outputs: [/head/Constant_3_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_4 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_4 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_4 [Constant] outputs: [/head/Constant_4_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_5 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_5 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_5 [Constant] outputs: [/head/Constant_5_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_6 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_6 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_6 [Constant] outputs: [/head/Constant_6_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Slice [Slice]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Reshape_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_4_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_5_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_3_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_6_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Slice [Slice] inputs: [/head/Reshape_output_0 -> (1, 69, 121, 3, 21)[FLOAT]], [/head/Constant_4_output_0 -> (1)[INT32]], [/head/Constant_5_output_0 -> (1)[INT32]], [/head/Constant_3_output_0 -> (1)[INT32]], [/head/Constant_6_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Slice for ONNX node: /head/Slice
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Slice_output_0 for ONNX tensor: /head/Slice_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Slice [Slice] outputs: [/head/Slice_output_0 -> (1, 69, 121, 3, 16)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_7 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_7 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_7 [Constant] outputs: [/head/Constant_7_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_8 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_8 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_8 [Constant] outputs: [/head/Constant_8_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_9 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_9 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_9 [Constant] outputs: [/head/Constant_9_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_10 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_10 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_10 [Constant] outputs: [/head/Constant_10_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Slice_1 [Slice]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Reshape_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_8_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_9_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_7_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_10_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Slice_1 [Slice] inputs: [/head/Reshape_output_0 -> (1, 69, 121, 3, 21)[FLOAT]], [/head/Constant_8_output_0 -> (1)[INT32]], [/head/Constant_9_output_0 -> (1)[INT32]], [/head/Constant_7_output_0 -> (1)[INT32]], [/head/Constant_10_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Slice_1 for ONNX node: /head/Slice_1
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Slice_1_output_0 for ONNX tensor: /head/Slice_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Slice_1 [Slice] outputs: [/head/Slice_1_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Sigmoid [Sigmoid]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Slice_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sigmoid [Sigmoid] inputs: [/head/Slice_1_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Sigmoid for ONNX node: /head/Sigmoid
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Sigmoid_output_0 for ONNX tensor: /head/Sigmoid_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sigmoid [Sigmoid] outputs: [/head/Sigmoid_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Gather [Gather]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Reshape_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Gather [Gather] inputs: [/head/Reshape_output_0 -> (1, 69, 121, 3, 21)[FLOAT]], [/head/Constant_2_output_0 -> ()[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_2_output_0 for ONNX node: /head/Constant_2_output_0
[10/31/2023-17:53:35] [V] [TRT] Using Gather axis: 4
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Gather for ONNX node: /head/Gather
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Gather_output_0 for ONNX tensor: /head/Gather_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Gather [Gather] outputs: [/head/Gather_output_0 -> (1, 69, 121, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Sigmoid_1 [Sigmoid]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Gather_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sigmoid_1 [Sigmoid] inputs: [/head/Gather_output_0 -> (1, 69, 121, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Sigmoid_1 for ONNX node: /head/Sigmoid_1
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Sigmoid_1_output_0 for ONNX tensor: /head/Sigmoid_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sigmoid_1 [Sigmoid] outputs: [/head/Sigmoid_1_output_0 -> (1, 69, 121, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Softmax [Softmax]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Slice_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Softmax [Softmax] inputs: [/head/Slice_output_0 -> (1, 69, 121, 3, 16)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Softmax for ONNX node: /head/Softmax
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Softmax_output_0 for ONNX tensor: /head/Softmax_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Softmax [Softmax] outputs: [/head/Softmax_output_0 -> (1, 69, 121, 3, 16)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_11 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_11 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_11 [Constant] outputs: [/head/Constant_11_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_12 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_12 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_12 [Constant] outputs: [/head/Constant_12_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_13 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_13 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_13 [Constant] outputs: [/head/Constant_13_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_14 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_14 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_14 [Constant] outputs: [/head/Constant_14_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Slice_2 [Slice]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Reshape_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_12_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_13_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_11_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_14_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Slice_2 [Slice] inputs: [/head/Reshape_output_0 -> (1, 69, 121, 3, 21)[FLOAT]], [/head/Constant_12_output_0 -> (1)[INT32]], [/head/Constant_13_output_0 -> (1)[INT32]], [/head/Constant_11_output_0 -> (1)[INT32]], [/head/Constant_14_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Slice_2 for ONNX node: /head/Slice_2
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Slice_2_output_0 for ONNX tensor: /head/Slice_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Slice_2 [Slice] outputs: [/head/Slice_2_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_15 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_15 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_15 [Constant] outputs: [/head/Constant_15_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Mul [Mul]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Sigmoid_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_15_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Mul [Mul] inputs: [/head/Sigmoid_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], [/head/Constant_15_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_15_output_0 for ONNX node: /head/Constant_15_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Mul for ONNX node: /head/Mul
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Mul_output_0 for ONNX tensor: /head/Mul_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Mul [Mul] outputs: [/head/Mul_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_16 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_16 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_16 [Constant] outputs: [/head/Constant_16_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Sub [Sub]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Mul_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_16_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sub [Sub] inputs: [/head/Mul_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], [/head/Constant_16_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_16_output_0 for ONNX node: /head/Constant_16_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Sub for ONNX node: /head/Sub
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Sub_output_0 for ONNX tensor: /head/Sub_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sub [Sub] outputs: [/head/Sub_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_17 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_17 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_17 [Constant] outputs: [/head/Constant_17_output_0 -> (2)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_18 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_18 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_18 [Constant] outputs: [/head/Constant_18_output_0 -> (69, 1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Expand [Expand]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_18_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_17_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Expand [Expand] inputs: [/head/Constant_18_output_0 -> (69, 1)[INT32]], [/head/Constant_17_output_0 -> (2)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_18_output_0 for ONNX node: /head/Constant_18_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Expand for ONNX node: /head/Expand
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Expand_output_0 for ONNX tensor: /head/Expand_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Expand [Expand] outputs: [/head/Expand_output_0 -> (69, 121)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_19 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_19 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_19 [Constant] outputs: [/head/Constant_19_output_0 -> (1, 121)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Expand_1 [Expand]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_19_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_17_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Expand_1 [Expand] inputs: [/head/Constant_19_output_0 -> (1, 121)[INT32]], [/head/Constant_17_output_0 -> (2)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_19_output_0 for ONNX node: /head/Constant_19_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Expand_1 for ONNX node: /head/Expand_1
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Expand_1_output_0 for ONNX tensor: /head/Expand_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Expand_1 [Expand] outputs: [/head/Expand_1_output_0 -> (69, 121)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_20 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_20 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_20 [Constant] outputs: [/head/Constant_20_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Unsqueeze [Unsqueeze]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Expand_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_20_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze [Unsqueeze] inputs: [/head/Expand_1_output_0 -> (69, 121)[INT32]], [/head/Constant_20_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Original shape: (69, 121), unsqueezing to: (69, 121, 1)
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Unsqueeze for ONNX node: /head/Unsqueeze
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Unsqueeze_output_0 for ONNX tensor: /head/Unsqueeze_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze [Unsqueeze] outputs: [/head/Unsqueeze_output_0 -> (69, 121, 1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_21 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_21 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_21 [Constant] outputs: [/head/Constant_21_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Unsqueeze_1 [Unsqueeze]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Expand_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_21_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze_1 [Unsqueeze] inputs: [/head/Expand_output_0 -> (69, 121)[INT32]], [/head/Constant_21_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Original shape: (69, 121), unsqueezing to: (69, 121, 1)
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Unsqueeze_1 for ONNX node: /head/Unsqueeze_1
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Unsqueeze_1_output_0 for ONNX tensor: /head/Unsqueeze_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze_1 [Unsqueeze] outputs: [/head/Unsqueeze_1_output_0 -> (69, 121, 1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Concat [Concat]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Unsqueeze_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Unsqueeze_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Concat [Concat] inputs: [/head/Unsqueeze_output_0 -> (69, 121, 1)[INT32]], [/head/Unsqueeze_1_output_0 -> (69, 121, 1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Concat for ONNX node: /head/Concat
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Concat_output_0 for ONNX tensor: /head/Concat_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Concat [Concat] outputs: [/head/Concat_output_0 -> (69, 121, 2)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_22 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_22 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_22 [Constant] outputs: [/head/Constant_22_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Unsqueeze_2 [Unsqueeze]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Concat_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_22_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze_2 [Unsqueeze] inputs: [/head/Concat_output_0 -> (69, 121, 2)[INT32]], [/head/Constant_22_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Original shape: (69, 121, 2), unsqueezing to: (69, 121, 1, 2)
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Unsqueeze_2 for ONNX node: /head/Unsqueeze_2
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Unsqueeze_2_output_0 for ONNX tensor: /head/Unsqueeze_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze_2 [Unsqueeze] outputs: [/head/Unsqueeze_2_output_0 -> (69, 121, 1, 2)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Cast [Cast]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Unsqueeze_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Cast [Cast] inputs: [/head/Unsqueeze_2_output_0 -> (69, 121, 1, 2)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Casting to type: float32
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Cast for ONNX node: /head/Cast
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Cast_output_0 for ONNX tensor: /head/Cast_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Cast [Cast] outputs: [/head/Cast_output_0 -> (69, 121, 1, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Add [Add]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Sub_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Cast_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Add [Add] inputs: [/head/Sub_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], [/head/Cast_output_0 -> (69, 121, 1, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Add for ONNX node: /head/Add
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Add_output_0 for ONNX tensor: /head/Add_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Add [Add] outputs: [/head/Add_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_23 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_23 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_23 [Constant] outputs: [/head/Constant_23_output_0 -> (2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Mul_1 [Mul]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Add_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_23_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Mul_1 [Mul] inputs: [/head/Add_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], [/head/Constant_23_output_0 -> (2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_23_output_0 for ONNX node: /head/Constant_23_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Mul_1 for ONNX node: /head/Mul_1
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Mul_1_output_0 for ONNX tensor: /head/Mul_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Mul_1 [Mul] outputs: [/head/Mul_1_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Exp [Exp]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Slice_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Exp [Exp] inputs: [/head/Slice_2_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Exp for ONNX node: /head/Exp
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Exp_output_0 for ONNX tensor: /head/Exp_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Exp [Exp] outputs: [/head/Exp_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_24 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_24 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_24 [Constant] outputs: [/head/Constant_24_output_0 -> (3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Mul_2 [Mul]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Exp_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_24_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Mul_2 [Mul] inputs: [/head/Exp_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], [/head/Constant_24_output_0 -> (3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_24_output_0 for ONNX node: /head/Constant_24_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Mul_2 for ONNX node: /head/Mul_2
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Mul_2_output_0 for ONNX tensor: /head/Mul_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Mul_2 [Mul] outputs: [/head/Mul_2_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_25 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_25 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_25 [Constant] outputs: [/head/Constant_25_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Div [Div]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Mul_2_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_25_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Div [Div] inputs: [/head/Mul_2_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], [/head/Constant_25_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_25_output_0 for ONNX node: /head/Constant_25_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Div for ONNX node: /head/Div
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Div_output_0 for ONNX tensor: /head/Div_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Div [Div] outputs: [/head/Div_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Sub_1 [Sub]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Mul_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Div_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sub_1 [Sub] inputs: [/head/Mul_1_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], [/head/Div_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Sub_1 for ONNX node: /head/Sub_1
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Sub_1_output_0 for ONNX tensor: /head/Sub_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sub_1 [Sub] outputs: [/head/Sub_1_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Add_1 [Add]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Mul_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Div_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Add_1 [Add] inputs: [/head/Mul_1_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], [/head/Div_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Add_1 for ONNX node: /head/Add_1
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Add_1_output_0 for ONNX tensor: /head/Add_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Add_1 [Add] outputs: [/head/Add_1_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_26 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_26 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_26 [Constant] outputs: [/head/Constant_26_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Unsqueeze_3 [Unsqueeze]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Sigmoid_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_26_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze_3 [Unsqueeze] inputs: [/head/Sigmoid_1_output_0 -> (1, 69, 121, 3)[FLOAT]], [/head/Constant_26_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Original shape: (1, 69, 121, 3), unsqueezing to: (1, 69, 121, 3, 1)
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Unsqueeze_3 for ONNX node: /head/Unsqueeze_3
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Unsqueeze_3_output_0 for ONNX tensor: /head/Unsqueeze_3_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze_3 [Unsqueeze] outputs: [/head/Unsqueeze_3_output_0 -> (1, 69, 121, 3, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Concat_1 [Concat]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Sub_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Add_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Unsqueeze_3_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Softmax_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Concat_1 [Concat] inputs: [/head/Sub_1_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], [/head/Add_1_output_0 -> (1, 69, 121, 3, 2)[FLOAT]], [/head/Unsqueeze_3_output_0 -> (1, 69, 121, 3, 1)[FLOAT]], [/head/Softmax_output_0 -> (1, 69, 121, 3, 16)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Concat_1 for ONNX node: /head/Concat_1
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Concat_1_output_0 for ONNX tensor: /head/Concat_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Concat_1 [Concat] outputs: [/head/Concat_1_output_0 -> (1, 69, 121, 3, 21)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_27 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_27 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_27 [Constant] outputs: [/head/Constant_27_output_0 -> (3)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Reshape_1 [Reshape]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Concat_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_27_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Reshape_1 [Reshape] inputs: [/head/Concat_1_output_0 -> (1, 69, 121, 3, 21)[FLOAT]], [/head/Constant_27_output_0 -> (3)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Reshape_1 for ONNX node: /head/Reshape_1
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Reshape_1_output_0 for ONNX tensor: /head/Reshape_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Reshape_1 [Reshape] outputs: [/head/Reshape_1_output_0 -> (1, 25047, 21)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Transpose_1 [Transpose]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect2/conv7/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Transpose_1 [Transpose] inputs: [/neck/detect2/conv7/Conv_output_0 -> (1, 63, 138, 242)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Transpose_1 for ONNX node: /head/Transpose_1
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Transpose_1_output_0 for ONNX tensor: /head/Transpose_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Transpose_1 [Transpose] outputs: [/head/Transpose_1_output_0 -> (1, 138, 242, 63)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_28 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_28 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_28 [Constant] outputs: [/head/Constant_28_output_0 -> (5)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Reshape_2 [Reshape]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Transpose_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_28_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Reshape_2 [Reshape] inputs: [/head/Transpose_1_output_0 -> (1, 138, 242, 63)[FLOAT]], [/head/Constant_28_output_0 -> (5)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Reshape_2 for ONNX node: /head/Reshape_2
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Reshape_2_output_0 for ONNX tensor: /head/Reshape_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Reshape_2 [Reshape] outputs: [/head/Reshape_2_output_0 -> (1, 138, 242, 3, 21)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_29 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_29 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_29 [Constant] outputs: [/head/Constant_29_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_30 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_30 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_30 [Constant] outputs: [/head/Constant_30_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_31 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_31 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_31 [Constant] outputs: [/head/Constant_31_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_32 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_32 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_32 [Constant] outputs: [/head/Constant_32_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Slice_3 [Slice]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Reshape_2_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_30_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_31_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_29_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_32_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Slice_3 [Slice] inputs: [/head/Reshape_2_output_0 -> (1, 138, 242, 3, 21)[FLOAT]], [/head/Constant_30_output_0 -> (1)[INT32]], [/head/Constant_31_output_0 -> (1)[INT32]], [/head/Constant_29_output_0 -> (1)[INT32]], [/head/Constant_32_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Slice_3 for ONNX node: /head/Slice_3
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Slice_3_output_0 for ONNX tensor: /head/Slice_3_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Slice_3 [Slice] outputs: [/head/Slice_3_output_0 -> (1, 138, 242, 3, 16)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_33 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_33 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_33 [Constant] outputs: [/head/Constant_33_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_34 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_34 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_34 [Constant] outputs: [/head/Constant_34_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_35 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_35 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_35 [Constant] outputs: [/head/Constant_35_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_36 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_36 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_36 [Constant] outputs: [/head/Constant_36_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Slice_4 [Slice]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Reshape_2_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_34_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_35_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_33_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_36_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Slice_4 [Slice] inputs: [/head/Reshape_2_output_0 -> (1, 138, 242, 3, 21)[FLOAT]], [/head/Constant_34_output_0 -> (1)[INT32]], [/head/Constant_35_output_0 -> (1)[INT32]], [/head/Constant_33_output_0 -> (1)[INT32]], [/head/Constant_36_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Slice_4 for ONNX node: /head/Slice_4
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Slice_4_output_0 for ONNX tensor: /head/Slice_4_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Slice_4 [Slice] outputs: [/head/Slice_4_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Sigmoid_2 [Sigmoid]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Slice_4_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sigmoid_2 [Sigmoid] inputs: [/head/Slice_4_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Sigmoid_2 for ONNX node: /head/Sigmoid_2
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Sigmoid_2_output_0 for ONNX tensor: /head/Sigmoid_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sigmoid_2 [Sigmoid] outputs: [/head/Sigmoid_2_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Gather_1 [Gather]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Reshape_2_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Gather_1 [Gather] inputs: [/head/Reshape_2_output_0 -> (1, 138, 242, 3, 21)[FLOAT]], [/head/Constant_2_output_0 -> ()[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Using Gather axis: 4
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Gather_1 for ONNX node: /head/Gather_1
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Gather_1_output_0 for ONNX tensor: /head/Gather_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Gather_1 [Gather] outputs: [/head/Gather_1_output_0 -> (1, 138, 242, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Sigmoid_3 [Sigmoid]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Gather_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sigmoid_3 [Sigmoid] inputs: [/head/Gather_1_output_0 -> (1, 138, 242, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Sigmoid_3 for ONNX node: /head/Sigmoid_3
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Sigmoid_3_output_0 for ONNX tensor: /head/Sigmoid_3_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sigmoid_3 [Sigmoid] outputs: [/head/Sigmoid_3_output_0 -> (1, 138, 242, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Softmax_1 [Softmax]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Slice_3_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Softmax_1 [Softmax] inputs: [/head/Slice_3_output_0 -> (1, 138, 242, 3, 16)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Softmax_1 for ONNX node: /head/Softmax_1
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Softmax_1_output_0 for ONNX tensor: /head/Softmax_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Softmax_1 [Softmax] outputs: [/head/Softmax_1_output_0 -> (1, 138, 242, 3, 16)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_37 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_37 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_37 [Constant] outputs: [/head/Constant_37_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_38 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_38 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_38 [Constant] outputs: [/head/Constant_38_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_39 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_39 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_39 [Constant] outputs: [/head/Constant_39_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_40 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_40 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_40 [Constant] outputs: [/head/Constant_40_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Slice_5 [Slice]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Reshape_2_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_38_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_39_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_37_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_40_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Slice_5 [Slice] inputs: [/head/Reshape_2_output_0 -> (1, 138, 242, 3, 21)[FLOAT]], [/head/Constant_38_output_0 -> (1)[INT32]], [/head/Constant_39_output_0 -> (1)[INT32]], [/head/Constant_37_output_0 -> (1)[INT32]], [/head/Constant_40_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Slice_5 for ONNX node: /head/Slice_5
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Slice_5_output_0 for ONNX tensor: /head/Slice_5_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Slice_5 [Slice] outputs: [/head/Slice_5_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_41 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_41 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_41 [Constant] outputs: [/head/Constant_41_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Mul_3 [Mul]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Sigmoid_2_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_41_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Mul_3 [Mul] inputs: [/head/Sigmoid_2_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], [/head/Constant_41_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_41_output_0 for ONNX node: /head/Constant_41_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Mul_3 for ONNX node: /head/Mul_3
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Mul_3_output_0 for ONNX tensor: /head/Mul_3_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Mul_3 [Mul] outputs: [/head/Mul_3_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_42 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_42 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_42 [Constant] outputs: [/head/Constant_42_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Sub_2 [Sub]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Mul_3_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_42_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sub_2 [Sub] inputs: [/head/Mul_3_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], [/head/Constant_42_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_42_output_0 for ONNX node: /head/Constant_42_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Sub_2 for ONNX node: /head/Sub_2
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Sub_2_output_0 for ONNX tensor: /head/Sub_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sub_2 [Sub] outputs: [/head/Sub_2_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_43 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_43 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_43 [Constant] outputs: [/head/Constant_43_output_0 -> (2)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_44 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_44 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_44 [Constant] outputs: [/head/Constant_44_output_0 -> (138, 1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Expand_2 [Expand]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_44_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_43_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Expand_2 [Expand] inputs: [/head/Constant_44_output_0 -> (138, 1)[INT32]], [/head/Constant_43_output_0 -> (2)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_44_output_0 for ONNX node: /head/Constant_44_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Expand_2 for ONNX node: /head/Expand_2
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Expand_2_output_0 for ONNX tensor: /head/Expand_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Expand_2 [Expand] outputs: [/head/Expand_2_output_0 -> (138, 242)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_45 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_45 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_45 [Constant] outputs: [/head/Constant_45_output_0 -> (1, 242)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Expand_3 [Expand]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_45_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_43_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Expand_3 [Expand] inputs: [/head/Constant_45_output_0 -> (1, 242)[INT32]], [/head/Constant_43_output_0 -> (2)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_45_output_0 for ONNX node: /head/Constant_45_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Expand_3 for ONNX node: /head/Expand_3
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Expand_3_output_0 for ONNX tensor: /head/Expand_3_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Expand_3 [Expand] outputs: [/head/Expand_3_output_0 -> (138, 242)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_46 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_46 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_46 [Constant] outputs: [/head/Constant_46_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Unsqueeze_4 [Unsqueeze]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Expand_3_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_46_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze_4 [Unsqueeze] inputs: [/head/Expand_3_output_0 -> (138, 242)[INT32]], [/head/Constant_46_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Original shape: (138, 242), unsqueezing to: (138, 242, 1)
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Unsqueeze_4 for ONNX node: /head/Unsqueeze_4
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Unsqueeze_4_output_0 for ONNX tensor: /head/Unsqueeze_4_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze_4 [Unsqueeze] outputs: [/head/Unsqueeze_4_output_0 -> (138, 242, 1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_47 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_47 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_47 [Constant] outputs: [/head/Constant_47_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Unsqueeze_5 [Unsqueeze]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Expand_2_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_47_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze_5 [Unsqueeze] inputs: [/head/Expand_2_output_0 -> (138, 242)[INT32]], [/head/Constant_47_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Original shape: (138, 242), unsqueezing to: (138, 242, 1)
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Unsqueeze_5 for ONNX node: /head/Unsqueeze_5
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Unsqueeze_5_output_0 for ONNX tensor: /head/Unsqueeze_5_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze_5 [Unsqueeze] outputs: [/head/Unsqueeze_5_output_0 -> (138, 242, 1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Concat_2 [Concat]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Unsqueeze_4_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Unsqueeze_5_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Concat_2 [Concat] inputs: [/head/Unsqueeze_4_output_0 -> (138, 242, 1)[INT32]], [/head/Unsqueeze_5_output_0 -> (138, 242, 1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Concat_2 for ONNX node: /head/Concat_2
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Concat_2_output_0 for ONNX tensor: /head/Concat_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Concat_2 [Concat] outputs: [/head/Concat_2_output_0 -> (138, 242, 2)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_48 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_48 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_48 [Constant] outputs: [/head/Constant_48_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Unsqueeze_6 [Unsqueeze]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Concat_2_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_48_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze_6 [Unsqueeze] inputs: [/head/Concat_2_output_0 -> (138, 242, 2)[INT32]], [/head/Constant_48_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Original shape: (138, 242, 2), unsqueezing to: (138, 242, 1, 2)
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Unsqueeze_6 for ONNX node: /head/Unsqueeze_6
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Unsqueeze_6_output_0 for ONNX tensor: /head/Unsqueeze_6_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze_6 [Unsqueeze] outputs: [/head/Unsqueeze_6_output_0 -> (138, 242, 1, 2)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Cast_1 [Cast]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Unsqueeze_6_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Cast_1 [Cast] inputs: [/head/Unsqueeze_6_output_0 -> (138, 242, 1, 2)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Casting to type: float32
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Cast_1 for ONNX node: /head/Cast_1
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Cast_1_output_0 for ONNX tensor: /head/Cast_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Cast_1 [Cast] outputs: [/head/Cast_1_output_0 -> (138, 242, 1, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Add_2 [Add]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Sub_2_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Cast_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Add_2 [Add] inputs: [/head/Sub_2_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], [/head/Cast_1_output_0 -> (138, 242, 1, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Add_2 for ONNX node: /head/Add_2
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Add_2_output_0 for ONNX tensor: /head/Add_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Add_2 [Add] outputs: [/head/Add_2_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_49 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_49 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_49 [Constant] outputs: [/head/Constant_49_output_0 -> (2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Mul_4 [Mul]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Add_2_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_49_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Mul_4 [Mul] inputs: [/head/Add_2_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], [/head/Constant_49_output_0 -> (2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_49_output_0 for ONNX node: /head/Constant_49_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Mul_4 for ONNX node: /head/Mul_4
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Mul_4_output_0 for ONNX tensor: /head/Mul_4_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Mul_4 [Mul] outputs: [/head/Mul_4_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Exp_1 [Exp]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Slice_5_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Exp_1 [Exp] inputs: [/head/Slice_5_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Exp_1 for ONNX node: /head/Exp_1
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Exp_1_output_0 for ONNX tensor: /head/Exp_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Exp_1 [Exp] outputs: [/head/Exp_1_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_50 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_50 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_50 [Constant] outputs: [/head/Constant_50_output_0 -> (3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Mul_5 [Mul]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Exp_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_50_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Mul_5 [Mul] inputs: [/head/Exp_1_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], [/head/Constant_50_output_0 -> (3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_50_output_0 for ONNX node: /head/Constant_50_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Mul_5 for ONNX node: /head/Mul_5
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Mul_5_output_0 for ONNX tensor: /head/Mul_5_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Mul_5 [Mul] outputs: [/head/Mul_5_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_51 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_51 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_51 [Constant] outputs: [/head/Constant_51_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Div_1 [Div]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Mul_5_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_51_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Div_1 [Div] inputs: [/head/Mul_5_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], [/head/Constant_51_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_51_output_0 for ONNX node: /head/Constant_51_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Div_1 for ONNX node: /head/Div_1
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Div_1_output_0 for ONNX tensor: /head/Div_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Div_1 [Div] outputs: [/head/Div_1_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Sub_3 [Sub]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Mul_4_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Div_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sub_3 [Sub] inputs: [/head/Mul_4_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], [/head/Div_1_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Sub_3 for ONNX node: /head/Sub_3
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Sub_3_output_0 for ONNX tensor: /head/Sub_3_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sub_3 [Sub] outputs: [/head/Sub_3_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Add_3 [Add]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Mul_4_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Div_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Add_3 [Add] inputs: [/head/Mul_4_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], [/head/Div_1_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Add_3 for ONNX node: /head/Add_3
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Add_3_output_0 for ONNX tensor: /head/Add_3_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Add_3 [Add] outputs: [/head/Add_3_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_52 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_52 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_52 [Constant] outputs: [/head/Constant_52_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Unsqueeze_7 [Unsqueeze]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Sigmoid_3_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_52_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze_7 [Unsqueeze] inputs: [/head/Sigmoid_3_output_0 -> (1, 138, 242, 3)[FLOAT]], [/head/Constant_52_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Original shape: (1, 138, 242, 3), unsqueezing to: (1, 138, 242, 3, 1)
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Unsqueeze_7 for ONNX node: /head/Unsqueeze_7
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Unsqueeze_7_output_0 for ONNX tensor: /head/Unsqueeze_7_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze_7 [Unsqueeze] outputs: [/head/Unsqueeze_7_output_0 -> (1, 138, 242, 3, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Concat_3 [Concat]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Sub_3_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Add_3_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Unsqueeze_7_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Softmax_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Concat_3 [Concat] inputs: [/head/Sub_3_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], [/head/Add_3_output_0 -> (1, 138, 242, 3, 2)[FLOAT]], [/head/Unsqueeze_7_output_0 -> (1, 138, 242, 3, 1)[FLOAT]], [/head/Softmax_1_output_0 -> (1, 138, 242, 3, 16)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Concat_3 for ONNX node: /head/Concat_3
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Concat_3_output_0 for ONNX tensor: /head/Concat_3_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Concat_3 [Concat] outputs: [/head/Concat_3_output_0 -> (1, 138, 242, 3, 21)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_53 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_53 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_53 [Constant] outputs: [/head/Constant_53_output_0 -> (3)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Reshape_3 [Reshape]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Concat_3_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_53_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Reshape_3 [Reshape] inputs: [/head/Concat_3_output_0 -> (1, 138, 242, 3, 21)[FLOAT]], [/head/Constant_53_output_0 -> (3)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Reshape_3 for ONNX node: /head/Reshape_3
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Reshape_3_output_0 for ONNX tensor: /head/Reshape_3_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Reshape_3 [Reshape] outputs: [/head/Reshape_3_output_0 -> (1, 100188, 21)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Transpose_2 [Transpose]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /neck/detect3/conv7/Conv_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Transpose_2 [Transpose] inputs: [/neck/detect3/conv7/Conv_output_0 -> (1, 63, 276, 484)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Transpose_2 for ONNX node: /head/Transpose_2
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Transpose_2_output_0 for ONNX tensor: /head/Transpose_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Transpose_2 [Transpose] outputs: [/head/Transpose_2_output_0 -> (1, 276, 484, 63)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_54 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_54 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_54 [Constant] outputs: [/head/Constant_54_output_0 -> (5)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Reshape_4 [Reshape]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Transpose_2_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_54_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Reshape_4 [Reshape] inputs: [/head/Transpose_2_output_0 -> (1, 276, 484, 63)[FLOAT]], [/head/Constant_54_output_0 -> (5)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Reshape_4 for ONNX node: /head/Reshape_4
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Reshape_4_output_0 for ONNX tensor: /head/Reshape_4_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Reshape_4 [Reshape] outputs: [/head/Reshape_4_output_0 -> (1, 276, 484, 3, 21)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_55 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_55 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_55 [Constant] outputs: [/head/Constant_55_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_56 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_56 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_56 [Constant] outputs: [/head/Constant_56_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_57 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_57 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_57 [Constant] outputs: [/head/Constant_57_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_58 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_58 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_58 [Constant] outputs: [/head/Constant_58_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Slice_6 [Slice]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Reshape_4_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_56_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_57_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_55_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_58_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Slice_6 [Slice] inputs: [/head/Reshape_4_output_0 -> (1, 276, 484, 3, 21)[FLOAT]], [/head/Constant_56_output_0 -> (1)[INT32]], [/head/Constant_57_output_0 -> (1)[INT32]], [/head/Constant_55_output_0 -> (1)[INT32]], [/head/Constant_58_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Slice_6 for ONNX node: /head/Slice_6
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Slice_6_output_0 for ONNX tensor: /head/Slice_6_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Slice_6 [Slice] outputs: [/head/Slice_6_output_0 -> (1, 276, 484, 3, 16)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_59 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_59 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_59 [Constant] outputs: [/head/Constant_59_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_60 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_60 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_60 [Constant] outputs: [/head/Constant_60_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_61 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_61 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_61 [Constant] outputs: [/head/Constant_61_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_62 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_62 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_62 [Constant] outputs: [/head/Constant_62_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Slice_7 [Slice]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Reshape_4_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_60_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_61_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_59_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_62_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Slice_7 [Slice] inputs: [/head/Reshape_4_output_0 -> (1, 276, 484, 3, 21)[FLOAT]], [/head/Constant_60_output_0 -> (1)[INT32]], [/head/Constant_61_output_0 -> (1)[INT32]], [/head/Constant_59_output_0 -> (1)[INT32]], [/head/Constant_62_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Slice_7 for ONNX node: /head/Slice_7
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Slice_7_output_0 for ONNX tensor: /head/Slice_7_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Slice_7 [Slice] outputs: [/head/Slice_7_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Sigmoid_4 [Sigmoid]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Slice_7_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sigmoid_4 [Sigmoid] inputs: [/head/Slice_7_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Sigmoid_4 for ONNX node: /head/Sigmoid_4
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Sigmoid_4_output_0 for ONNX tensor: /head/Sigmoid_4_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sigmoid_4 [Sigmoid] outputs: [/head/Sigmoid_4_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Gather_2 [Gather]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Reshape_4_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Gather_2 [Gather] inputs: [/head/Reshape_4_output_0 -> (1, 276, 484, 3, 21)[FLOAT]], [/head/Constant_2_output_0 -> ()[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Using Gather axis: 4
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Gather_2 for ONNX node: /head/Gather_2
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Gather_2_output_0 for ONNX tensor: /head/Gather_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Gather_2 [Gather] outputs: [/head/Gather_2_output_0 -> (1, 276, 484, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Sigmoid_5 [Sigmoid]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Gather_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sigmoid_5 [Sigmoid] inputs: [/head/Gather_2_output_0 -> (1, 276, 484, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Sigmoid_5 for ONNX node: /head/Sigmoid_5
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Sigmoid_5_output_0 for ONNX tensor: /head/Sigmoid_5_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sigmoid_5 [Sigmoid] outputs: [/head/Sigmoid_5_output_0 -> (1, 276, 484, 3)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Softmax_2 [Softmax]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Slice_6_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Softmax_2 [Softmax] inputs: [/head/Slice_6_output_0 -> (1, 276, 484, 3, 16)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Softmax_2 for ONNX node: /head/Softmax_2
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Softmax_2_output_0 for ONNX tensor: /head/Softmax_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Softmax_2 [Softmax] outputs: [/head/Softmax_2_output_0 -> (1, 276, 484, 3, 16)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_63 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_63 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_63 [Constant] outputs: [/head/Constant_63_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_64 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_64 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_64 [Constant] outputs: [/head/Constant_64_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_65 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_65 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_65 [Constant] outputs: [/head/Constant_65_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_66 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_66 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_66 [Constant] outputs: [/head/Constant_66_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Slice_8 [Slice]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Reshape_4_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_64_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_65_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_63_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_66_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Slice_8 [Slice] inputs: [/head/Reshape_4_output_0 -> (1, 276, 484, 3, 21)[FLOAT]], [/head/Constant_64_output_0 -> (1)[INT32]], [/head/Constant_65_output_0 -> (1)[INT32]], [/head/Constant_63_output_0 -> (1)[INT32]], [/head/Constant_66_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Slice_8 for ONNX node: /head/Slice_8
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Slice_8_output_0 for ONNX tensor: /head/Slice_8_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Slice_8 [Slice] outputs: [/head/Slice_8_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_67 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_67 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_67 [Constant] outputs: [/head/Constant_67_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Mul_6 [Mul]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Sigmoid_4_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_67_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Mul_6 [Mul] inputs: [/head/Sigmoid_4_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], [/head/Constant_67_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_67_output_0 for ONNX node: /head/Constant_67_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Mul_6 for ONNX node: /head/Mul_6
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Mul_6_output_0 for ONNX tensor: /head/Mul_6_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Mul_6 [Mul] outputs: [/head/Mul_6_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_68 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_68 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_68 [Constant] outputs: [/head/Constant_68_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Sub_4 [Sub]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Mul_6_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_68_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sub_4 [Sub] inputs: [/head/Mul_6_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], [/head/Constant_68_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_68_output_0 for ONNX node: /head/Constant_68_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Sub_4 for ONNX node: /head/Sub_4
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Sub_4_output_0 for ONNX tensor: /head/Sub_4_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sub_4 [Sub] outputs: [/head/Sub_4_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_69 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_69 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_69 [Constant] outputs: [/head/Constant_69_output_0 -> (2)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_70 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_70 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_70 [Constant] outputs: [/head/Constant_70_output_0 -> (276, 1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Expand_4 [Expand]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_70_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_69_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Expand_4 [Expand] inputs: [/head/Constant_70_output_0 -> (276, 1)[INT32]], [/head/Constant_69_output_0 -> (2)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_70_output_0 for ONNX node: /head/Constant_70_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Expand_4 for ONNX node: /head/Expand_4
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Expand_4_output_0 for ONNX tensor: /head/Expand_4_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Expand_4 [Expand] outputs: [/head/Expand_4_output_0 -> (276, 484)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_71 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_71 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_71 [Constant] outputs: [/head/Constant_71_output_0 -> (1, 484)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Expand_5 [Expand]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_71_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_69_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Expand_5 [Expand] inputs: [/head/Constant_71_output_0 -> (1, 484)[INT32]], [/head/Constant_69_output_0 -> (2)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_71_output_0 for ONNX node: /head/Constant_71_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Expand_5 for ONNX node: /head/Expand_5
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Expand_5_output_0 for ONNX tensor: /head/Expand_5_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Expand_5 [Expand] outputs: [/head/Expand_5_output_0 -> (276, 484)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_72 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_72 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_72 [Constant] outputs: [/head/Constant_72_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Unsqueeze_8 [Unsqueeze]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Expand_5_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_72_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze_8 [Unsqueeze] inputs: [/head/Expand_5_output_0 -> (276, 484)[INT32]], [/head/Constant_72_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Original shape: (276, 484), unsqueezing to: (276, 484, 1)
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Unsqueeze_8 for ONNX node: /head/Unsqueeze_8
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Unsqueeze_8_output_0 for ONNX tensor: /head/Unsqueeze_8_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze_8 [Unsqueeze] outputs: [/head/Unsqueeze_8_output_0 -> (276, 484, 1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_73 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_73 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_73 [Constant] outputs: [/head/Constant_73_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Unsqueeze_9 [Unsqueeze]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Expand_4_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_73_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze_9 [Unsqueeze] inputs: [/head/Expand_4_output_0 -> (276, 484)[INT32]], [/head/Constant_73_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Original shape: (276, 484), unsqueezing to: (276, 484, 1)
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Unsqueeze_9 for ONNX node: /head/Unsqueeze_9
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Unsqueeze_9_output_0 for ONNX tensor: /head/Unsqueeze_9_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze_9 [Unsqueeze] outputs: [/head/Unsqueeze_9_output_0 -> (276, 484, 1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Concat_4 [Concat]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Unsqueeze_8_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Unsqueeze_9_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Concat_4 [Concat] inputs: [/head/Unsqueeze_8_output_0 -> (276, 484, 1)[INT32]], [/head/Unsqueeze_9_output_0 -> (276, 484, 1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Concat_4 for ONNX node: /head/Concat_4
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Concat_4_output_0 for ONNX tensor: /head/Concat_4_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Concat_4 [Concat] outputs: [/head/Concat_4_output_0 -> (276, 484, 2)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_74 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_74 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_74 [Constant] outputs: [/head/Constant_74_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Unsqueeze_10 [Unsqueeze]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Concat_4_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_74_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze_10 [Unsqueeze] inputs: [/head/Concat_4_output_0 -> (276, 484, 2)[INT32]], [/head/Constant_74_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Original shape: (276, 484, 2), unsqueezing to: (276, 484, 1, 2)
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Unsqueeze_10 for ONNX node: /head/Unsqueeze_10
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Unsqueeze_10_output_0 for ONNX tensor: /head/Unsqueeze_10_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze_10 [Unsqueeze] outputs: [/head/Unsqueeze_10_output_0 -> (276, 484, 1, 2)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Cast_2 [Cast]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Unsqueeze_10_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Cast_2 [Cast] inputs: [/head/Unsqueeze_10_output_0 -> (276, 484, 1, 2)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Casting to type: float32
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Cast_2 for ONNX node: /head/Cast_2
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Cast_2_output_0 for ONNX tensor: /head/Cast_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Cast_2 [Cast] outputs: [/head/Cast_2_output_0 -> (276, 484, 1, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Add_4 [Add]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Sub_4_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Cast_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Add_4 [Add] inputs: [/head/Sub_4_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], [/head/Cast_2_output_0 -> (276, 484, 1, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Add_4 for ONNX node: /head/Add_4
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Add_4_output_0 for ONNX tensor: /head/Add_4_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Add_4 [Add] outputs: [/head/Add_4_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_75 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_75 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_75 [Constant] outputs: [/head/Constant_75_output_0 -> (2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Mul_7 [Mul]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Add_4_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_75_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Mul_7 [Mul] inputs: [/head/Add_4_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], [/head/Constant_75_output_0 -> (2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_75_output_0 for ONNX node: /head/Constant_75_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Mul_7 for ONNX node: /head/Mul_7
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Mul_7_output_0 for ONNX tensor: /head/Mul_7_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Mul_7 [Mul] outputs: [/head/Mul_7_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Exp_2 [Exp]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Slice_8_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Exp_2 [Exp] inputs: [/head/Slice_8_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Exp_2 for ONNX node: /head/Exp_2
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Exp_2_output_0 for ONNX tensor: /head/Exp_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Exp_2 [Exp] outputs: [/head/Exp_2_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_76 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_76 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_76 [Constant] outputs: [/head/Constant_76_output_0 -> (3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Mul_8 [Mul]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Exp_2_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_76_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Mul_8 [Mul] inputs: [/head/Exp_2_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], [/head/Constant_76_output_0 -> (3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_76_output_0 for ONNX node: /head/Constant_76_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Mul_8 for ONNX node: /head/Mul_8
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Mul_8_output_0 for ONNX tensor: /head/Mul_8_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Mul_8 [Mul] outputs: [/head/Mul_8_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_77 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_77 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_77 [Constant] outputs: [/head/Constant_77_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Div_2 [Div]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Mul_8_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_77_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Div_2 [Div] inputs: [/head/Mul_8_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], [/head/Constant_77_output_0 -> ()[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_77_output_0 for ONNX node: /head/Constant_77_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Div_2 for ONNX node: /head/Div_2
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Div_2_output_0 for ONNX tensor: /head/Div_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Div_2 [Div] outputs: [/head/Div_2_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Sub_5 [Sub]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Mul_7_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Div_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sub_5 [Sub] inputs: [/head/Mul_7_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], [/head/Div_2_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Sub_5 for ONNX node: /head/Sub_5
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Sub_5_output_0 for ONNX tensor: /head/Sub_5_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Sub_5 [Sub] outputs: [/head/Sub_5_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Add_5 [Add]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Mul_7_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Div_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Add_5 [Add] inputs: [/head/Mul_7_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], [/head/Div_2_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Add_5 for ONNX node: /head/Add_5
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Add_5_output_0 for ONNX tensor: /head/Add_5_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Add_5 [Add] outputs: [/head/Add_5_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_78 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_78 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_78 [Constant] outputs: [/head/Constant_78_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Unsqueeze_11 [Unsqueeze]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Sigmoid_5_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_78_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze_11 [Unsqueeze] inputs: [/head/Sigmoid_5_output_0 -> (1, 276, 484, 3)[FLOAT]], [/head/Constant_78_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Original shape: (1, 276, 484, 3), unsqueezing to: (1, 276, 484, 3, 1)
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Unsqueeze_11 for ONNX node: /head/Unsqueeze_11
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Unsqueeze_11_output_0 for ONNX tensor: /head/Unsqueeze_11_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Unsqueeze_11 [Unsqueeze] outputs: [/head/Unsqueeze_11_output_0 -> (1, 276, 484, 3, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Concat_5 [Concat]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Sub_5_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Add_5_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Unsqueeze_11_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Softmax_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Concat_5 [Concat] inputs: [/head/Sub_5_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], [/head/Add_5_output_0 -> (1, 276, 484, 3, 2)[FLOAT]], [/head/Unsqueeze_11_output_0 -> (1, 276, 484, 3, 1)[FLOAT]], [/head/Softmax_2_output_0 -> (1, 276, 484, 3, 16)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Concat_5 for ONNX node: /head/Concat_5
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Concat_5_output_0 for ONNX tensor: /head/Concat_5_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Concat_5 [Concat] outputs: [/head/Concat_5_output_0 -> (1, 276, 484, 3, 21)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Constant_79 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_79 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /head/Constant_79 [Constant] outputs: [/head/Constant_79_output_0 -> (3)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /head/Reshape_5 [Reshape]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Concat_5_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_79_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Reshape_5 [Reshape] inputs: [/head/Concat_5_output_0 -> (1, 276, 484, 3, 21)[FLOAT]], [/head/Constant_79_output_0 -> (3)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Reshape_5 for ONNX node: /head/Reshape_5
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /head/Reshape_5_output_0 for ONNX tensor: /head/Reshape_5_output_0
[10/31/2023-17:53:35] [V] [TRT] /head/Reshape_5 [Reshape] outputs: [/head/Reshape_5_output_0 -> (1, 400752, 21)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Concat [Concat]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Reshape_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Reshape_3_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Reshape_5_output_0
[10/31/2023-17:53:35] [V] [TRT] /Concat [Concat] inputs: [/head/Reshape_1_output_0 -> (1, 25047, 21)[FLOAT]], [/head/Reshape_3_output_0 -> (1, 100188, 21)[FLOAT]], [/head/Reshape_5_output_0 -> (1, 400752, 21)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Concat for ONNX node: /Concat
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Concat_output_0 for ONNX tensor: /Concat_output_0
[10/31/2023-17:53:35] [V] [TRT] /Concat [Concat] outputs: [/Concat_output_0 -> (1, 525987, 21)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Constant [Constant]
[10/31/2023-17:53:35] [V] [TRT] /Constant [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /Constant [Constant] outputs: [/Constant_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Constant_1 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /Constant_1 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /Constant_1 [Constant] outputs: [/Constant_1_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Constant_2 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /Constant_2 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /Constant_2 [Constant] outputs: [/Constant_2_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Constant_3 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /Constant_3 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /Constant_3 [Constant] outputs: [/Constant_3_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Slice [Slice]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Concat_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Constant_2_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Constant_3_output_0
[10/31/2023-17:53:35] [V] [TRT] /Slice [Slice] inputs: [/Concat_output_0 -> (1, 525987, 21)[FLOAT]], [/Constant_1_output_0 -> (1)[INT32]], [/Constant_2_output_0 -> (1)[INT32]], [/Constant_output_0 -> (1)[INT32]], [/Constant_3_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Slice for ONNX node: /Slice
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Slice_output_0 for ONNX tensor: /Slice_output_0
[10/31/2023-17:53:35] [V] [TRT] /Slice [Slice] outputs: [/Slice_output_0 -> (1, 525987, 4)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Gather [Gather]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Concat_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /Gather [Gather] inputs: [/Concat_output_0 -> (1, 525987, 21)[FLOAT]], [/head/Constant_2_output_0 -> ()[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Using Gather axis: 2
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Gather for ONNX node: /Gather
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Gather_output_0 for ONNX tensor: /Gather_output_0
[10/31/2023-17:53:35] [V] [TRT] /Gather [Gather] outputs: [/Gather_output_0 -> (1, 525987)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Constant_4 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /Constant_4 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /Constant_4 [Constant] outputs: [/Constant_4_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Constant_5 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /Constant_5 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /Constant_5 [Constant] outputs: [/Constant_5_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Constant_6 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /Constant_6 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] Weight at index 0: 9223372036854775807 is out of range. Clamping to: 2147483647
[10/31/2023-17:53:35] [V] [TRT] /Constant_6 [Constant] outputs: [/Constant_6_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Constant_7 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /Constant_7 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /Constant_7 [Constant] outputs: [/Constant_7_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Slice_1 [Slice]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Concat_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Constant_5_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Constant_6_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Constant_4_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Constant_7_output_0
[10/31/2023-17:53:35] [V] [TRT] /Slice_1 [Slice] inputs: [/Concat_output_0 -> (1, 525987, 21)[FLOAT]], [/Constant_5_output_0 -> (1)[INT32]], [/Constant_6_output_0 -> (1)[INT32]], [/Constant_4_output_0 -> (1)[INT32]], [/Constant_7_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Slice_1 for ONNX node: /Slice_1
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Slice_1_output_0 for ONNX tensor: /Slice_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /Slice_1 [Slice] outputs: [/Slice_1_output_0 -> (1, 525987, 16)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /ReduceMax [ReduceMax]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Slice_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /ReduceMax [ReduceMax] inputs: [/Slice_1_output_0 -> (1, 525987, 16)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /ReduceMax for ONNX node: /ReduceMax
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /ReduceMax_output_0 for ONNX tensor: /ReduceMax_output_0
[10/31/2023-17:53:35] [V] [TRT] /ReduceMax [ReduceMax] outputs: [/ReduceMax_output_0 -> (1, 525987)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Mul [Mul]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Gather_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /ReduceMax_output_0
[10/31/2023-17:53:35] [V] [TRT] /Mul [Mul] inputs: [/Gather_output_0 -> (1, 525987)[FLOAT]], [/ReduceMax_output_0 -> (1, 525987)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Mul for ONNX node: /Mul
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Mul_output_0 for ONNX tensor: /Mul_output_0
[10/31/2023-17:53:35] [V] [TRT] /Mul [Mul] outputs: [/Mul_output_0 -> (1, 525987)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: Constant_660 [Constant]
[10/31/2023-17:53:35] [V] [TRT] Constant_660 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] Constant_660 [Constant] outputs: [onnx::NonMaxSuppression_1140 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: Constant_661 [Constant]
[10/31/2023-17:53:35] [V] [TRT] Constant_661 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] Constant_661 [Constant] outputs: [onnx::NonMaxSuppression_1141 -> (1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: Constant_662 [Constant]
[10/31/2023-17:53:35] [V] [TRT] Constant_662 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] Constant_662 [Constant] outputs: [onnx::NonMaxSuppression_1142 -> (1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Constant_8 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /Constant_8 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /Constant_8 [Constant] outputs: [/Constant_8_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Split [Split]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Slice_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Constant_8_output_0
[10/31/2023-17:53:35] [V] [TRT] /Split [Split] inputs: [/Slice_output_0 -> (1, 525987, 4)[FLOAT]], [/Constant_8_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Split for ONNX node: /Split
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Split_output_0 for ONNX tensor: /Split_output_0
[10/31/2023-17:53:35] [V] [TRT] /Split [Split] outputs: [/Split_output_0 -> (1, 525987, 4)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Constant_9 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /Constant_9 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /Constant_9 [Constant] outputs: [/Constant_9_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Squeeze [Squeeze]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Split_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Constant_9_output_0
[10/31/2023-17:53:35] [V] [TRT] /Squeeze [Squeeze] inputs: [/Split_output_0 -> (1, 525987, 4)[FLOAT]], [/Constant_9_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Original shape: (1, 525987, 4), squeezing to: (525987, 4)
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Squeeze for ONNX node: /Squeeze
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Squeeze_output_0 for ONNX tensor: /Squeeze_output_0
[10/31/2023-17:53:35] [V] [TRT] /Squeeze [Squeeze] outputs: [/Squeeze_output_0 -> (525987, 4)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Constant_10 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /Constant_10 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /Constant_10 [Constant] outputs: [/Constant_10_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Split_1 [Split]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Mul_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Constant_10_output_0
[10/31/2023-17:53:35] [V] [TRT] /Split_1 [Split] inputs: [/Mul_output_0 -> (1, 525987)[FLOAT]], [/Constant_10_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Split_1 for ONNX node: /Split_1
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Split_1_output_0 for ONNX tensor: /Split_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /Split_1 [Split] outputs: [/Split_1_output_0 -> (1, 525987)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Constant_11 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /Constant_11 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /Constant_11 [Constant] outputs: [/Constant_11_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Squeeze_1 [Squeeze]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Split_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Constant_11_output_0
[10/31/2023-17:53:35] [V] [TRT] /Squeeze_1 [Squeeze] inputs: [/Split_1_output_0 -> (1, 525987)[FLOAT]], [/Constant_11_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Original shape: (1, 525987), squeezing to: (525987,)
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Squeeze_1 for ONNX node: /Squeeze_1
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Squeeze_1_output_0 for ONNX tensor: /Squeeze_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /Squeeze_1 [Squeeze] outputs: [/Squeeze_1_output_0 -> (525987)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Constant_12 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /Constant_12 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /Constant_12 [Constant] outputs: [/Constant_12_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Split_2 [Split]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Slice_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Constant_12_output_0
[10/31/2023-17:53:35] [V] [TRT] /Split_2 [Split] inputs: [/Slice_1_output_0 -> (1, 525987, 16)[FLOAT]], [/Constant_12_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Split_2 for ONNX node: /Split_2
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Split_2_output_0 for ONNX tensor: /Split_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /Split_2 [Split] outputs: [/Split_2_output_0 -> (1, 525987, 16)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Constant_13 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /Constant_13 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /Constant_13 [Constant] outputs: [/Constant_13_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Squeeze_2 [Squeeze]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Split_2_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Constant_13_output_0
[10/31/2023-17:53:35] [V] [TRT] /Squeeze_2 [Squeeze] inputs: [/Split_2_output_0 -> (1, 525987, 16)[FLOAT]], [/Constant_13_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Original shape: (1, 525987, 16), squeezing to: (525987, 16)
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Squeeze_2 for ONNX node: /Squeeze_2
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Squeeze_2_output_0 for ONNX tensor: /Squeeze_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /Squeeze_2 [Squeeze] outputs: [/Squeeze_2_output_0 -> (525987, 16)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Constant_14 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /Constant_14 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /Constant_14 [Constant] outputs: [/Constant_14_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /TopK [TopK]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Squeeze_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Constant_14_output_0
[10/31/2023-17:53:35] [V] [TRT] /TopK [TopK] inputs: [/Squeeze_1_output_0 -> (525987)[FLOAT]], [/Constant_14_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Original shape: (525987,), unsqueezing to: (525987, 1)
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /TopK for ONNX node: /TopK
[10/31/2023-17:53:35] [V] [TRT] Original shape: (3840, 1), squeezing to: (3840,)
[10/31/2023-17:53:35] [V] [TRT] Original shape: (3840, 1), squeezing to: (3840,)
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /TopK_output_0 for ONNX tensor: /TopK_output_0
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /TopK_output_1 for ONNX tensor: /TopK_output_1
[10/31/2023-17:53:35] [V] [TRT] /TopK [TopK] outputs: [/TopK_output_0 -> (3840)[FLOAT]], [/TopK_output_1 -> (3840)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Gather_1 [Gather]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Squeeze_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /TopK_output_1
[10/31/2023-17:53:35] [V] [TRT] /Gather_1 [Gather] inputs: [/Squeeze_output_0 -> (525987, 4)[FLOAT]], [/TopK_output_1 -> (3840)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Using Gather axis: 0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Gather_1 for ONNX node: /Gather_1
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Gather_1_output_0 for ONNX tensor: /Gather_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /Gather_1 [Gather] outputs: [/Gather_1_output_0 -> (3840, 4)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Gather_2 [Gather]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Squeeze_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /TopK_output_1
[10/31/2023-17:53:35] [V] [TRT] /Gather_2 [Gather] inputs: [/Squeeze_1_output_0 -> (525987)[FLOAT]], [/TopK_output_1 -> (3840)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Using Gather axis: 0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Gather_2 for ONNX node: /Gather_2
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Gather_2_output_0 for ONNX tensor: /Gather_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /Gather_2 [Gather] outputs: [/Gather_2_output_0 -> (3840)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Gather_3 [Gather]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Squeeze_2_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /TopK_output_1
[10/31/2023-17:53:35] [V] [TRT] /Gather_3 [Gather] inputs: [/Squeeze_2_output_0 -> (525987, 16)[FLOAT]], [/TopK_output_1 -> (3840)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Using Gather axis: 0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Gather_3 for ONNX node: /Gather_3
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Gather_3_output_0 for ONNX tensor: /Gather_3_output_0
[10/31/2023-17:53:35] [V] [TRT] /Gather_3 [Gather] outputs: [/Gather_3_output_0 -> (3840, 16)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Constant_15 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /Constant_15 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /Constant_15 [Constant] outputs: [/Constant_15_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Unsqueeze [Unsqueeze]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Gather_2_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Constant_15_output_0
[10/31/2023-17:53:35] [V] [TRT] /Unsqueeze [Unsqueeze] inputs: [/Gather_2_output_0 -> (3840)[FLOAT]], [/Constant_15_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Original shape: (3840,), unsqueezing to: (1, 3840)
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Unsqueeze for ONNX node: /Unsqueeze
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Unsqueeze_output_0 for ONNX tensor: /Unsqueeze_output_0
[10/31/2023-17:53:35] [V] [TRT] /Unsqueeze [Unsqueeze] outputs: [/Unsqueeze_output_0 -> (1, 3840)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Constant_16 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /Constant_16 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /Constant_16 [Constant] outputs: [/Constant_16_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Unsqueeze_1 [Unsqueeze]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Unsqueeze_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Constant_16_output_0
[10/31/2023-17:53:35] [V] [TRT] /Unsqueeze_1 [Unsqueeze] inputs: [/Unsqueeze_output_0 -> (1, 3840)[FLOAT]], [/Constant_16_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Original shape: (1, 3840), unsqueezing to: (1, 1, 3840)
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Unsqueeze_1 for ONNX node: /Unsqueeze_1
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Unsqueeze_1_output_0 for ONNX tensor: /Unsqueeze_1_output_0
[10/31/2023-17:53:35] [V] [TRT] /Unsqueeze_1 [Unsqueeze] outputs: [/Unsqueeze_1_output_0 -> (1, 1, 3840)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Constant_17 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /Constant_17 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /Constant_17 [Constant] outputs: [/Constant_17_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Unsqueeze_2 [Unsqueeze]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Gather_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Constant_17_output_0
[10/31/2023-17:53:35] [V] [TRT] /Unsqueeze_2 [Unsqueeze] inputs: [/Gather_1_output_0 -> (3840, 4)[FLOAT]], [/Constant_17_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Original shape: (3840, 4), unsqueezing to: (1, 3840, 4)
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Unsqueeze_2 for ONNX node: /Unsqueeze_2
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Unsqueeze_2_output_0 for ONNX tensor: /Unsqueeze_2_output_0
[10/31/2023-17:53:35] [V] [TRT] /Unsqueeze_2 [Unsqueeze] outputs: [/Unsqueeze_2_output_0 -> (1, 3840, 4)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /NonMaxSuppression [NonMaxSuppression]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Unsqueeze_2_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Unsqueeze_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: onnx::NonMaxSuppression_1140
[10/31/2023-17:53:35] [V] [TRT] Searching for input: onnx::NonMaxSuppression_1141
[10/31/2023-17:53:35] [V] [TRT] Searching for input: onnx::NonMaxSuppression_1142
[10/31/2023-17:53:35] [V] [TRT] /NonMaxSuppression [NonMaxSuppression] inputs: [/Unsqueeze_2_output_0 -> (1, 3840, 4)[FLOAT]], [/Unsqueeze_1_output_0 -> (1, 1, 3840)[FLOAT]], [onnx::NonMaxSuppression_1140 -> (1)[INT32]], [onnx::NonMaxSuppression_1141 -> (1)[FLOAT]], [onnx::NonMaxSuppression_1142 -> (1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /NonMaxSuppression for ONNX node: /NonMaxSuppression
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /NonMaxSuppression_246 for ONNX node: /NonMaxSuppression
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /NonMaxSuppression_output_0 for ONNX tensor: /NonMaxSuppression_output_0
[10/31/2023-17:53:35] [V] [TRT] /NonMaxSuppression [NonMaxSuppression] outputs: [/NonMaxSuppression_output_0 -> (-1, 3)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Gather_4 [Gather]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /NonMaxSuppression_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /head/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] /Gather_4 [Gather] inputs: [/NonMaxSuppression_output_0 -> (-1, 3)[INT32]], [/head/Constant_output_0 -> ()[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /head/Constant_output_0 for ONNX node: /head/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Using Gather axis: 1
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Gather_4 for ONNX node: /Gather_4
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Gather_4_output_0 for ONNX tensor: /Gather_4_output_0
[10/31/2023-17:53:35] [V] [TRT] /Gather_4 [Gather] outputs: [/Gather_4_output_0 -> (-1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Constant_18 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /Constant_18 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /Constant_18 [Constant] outputs: [/Constant_18_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Squeeze_3 [Squeeze]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Unsqueeze_2_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Constant_18_output_0
[10/31/2023-17:53:35] [V] [TRT] /Squeeze_3 [Squeeze] inputs: [/Unsqueeze_2_output_0 -> (1, 3840, 4)[FLOAT]], [/Constant_18_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Original shape: (1, 3840, 4), squeezing to: (3840, 4)
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Squeeze_3 for ONNX node: /Squeeze_3
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Squeeze_3_output_0 for ONNX tensor: /Squeeze_3_output_0
[10/31/2023-17:53:35] [V] [TRT] /Squeeze_3 [Squeeze] outputs: [/Squeeze_3_output_0 -> (3840, 4)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Constant_19 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /Constant_19 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /Constant_19 [Constant] outputs: [/Constant_19_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Squeeze_4 [Squeeze]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Unsqueeze_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Constant_19_output_0
[10/31/2023-17:53:35] [V] [TRT] /Squeeze_4 [Squeeze] inputs: [/Unsqueeze_1_output_0 -> (1, 1, 3840)[FLOAT]], [/Constant_19_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Original shape: (1, 1, 3840), squeezing to: (1, 3840)
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Squeeze_4 for ONNX node: /Squeeze_4
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Squeeze_4_output_0 for ONNX tensor: /Squeeze_4_output_0
[10/31/2023-17:53:35] [V] [TRT] /Squeeze_4 [Squeeze] outputs: [/Squeeze_4_output_0 -> (1, 3840)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Constant_20 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /Constant_20 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /Constant_20 [Constant] outputs: [/Constant_20_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Squeeze_5 [Squeeze]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Squeeze_4_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Constant_20_output_0
[10/31/2023-17:53:35] [V] [TRT] /Squeeze_5 [Squeeze] inputs: [/Squeeze_4_output_0 -> (1, 3840)[FLOAT]], [/Constant_20_output_0 -> (1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Original shape: (1, 3840), squeezing to: (3840,)
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Squeeze_5 for ONNX node: /Squeeze_5
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Squeeze_5_output_0 for ONNX tensor: /Squeeze_5_output_0
[10/31/2023-17:53:35] [V] [TRT] /Squeeze_5 [Squeeze] outputs: [/Squeeze_5_output_0 -> (3840)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Cast [Cast]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Gather_4_output_0
[10/31/2023-17:53:35] [V] [TRT] /Cast [Cast] inputs: [/Gather_4_output_0 -> (-1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Casting to type: int32
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Cast for ONNX node: /Cast
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Cast_output_0 for ONNX tensor: /Cast_output_0
[10/31/2023-17:53:35] [V] [TRT] /Cast [Cast] outputs: [/Cast_output_0 -> (-1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Gather_5 [Gather]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Squeeze_3_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Cast_output_0
[10/31/2023-17:53:35] [V] [TRT] /Gather_5 [Gather] inputs: [/Squeeze_3_output_0 -> (3840, 4)[FLOAT]], [/Cast_output_0 -> (-1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Using Gather axis: 0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Gather_5 for ONNX node: /Gather_5
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Gather_5_output_0 for ONNX tensor: /Gather_5_output_0
[10/31/2023-17:53:35] [V] [TRT] /Gather_5 [Gather] outputs: [/Gather_5_output_0 -> (-1, 4)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Gather_6 [Gather]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Squeeze_5_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Cast_output_0
[10/31/2023-17:53:35] [V] [TRT] /Gather_6 [Gather] inputs: [/Squeeze_5_output_0 -> (3840)[FLOAT]], [/Cast_output_0 -> (-1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Using Gather axis: 0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Gather_6 for ONNX node: /Gather_6
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Gather_6_output_0 for ONNX tensor: /Gather_6_output_0
[10/31/2023-17:53:35] [V] [TRT] /Gather_6 [Gather] outputs: [/Gather_6_output_0 -> (-1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Gather_7 [Gather]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Gather_3_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Cast_output_0
[10/31/2023-17:53:35] [V] [TRT] /Gather_7 [Gather] inputs: [/Gather_3_output_0 -> (3840, 16)[FLOAT]], [/Cast_output_0 -> (-1)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Using Gather axis: 0
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Gather_7 for ONNX node: /Gather_7
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Gather_7_output_0 for ONNX tensor: /Gather_7_output_0
[10/31/2023-17:53:35] [V] [TRT] /Gather_7 [Gather] outputs: [/Gather_7_output_0 -> (-1, 16)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Constant_21 [Constant]
[10/31/2023-17:53:35] [V] [TRT] /Constant_21 [Constant] inputs: 
[10/31/2023-17:53:35] [V] [TRT] /Constant_21 [Constant] outputs: [/Constant_21_output_0 -> (2)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Reshape [Reshape]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Gather_6_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Constant_21_output_0
[10/31/2023-17:53:35] [V] [TRT] /Reshape [Reshape] inputs: [/Gather_6_output_0 -> (-1)[FLOAT]], [/Constant_21_output_0 -> (2)[INT32]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Reshape for ONNX node: /Reshape
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: /Reshape_output_0 for ONNX tensor: /Reshape_output_0
[10/31/2023-17:53:35] [V] [TRT] /Reshape [Reshape] outputs: [/Reshape_output_0 -> (-1, 1)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Parsing node: /Concat_1 [Concat]
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Gather_5_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Reshape_output_0
[10/31/2023-17:53:35] [V] [TRT] Searching for input: /Gather_7_output_0
[10/31/2023-17:53:35] [V] [TRT] /Concat_1 [Concat] inputs: [/Gather_5_output_0 -> (-1, 4)[FLOAT]], [/Reshape_output_0 -> (-1, 1)[FLOAT]], [/Gather_7_output_0 -> (-1, 16)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Registering layer: /Concat_1 for ONNX node: /Concat_1
[10/31/2023-17:53:35] [V] [TRT] Registering tensor: output_251 for ONNX tensor: output
[10/31/2023-17:53:35] [V] [TRT] /Concat_1 [Concat] outputs: [output -> (-1, 21)[FLOAT]], 
[10/31/2023-17:53:35] [V] [TRT] Marking output_251 as output: output
[10/31/2023-17:53:35] [I] Finish parsing network model
[10/31/2023-17:53:35] [I] FP32 and INT8 precisions have been specified - more performance might be enabled by additionally specifying --fp16 or --best
[10/31/2023-17:53:35] [V] [TRT] Original: 730 layers
[10/31/2023-17:53:35] [V] [TRT] After dead-layer removal: 730 layers
[10/31/2023-17:53:35] [V] [TRT] Applying generic optimizations to the graph for inference.
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on backbone.layer2.0.downsample.1.weight
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing backbone.layer2.0.downsample.1.weight with (Unnamed Layer* 105) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on backbone.layer2.0.downsample.1.running_mean
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing backbone.layer2.0.downsample.1.running_mean with (Unnamed Layer* 107) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on backbone.layer2.0.downsample.1.running_var
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing backbone.layer2.0.downsample.1.running_var with (Unnamed Layer* 108) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on backbone.layer3.0.downsample.1.weight
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing backbone.layer3.0.downsample.1.weight with (Unnamed Layer* 179) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on backbone.layer3.0.downsample.1.running_mean
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing backbone.layer3.0.downsample.1.running_mean with (Unnamed Layer* 181) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on backbone.layer3.0.downsample.1.running_var
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing backbone.layer3.0.downsample.1.running_var with (Unnamed Layer* 182) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on backbone.layer4.0.downsample.1.weight
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing backbone.layer4.0.downsample.1.weight with (Unnamed Layer* 253) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on backbone.layer4.0.downsample.1.running_mean
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing backbone.layer4.0.downsample.1.running_mean with (Unnamed Layer* 255) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on backbone.layer4.0.downsample.1.running_var
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing backbone.layer4.0.downsample.1.running_var with (Unnamed Layer* 256) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on /head/Constant_15_output_0
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing /head/Constant_15_output_0 with (Unnamed Layer* 577) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on /head/Constant_16_output_0
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing /head/Constant_16_output_0 with (Unnamed Layer* 580) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on /head/Constant_23_output_0
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing /head/Constant_23_output_0 with (Unnamed Layer* 594) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on /head/Constant_24_output_0
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing /head/Constant_24_output_0 with (Unnamed Layer* 598) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on /head/Constant_25_output_0
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing /head/Constant_25_output_0 with (Unnamed Layer* 601) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on /head/Constant_41_output_0
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing /head/Constant_41_output_0 with (Unnamed Layer* 619) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on /head/Constant_42_output_0
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing /head/Constant_42_output_0 with (Unnamed Layer* 622) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on /head/Constant_49_output_0
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing /head/Constant_49_output_0 with (Unnamed Layer* 636) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on /head/Constant_50_output_0
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing /head/Constant_50_output_0 with (Unnamed Layer* 640) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on /head/Constant_51_output_0
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing /head/Constant_51_output_0 with (Unnamed Layer* 643) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on /head/Constant_67_output_0
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing /head/Constant_67_output_0 with (Unnamed Layer* 661) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on /head/Constant_68_output_0
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing /head/Constant_68_output_0 with (Unnamed Layer* 664) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on /head/Constant_75_output_0
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing /head/Constant_75_output_0 with (Unnamed Layer* 678) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on /head/Constant_76_output_0
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing /head/Constant_76_output_0 with (Unnamed Layer* 682) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on /head/Constant_77_output_0
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing /head/Constant_77_output_0 with (Unnamed Layer* 685) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ShuffleShuffleFusion on /head/Transpose
[10/31/2023-17:53:35] [V] [TRT] ShuffleShuffleFusion: Fusing /head/Transpose with /head/Reshape
[10/31/2023-17:53:35] [V] [TRT] Running: ShuffleErasure on (Unnamed Layer* 574) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Removing (Unnamed Layer* 574) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ShuffleShuffleFusion on /head/Transpose_1
[10/31/2023-17:53:35] [V] [TRT] ShuffleShuffleFusion: Fusing /head/Transpose_1 with /head/Reshape_2
[10/31/2023-17:53:35] [V] [TRT] Running: ShuffleErasure on (Unnamed Layer* 616) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Removing (Unnamed Layer* 616) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ShuffleShuffleFusion on /head/Transpose_2
[10/31/2023-17:53:35] [V] [TRT] ShuffleShuffleFusion: Fusing /head/Transpose_2 with /head/Reshape_4
[10/31/2023-17:53:35] [V] [TRT] Running: ShuffleErasure on (Unnamed Layer* 658) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Removing (Unnamed Layer* 658) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ShuffleShuffleFusion on /Unsqueeze
[10/31/2023-17:53:35] [V] [TRT] ShuffleShuffleFusion: Fusing /Unsqueeze with /Unsqueeze_1
[10/31/2023-17:53:35] [V] [TRT] Running: ShuffleShuffleFusion on /Squeeze_4
[10/31/2023-17:53:35] [V] [TRT] ShuffleShuffleFusion: Fusing /Squeeze_4 with /Squeeze_5
[10/31/2023-17:53:35] [V] [TRT] QDQ graph optimizer - constant folding of Q/DQ initializers
[10/31/2023-17:53:35] [V] [TRT] Running: ConstCastFold on backbone.layer4.0.bn2.bias
[10/31/2023-17:53:35] [V] [TRT] ConstCastFold: Fusing backbone.layer4.0.bn2.bias with Identity_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstCastFold on backbone.layer3.0.bn2.bias
[10/31/2023-17:53:35] [V] [TRT] ConstCastFold: Fusing backbone.layer3.0.bn2.bias with Identity_1
[10/31/2023-17:53:35] [V] [TRT] Running: ConstCastFold on backbone.layer2.0.bn2.bias
[10/31/2023-17:53:35] [V] [TRT] ConstCastFold: Fusing backbone.layer2.0.bn2.bias with Identity_2
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer1/layer1.1/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer2/layer2.1/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer3/layer3.1/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer4/layer4.1/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv7/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/conv1/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv7/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/conv2/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv7/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/conv1/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer1/layer1.0/conv1/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer1/layer1.0/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer1/layer1.0/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer1/layer1.1/conv1/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer1/layer1.1/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer1/layer1.1/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer2/layer2.0/conv1/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer2/layer2.0/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer2/layer2.0/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer2/layer2.1/conv1/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer2/layer2.1/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer2/layer2.1/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer3/layer3.0/conv1/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer3/layer3.0/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer3/layer3.0/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer3/layer3.1/conv1/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer3/layer3.1/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer3/layer3.1/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer4/layer4.0/conv1/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer4/layer4.0/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer4/layer4.0/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer4/layer4.1/conv1/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer4/layer4.1/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer4/layer4.1/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv1/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv1/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv1/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv3/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv3/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv3/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv5/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv5/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv5/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv7/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv7/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv7/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv1/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv1/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv1/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv3/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv3/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv3/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv5/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv5/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv5/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv7/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv7/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv7/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv1/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv1/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv1/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv3/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv3/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv3/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv5/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv5/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv5/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv7/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv7/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv7/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer1/layer1.0/conv1/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer1/layer1.0/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer1/layer1.0/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer1/layer1.0/conv2/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer1/layer1.0/conv2/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer1/layer1.0/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer1/layer1.0/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer1/layer1.1/conv1/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer1/layer1.1/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer1/layer1.1/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer1/layer1.1/conv2/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer1/layer1.1/conv2/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer1/layer1.1/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer1/layer1.1/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer2/layer2.0/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer2/layer2.0/conv1/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer2/layer2.0/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer2/layer2.0/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer2/layer2.0/conv2/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer2/layer2.0/conv2/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer2/layer2.0/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer2/layer2.0/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer2/layer2.1/conv1/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer2/layer2.1/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer2/layer2.1/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer2/layer2.1/conv2/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer2/layer2.1/conv2/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer2/layer2.1/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer2/layer2.1/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer3/layer3.0/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer3/layer3.0/conv1/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer3/layer3.0/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer3/layer3.0/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer3/layer3.0/conv2/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer3/layer3.0/conv2/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer3/layer3.0/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer3/layer3.0/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer3/layer3.1/conv1/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer3/layer3.1/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer3/layer3.1/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer3/layer3.1/conv2/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer3/layer3.1/conv2/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer3/layer3.1/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer3/layer3.1/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer4/layer4.0/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer4/layer4.0/conv1/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer4/layer4.0/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer4/layer4.0/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer4/layer4.0/conv2/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer4/layer4.0/conv2/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer4/layer4.0/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer4/layer4.0/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer4/layer4.1/conv1/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer4/layer4.1/conv1/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer4/layer4.1/conv1/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer4/layer4.1/conv2/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer4/layer4.1/conv2/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer4/layer4.1/conv2/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer4/layer4.1/conv2/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv1/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv1/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv1/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv1/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv2/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv2/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv2/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv2/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv3/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv3/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv3/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv3/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv4/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv4/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv4/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv4/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv5/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv5/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv5/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv5/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv6/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/conv1/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv6/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv6/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv6/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv7/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv7/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv7/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv7/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv1/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv1/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv1/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv2/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv2/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv2/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv2/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv3/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv3/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv3/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv3/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv4/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv4/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv4/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv4/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv5/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv5/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv5/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv5/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv6/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/conv2/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv6/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv6/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv6/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv7/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv7/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv7/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv7/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv1/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv1/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv1/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv2/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv2/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv2/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv2/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv3/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv3/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv3/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv3/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv4/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv4/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv4/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv4/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv5/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv5/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv5/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv5/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv6/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv6/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv6/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv6/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv7/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv7/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv7/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv7/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/conv1/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/conv1/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/conv1/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer1/layer1.0/conv2/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer1/layer1.0/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer1/layer1.0/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer1/layer1.1/conv2/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer1/layer1.1/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer1/layer1.1/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer2/layer2.0/conv2/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer2/layer2.0/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer2/layer2.0/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer2/layer2.1/conv2/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer2/layer2.1/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer2/layer2.1/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer3/layer3.0/conv2/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer3/layer3.0/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer3/layer3.0/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer3/layer3.1/conv2/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer3/layer3.1/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer3/layer3.1/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer4/layer4.0/conv2/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer4/layer4.0/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer4/layer4.0/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer4/layer4.1/conv2/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer4/layer4.1/conv2/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer4/layer4.1/conv2/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv2/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv2/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv2/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv4/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv4/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv4/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect1/conv6/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv6/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect1/conv6/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/conv1/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/conv1/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/conv1/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv2/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv2/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv2/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv4/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv4/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv4/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect2/conv6/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv6/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect2/conv6/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/conv2/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/conv2/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/conv2/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv2/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv2/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv2/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv4/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv4/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv4/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/detect3/conv6/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv6/conv/_weight_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/detect3/conv6/conv/_weight_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/conv1/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/conv1/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/conv1/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] Running: ConstQDQInitializersFusion on /neck/conv2/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/conv2/conv/_input_quantizer/Constant_output_0
[10/31/2023-17:53:35] [V] [TRT] Removing /neck/conv2/conv/_input_quantizer/Constant_1_output_0
[10/31/2023-17:53:35] [V] [TRT] After Myelin optimization: 388 layers
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on backbone.layer2.0.bn2.bias + Identity_2
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing backbone.layer2.0.bn2.bias + Identity_2 with (Unnamed Layer* 106) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on backbone.layer3.0.bn2.bias + Identity_1
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing backbone.layer3.0.bn2.bias + Identity_1 with (Unnamed Layer* 180) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] Running: ConstShuffleFusion on backbone.layer4.0.bn2.bias + Identity_0
[10/31/2023-17:53:35] [V] [TRT] ConstShuffleFusion: Fusing backbone.layer4.0.bn2.bias + Identity_0 with (Unnamed Layer* 254) [Shuffle]
[10/31/2023-17:53:35] [V] [TRT] QDQ graph optimizer - constant folding of Q/DQ initializers
[10/31/2023-17:53:36] [V] [TRT] QDQ graph optimizer forward pass - DQ motions and fusions
[10/31/2023-17:53:36] [V] [TRT] QDQ graph optimizer backward pass
[10/31/2023-17:53:36] [V] [TRT] QDQ graph optimizer quantization pass - Generate quantized ops
[10/31/2023-17:53:36] [V] [TRT] Running: EltReluFusion on /backbone/layer1/layer1.0/Add
[10/31/2023-17:53:36] [V] [TRT] EltReluFusion: Fusing /backbone/layer1/layer1.0/Add with /backbone/layer1/layer1.0/relu_1/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: EltReluFusion on /backbone/layer1/layer1.1/Add
[10/31/2023-17:53:36] [V] [TRT] EltReluFusion: Fusing /backbone/layer1/layer1.1/Add with /backbone/layer1/layer1.1/relu_1/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: EltReluFusion on /backbone/layer2/layer2.0/Add
[10/31/2023-17:53:36] [V] [TRT] EltReluFusion: Fusing /backbone/layer2/layer2.0/Add with /backbone/layer2/layer2.0/relu_1/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: EltReluFusion on /backbone/layer2/layer2.1/Add
[10/31/2023-17:53:36] [V] [TRT] EltReluFusion: Fusing /backbone/layer2/layer2.1/Add with /backbone/layer2/layer2.1/relu_1/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: EltReluFusion on /backbone/layer3/layer3.0/Add
[10/31/2023-17:53:36] [V] [TRT] EltReluFusion: Fusing /backbone/layer3/layer3.0/Add with /backbone/layer3/layer3.0/relu_1/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: EltReluFusion on /backbone/layer3/layer3.1/Add
[10/31/2023-17:53:36] [V] [TRT] EltReluFusion: Fusing /backbone/layer3/layer3.1/Add with /backbone/layer3/layer3.1/relu_1/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: EltReluFusion on /backbone/layer4/layer4.0/Add
[10/31/2023-17:53:36] [V] [TRT] EltReluFusion: Fusing /backbone/layer4/layer4.0/Add with /backbone/layer4/layer4.0/relu_1/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: EltReluFusion on /backbone/layer4/layer4.1/Add
[10/31/2023-17:53:36] [V] [TRT] EltReluFusion: Fusing /backbone/layer4/layer4.1/Add with /backbone/layer4/layer4.1/relu_1/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: ScaleActivationFusion on /backbone/bn1/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] ScaleActivationFusion: Fusing /backbone/bn1/BatchNormalization with /backbone/relu/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: ScaleActivationFusion on /backbone/layer1/layer1.0/bn1/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] ScaleActivationFusion: Fusing /backbone/layer1/layer1.0/bn1/BatchNormalization with /backbone/layer1/layer1.0/relu/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: ScaleActivationFusion on /backbone/layer1/layer1.1/bn1/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] ScaleActivationFusion: Fusing /backbone/layer1/layer1.1/bn1/BatchNormalization with /backbone/layer1/layer1.1/relu/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: ScaleActivationFusion on /backbone/layer2/layer2.0/bn1/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] ScaleActivationFusion: Fusing /backbone/layer2/layer2.0/bn1/BatchNormalization with /backbone/layer2/layer2.0/relu/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: ScaleActivationFusion on /backbone/layer2/layer2.1/bn1/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] ScaleActivationFusion: Fusing /backbone/layer2/layer2.1/bn1/BatchNormalization with /backbone/layer2/layer2.1/relu/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: ScaleActivationFusion on /backbone/layer3/layer3.0/bn1/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] ScaleActivationFusion: Fusing /backbone/layer3/layer3.0/bn1/BatchNormalization with /backbone/layer3/layer3.0/relu/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: ScaleActivationFusion on /backbone/layer3/layer3.1/bn1/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] ScaleActivationFusion: Fusing /backbone/layer3/layer3.1/bn1/BatchNormalization with /backbone/layer3/layer3.1/relu/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: ScaleActivationFusion on /backbone/layer4/layer4.0/bn1/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] ScaleActivationFusion: Fusing /backbone/layer4/layer4.0/bn1/BatchNormalization with /backbone/layer4/layer4.0/relu/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: ScaleActivationFusion on /backbone/layer4/layer4.1/bn1/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] ScaleActivationFusion: Fusing /backbone/layer4/layer4.1/bn1/BatchNormalization with /backbone/layer4/layer4.1/relu/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on backbone.conv1.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing backbone.conv1.weight with /backbone/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on backbone.layer1.0.conv1.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing backbone.layer1.0.conv1.weight with /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on backbone.layer1.0.conv2.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing backbone.layer1.0.conv2.weight with /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on backbone.layer1.1.conv1.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing backbone.layer1.1.conv1.weight with /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on backbone.layer1.1.conv2.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing backbone.layer1.1.conv2.weight with /backbone/layer1/layer1.1/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on backbone.layer2.0.conv1.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing backbone.layer2.0.conv1.weight with /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on backbone.layer2.0.conv2.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing backbone.layer2.0.conv2.weight with /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on backbone.layer2.0.downsample.0.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing backbone.layer2.0.downsample.0.weight with /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on backbone.layer2.1.conv1.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing backbone.layer2.1.conv1.weight with /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on backbone.layer2.1.conv2.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing backbone.layer2.1.conv2.weight with /backbone/layer2/layer2.1/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on backbone.layer3.0.conv1.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing backbone.layer3.0.conv1.weight with /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on backbone.layer3.0.conv2.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing backbone.layer3.0.conv2.weight with /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on backbone.layer3.0.downsample.0.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing backbone.layer3.0.downsample.0.weight with /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on backbone.layer3.1.conv1.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing backbone.layer3.1.conv1.weight with /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on backbone.layer3.1.conv2.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing backbone.layer3.1.conv2.weight with /backbone/layer3/layer3.1/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on backbone.layer4.0.conv1.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing backbone.layer4.0.conv1.weight with /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on backbone.layer4.0.conv2.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing backbone.layer4.0.conv2.weight with /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on backbone.layer4.0.downsample.0.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing backbone.layer4.0.downsample.0.weight with /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on backbone.layer4.1.conv1.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing backbone.layer4.1.conv1.weight with /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on backbone.layer4.1.conv2.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing backbone.layer4.1.conv2.weight with /backbone/layer4/layer4.1/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.detect1.conv1.conv.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.detect1.conv1.conv.weight with /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.detect1.conv2.conv.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.detect1.conv2.conv.weight with /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.detect1.conv3.conv.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.detect1.conv3.conv.weight with /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.detect1.conv4.conv.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.detect1.conv4.conv.weight with /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.detect1.conv5.conv.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.detect1.conv5.conv.weight with /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.detect1.conv6.conv.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.detect1.conv6.conv.weight with /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.detect1.conv7.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.detect1.conv7.weight with /neck/detect1/conv7/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.conv1.conv.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.conv1.conv.weight with /neck/conv1/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.detect2.conv1.conv.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.detect2.conv1.conv.weight with /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.detect2.conv2.conv.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.detect2.conv2.conv.weight with /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.detect2.conv3.conv.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.detect2.conv3.conv.weight with /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.detect2.conv4.conv.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.detect2.conv4.conv.weight with /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.detect2.conv5.conv.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.detect2.conv5.conv.weight with /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.detect2.conv6.conv.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.detect2.conv6.conv.weight with /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.detect2.conv7.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.detect2.conv7.weight with /neck/detect2/conv7/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.conv2.conv.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.conv2.conv.weight with /neck/conv2/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.detect3.conv1.conv.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.detect3.conv1.conv.weight with /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.detect3.conv2.conv.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.detect3.conv2.conv.weight with /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.detect3.conv3.conv.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.detect3.conv3.conv.weight with /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.detect3.conv4.conv.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.detect3.conv4.conv.weight with /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.detect3.conv5.conv.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.detect3.conv5.conv.weight with /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.detect3.conv6.conv.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.detect3.conv6.conv.weight with /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsQuantizeFusion on neck.detect3.conv7.weight
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsQuantizeFusion: Fusing neck.detect3.conv7.weight with /neck/detect3/conv7/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstEltFusion on backbone.layer2.0.bn2.bias + Identity_2 + (Unnamed Layer* 106) [Shuffle]
[10/31/2023-17:53:36] [V] [TRT] ConstEltFusion: Fusing backbone.layer2.0.bn2.bias + Identity_2 + (Unnamed Layer* 106) [Shuffle] with /backbone/layer2/layer2.0/downsample/downsample.1/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: ConstEltFusion on backbone.layer2.0.downsample.1.weight + (Unnamed Layer* 105) [Shuffle]
[10/31/2023-17:53:36] [V] [TRT] ConstEltFusion: Fusing backbone.layer2.0.downsample.1.weight + (Unnamed Layer* 105) [Shuffle] with (Unnamed Layer* 114) [ElementWise]
[10/31/2023-17:53:36] [V] [TRT] Running: ConstEltFusion on (Unnamed Layer* 109) [Constant]
[10/31/2023-17:53:36] [V] [TRT] ConstEltFusion: Fusing (Unnamed Layer* 109) [Constant] with (Unnamed Layer* 110) [ElementWise]
[10/31/2023-17:53:36] [V] [TRT] Running: ConstEltFusion on backbone.layer3.0.bn2.bias + Identity_1 + (Unnamed Layer* 180) [Shuffle]
[10/31/2023-17:53:36] [V] [TRT] ConstEltFusion: Fusing backbone.layer3.0.bn2.bias + Identity_1 + (Unnamed Layer* 180) [Shuffle] with /backbone/layer3/layer3.0/downsample/downsample.1/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: ConstEltFusion on backbone.layer3.0.downsample.1.weight + (Unnamed Layer* 179) [Shuffle]
[10/31/2023-17:53:36] [V] [TRT] ConstEltFusion: Fusing backbone.layer3.0.downsample.1.weight + (Unnamed Layer* 179) [Shuffle] with (Unnamed Layer* 188) [ElementWise]
[10/31/2023-17:53:36] [V] [TRT] Running: ConstEltFusion on (Unnamed Layer* 183) [Constant]
[10/31/2023-17:53:36] [V] [TRT] ConstEltFusion: Fusing (Unnamed Layer* 183) [Constant] with (Unnamed Layer* 184) [ElementWise]
[10/31/2023-17:53:36] [V] [TRT] Running: ConstEltFusion on backbone.layer4.0.bn2.bias + Identity_0 + (Unnamed Layer* 254) [Shuffle]
[10/31/2023-17:53:36] [V] [TRT] ConstEltFusion: Fusing backbone.layer4.0.bn2.bias + Identity_0 + (Unnamed Layer* 254) [Shuffle] with /backbone/layer4/layer4.0/downsample/downsample.1/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: ConstEltFusion on backbone.layer4.0.downsample.1.weight + (Unnamed Layer* 253) [Shuffle]
[10/31/2023-17:53:36] [V] [TRT] ConstEltFusion: Fusing backbone.layer4.0.downsample.1.weight + (Unnamed Layer* 253) [Shuffle] with (Unnamed Layer* 262) [ElementWise]
[10/31/2023-17:53:36] [V] [TRT] Running: ConstEltFusion on (Unnamed Layer* 257) [Constant]
[10/31/2023-17:53:36] [V] [TRT] ConstEltFusion: Fusing (Unnamed Layer* 257) [Constant] with (Unnamed Layer* 258) [ElementWise]
[10/31/2023-17:53:36] [V] [TRT] Running: HorizontalMergeQNodes on /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Eliminating /backbone/layer2/layer2.0/conv1/_input_quantizer/QuantizeLinear which duplicates (Q) /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer2/layer2.0/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: HorizontalMergeQNodes on /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Eliminating /backbone/layer3/layer3.0/conv1/_input_quantizer/QuantizeLinear which duplicates (Q) /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer3/layer3.0/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: HorizontalMergeQNodes on /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Eliminating /backbone/layer4/layer4.0/conv1/_input_quantizer/QuantizeLinear which duplicates (Q) /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer4/layer4.0/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: HorizontalMergeQNodes on /neck/conv1/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Eliminating /neck/detect1/conv6/conv/_input_quantizer/QuantizeLinear which duplicates (Q) /neck/conv1/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv6/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: HorizontalMergeQNodes on /neck/conv2/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Eliminating /neck/detect2/conv6/conv/_input_quantizer/QuantizeLinear which duplicates (Q) /neck/conv2/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv6/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: SplitQAcrossPrecedingFanIn on /neck/Concat
[10/31/2023-17:53:36] [V] [TRT] Running: SplitQAcrossPrecedingFanIn on /neck/Concat_1
[10/31/2023-17:53:36] [V] [TRT] Running: VanillaSwapWithFollowingQ on /neck/Resize
[10/31/2023-17:53:36] [V] [TRT] Swapping /neck/Resize with /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_clone_0
[10/31/2023-17:53:36] [V] [TRT] Running: VanillaSwapWithFollowingQ on /neck/Resize_1
[10/31/2023-17:53:36] [V] [TRT] Swapping /neck/Resize_1 with /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_clone_0
[10/31/2023-17:53:36] [V] [TRT] Running: PointWiseFusion on (Unnamed Layer* 109) [Constant] + (Unnamed Layer* 110) [ElementWise]
[10/31/2023-17:53:36] [V] [TRT] PointWiseFusion: Fusing (Unnamed Layer* 109) [Constant] + (Unnamed Layer* 110) [ElementWise] with (Unnamed Layer* 111) [Unary]
[10/31/2023-17:53:36] [V] [TRT] Running: PointWiseFusion on PWN((Unnamed Layer* 109) [Constant] + (Unnamed Layer* 110) [ElementWise], (Unnamed Layer* 111) [Unary])
[10/31/2023-17:53:36] [V] [TRT] PointWiseFusion: Fusing PWN((Unnamed Layer* 109) [Constant] + (Unnamed Layer* 110) [ElementWise], (Unnamed Layer* 111) [Unary]) with (Unnamed Layer* 113) [ElementWise]
[10/31/2023-17:53:36] [V] [TRT] Running: PointWiseFusion on (Unnamed Layer* 112) [ElementWise]
[10/31/2023-17:53:36] [V] [TRT] PointWiseFusion: Fusing (Unnamed Layer* 112) [ElementWise] with PWN(PWN((Unnamed Layer* 109) [Constant] + (Unnamed Layer* 110) [ElementWise], (Unnamed Layer* 111) [Unary]), (Unnamed Layer* 113) [ElementWise])
[10/31/2023-17:53:36] [V] [TRT] Running: PointWiseFusion on (Unnamed Layer* 183) [Constant] + (Unnamed Layer* 184) [ElementWise]
[10/31/2023-17:53:36] [V] [TRT] PointWiseFusion: Fusing (Unnamed Layer* 183) [Constant] + (Unnamed Layer* 184) [ElementWise] with (Unnamed Layer* 185) [Unary]
[10/31/2023-17:53:36] [V] [TRT] Running: PointWiseFusion on PWN((Unnamed Layer* 183) [Constant] + (Unnamed Layer* 184) [ElementWise], (Unnamed Layer* 185) [Unary])
[10/31/2023-17:53:36] [V] [TRT] PointWiseFusion: Fusing PWN((Unnamed Layer* 183) [Constant] + (Unnamed Layer* 184) [ElementWise], (Unnamed Layer* 185) [Unary]) with (Unnamed Layer* 187) [ElementWise]
[10/31/2023-17:53:36] [V] [TRT] Running: PointWiseFusion on (Unnamed Layer* 186) [ElementWise]
[10/31/2023-17:53:36] [V] [TRT] PointWiseFusion: Fusing (Unnamed Layer* 186) [ElementWise] with PWN(PWN((Unnamed Layer* 183) [Constant] + (Unnamed Layer* 184) [ElementWise], (Unnamed Layer* 185) [Unary]), (Unnamed Layer* 187) [ElementWise])
[10/31/2023-17:53:36] [V] [TRT] Running: PointWiseFusion on (Unnamed Layer* 257) [Constant] + (Unnamed Layer* 258) [ElementWise]
[10/31/2023-17:53:36] [V] [TRT] PointWiseFusion: Fusing (Unnamed Layer* 257) [Constant] + (Unnamed Layer* 258) [ElementWise] with (Unnamed Layer* 259) [Unary]
[10/31/2023-17:53:36] [V] [TRT] Running: PointWiseFusion on PWN((Unnamed Layer* 257) [Constant] + (Unnamed Layer* 258) [ElementWise], (Unnamed Layer* 259) [Unary])
[10/31/2023-17:53:36] [V] [TRT] PointWiseFusion: Fusing PWN((Unnamed Layer* 257) [Constant] + (Unnamed Layer* 258) [ElementWise], (Unnamed Layer* 259) [Unary]) with (Unnamed Layer* 261) [ElementWise]
[10/31/2023-17:53:36] [V] [TRT] Running: PointWiseFusion on (Unnamed Layer* 260) [ElementWise]
[10/31/2023-17:53:36] [V] [TRT] PointWiseFusion: Fusing (Unnamed Layer* 260) [ElementWise] with PWN(PWN((Unnamed Layer* 257) [Constant] + (Unnamed Layer* 258) [ElementWise], (Unnamed Layer* 259) [Unary]), (Unnamed Layer* 261) [ElementWise])
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /backbone/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/bn1/BatchNormalization + /backbone/relu/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /backbone/layer1/layer1.0/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer1/layer1.0/bn1/BatchNormalization + /backbone/layer1/layer1.0/relu/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /backbone/layer1/layer1.0/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer1/layer1.0/bn2/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /backbone/layer1/layer1.1/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer1/layer1.1/bn1/BatchNormalization + /backbone/layer1/layer1.1/relu/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /backbone/layer1/layer1.1/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer1/layer1.1/bn2/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /backbone/layer2/layer2.0/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer2/layer2.0/bn1/BatchNormalization + /backbone/layer2/layer2.0/relu/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /backbone/layer2/layer2.0/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer2/layer2.0/bn2/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /backbone/layer2/layer2.1/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer2/layer2.1/bn1/BatchNormalization + /backbone/layer2/layer2.1/relu/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /backbone/layer2/layer2.1/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer2/layer2.1/bn2/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /backbone/layer3/layer3.0/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer3/layer3.0/bn1/BatchNormalization + /backbone/layer3/layer3.0/relu/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /backbone/layer3/layer3.0/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer3/layer3.0/bn2/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /backbone/layer3/layer3.1/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer3/layer3.1/bn1/BatchNormalization + /backbone/layer3/layer3.1/relu/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /backbone/layer3/layer3.1/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer3/layer3.1/bn2/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /backbone/layer4/layer4.0/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer4/layer4.0/bn1/BatchNormalization + /backbone/layer4/layer4.0/relu/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /backbone/layer4/layer4.0/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer4/layer4.0/bn2/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /backbone/layer4/layer4.1/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer4/layer4.1/bn1/BatchNormalization + /backbone/layer4/layer4.1/relu/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /backbone/layer4/layer4.1/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer4/layer4.1/bn2/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /neck/detect1/conv1/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv1/bn/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /neck/detect1/conv2/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv2/bn/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /neck/detect1/conv3/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv3/bn/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /neck/detect1/conv4/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv4/bn/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /neck/detect1/conv5/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv5/bn/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /neck/conv1/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/conv1/bn/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /neck/detect1/conv6/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv6/bn/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /neck/detect2/conv1/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv1/bn/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /neck/detect2/conv2/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv2/bn/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /neck/detect2/conv3/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv3/bn/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /neck/detect2/conv4/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv4/bn/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /neck/detect2/conv5/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv5/bn/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /neck/conv2/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/conv2/bn/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /neck/detect2/conv6/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv6/bn/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /neck/detect3/conv1/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv1/bn/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /neck/detect3/conv2/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv2/bn/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /neck/detect3/conv3/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv3/bn/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /neck/detect3/conv4/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv4/bn/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /neck/detect3/conv5/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv5/bn/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: QConvScaleFusion on /neck/detect3/conv6/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv6/bn/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: ActivationToPointwiseConversion on /neck/detect1/conv1/relu/LeakyRelu
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/detect1/conv1/relu/LeakyRelu from ACTIVATION to POINTWISE
[10/31/2023-17:53:36] [V] [TRT] Running: ActivationToPointwiseConversion on /neck/detect1/conv2/relu/LeakyRelu
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/detect1/conv2/relu/LeakyRelu from ACTIVATION to POINTWISE
[10/31/2023-17:53:36] [V] [TRT] Running: ActivationToPointwiseConversion on /neck/detect1/conv3/relu/LeakyRelu
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/detect1/conv3/relu/LeakyRelu from ACTIVATION to POINTWISE
[10/31/2023-17:53:36] [V] [TRT] Running: ActivationToPointwiseConversion on /neck/detect1/conv4/relu/LeakyRelu
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/detect1/conv4/relu/LeakyRelu from ACTIVATION to POINTWISE
[10/31/2023-17:53:36] [V] [TRT] Running: ActivationToPointwiseConversion on /neck/detect1/conv5/relu/LeakyRelu
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/detect1/conv5/relu/LeakyRelu from ACTIVATION to POINTWISE
[10/31/2023-17:53:36] [V] [TRT] Running: ActivationToPointwiseConversion on /neck/detect1/conv6/relu/LeakyRelu
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/detect1/conv6/relu/LeakyRelu from ACTIVATION to POINTWISE
[10/31/2023-17:53:36] [V] [TRT] Running: ActivationToPointwiseConversion on /neck/conv1/relu/LeakyRelu
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/conv1/relu/LeakyRelu from ACTIVATION to POINTWISE
[10/31/2023-17:53:36] [V] [TRT] Running: ActivationToPointwiseConversion on /neck/detect2/conv1/relu/LeakyRelu
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/detect2/conv1/relu/LeakyRelu from ACTIVATION to POINTWISE
[10/31/2023-17:53:36] [V] [TRT] Running: ActivationToPointwiseConversion on /neck/detect2/conv2/relu/LeakyRelu
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/detect2/conv2/relu/LeakyRelu from ACTIVATION to POINTWISE
[10/31/2023-17:53:36] [V] [TRT] Running: ActivationToPointwiseConversion on /neck/detect2/conv3/relu/LeakyRelu
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/detect2/conv3/relu/LeakyRelu from ACTIVATION to POINTWISE
[10/31/2023-17:53:36] [V] [TRT] Running: ActivationToPointwiseConversion on /neck/detect2/conv4/relu/LeakyRelu
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/detect2/conv4/relu/LeakyRelu from ACTIVATION to POINTWISE
[10/31/2023-17:53:36] [V] [TRT] Running: ActivationToPointwiseConversion on /neck/detect2/conv5/relu/LeakyRelu
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/detect2/conv5/relu/LeakyRelu from ACTIVATION to POINTWISE
[10/31/2023-17:53:36] [V] [TRT] Running: ActivationToPointwiseConversion on /neck/detect2/conv6/relu/LeakyRelu
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/detect2/conv6/relu/LeakyRelu from ACTIVATION to POINTWISE
[10/31/2023-17:53:36] [V] [TRT] Running: ActivationToPointwiseConversion on /neck/conv2/relu/LeakyRelu
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/conv2/relu/LeakyRelu from ACTIVATION to POINTWISE
[10/31/2023-17:53:36] [V] [TRT] Running: ActivationToPointwiseConversion on /neck/detect3/conv1/relu/LeakyRelu
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/detect3/conv1/relu/LeakyRelu from ACTIVATION to POINTWISE
[10/31/2023-17:53:36] [V] [TRT] Running: ActivationToPointwiseConversion on /neck/detect3/conv2/relu/LeakyRelu
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/detect3/conv2/relu/LeakyRelu from ACTIVATION to POINTWISE
[10/31/2023-17:53:36] [V] [TRT] Running: ActivationToPointwiseConversion on /neck/detect3/conv3/relu/LeakyRelu
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/detect3/conv3/relu/LeakyRelu from ACTIVATION to POINTWISE
[10/31/2023-17:53:36] [V] [TRT] Running: ActivationToPointwiseConversion on /neck/detect3/conv4/relu/LeakyRelu
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/detect3/conv4/relu/LeakyRelu from ACTIVATION to POINTWISE
[10/31/2023-17:53:36] [V] [TRT] Running: ActivationToPointwiseConversion on /neck/detect3/conv5/relu/LeakyRelu
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/detect3/conv5/relu/LeakyRelu from ACTIVATION to POINTWISE
[10/31/2023-17:53:36] [V] [TRT] Running: ActivationToPointwiseConversion on /neck/detect3/conv6/relu/LeakyRelu
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/detect3/conv6/relu/LeakyRelu from ACTIVATION to POINTWISE
[10/31/2023-17:53:36] [V] [TRT] Running: GenericConvActFusion on /neck/detect1/conv1/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] GenericConvActFusion: Fusing /neck/detect1/conv1/conv/Conv with PWN(/neck/detect1/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: GenericConvActFusion on /neck/detect1/conv2/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] GenericConvActFusion: Fusing /neck/detect1/conv2/conv/Conv with PWN(/neck/detect1/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: GenericConvActFusion on /neck/detect1/conv3/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] GenericConvActFusion: Fusing /neck/detect1/conv3/conv/Conv with PWN(/neck/detect1/conv3/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: GenericConvActFusion on /neck/detect1/conv4/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] GenericConvActFusion: Fusing /neck/detect1/conv4/conv/Conv with PWN(/neck/detect1/conv4/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: GenericConvActFusion on /neck/detect1/conv5/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] GenericConvActFusion: Fusing /neck/detect1/conv5/conv/Conv with PWN(/neck/detect1/conv5/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: GenericConvActFusion on /neck/conv1/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] GenericConvActFusion: Fusing /neck/conv1/conv/Conv with PWN(/neck/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: GenericConvActFusion on /neck/detect1/conv6/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] GenericConvActFusion: Fusing /neck/detect1/conv6/conv/Conv with PWN(/neck/detect1/conv6/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: GenericConvActFusion on /neck/detect2/conv1/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] GenericConvActFusion: Fusing /neck/detect2/conv1/conv/Conv with PWN(/neck/detect2/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: GenericConvActFusion on /neck/detect2/conv2/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] GenericConvActFusion: Fusing /neck/detect2/conv2/conv/Conv with PWN(/neck/detect2/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: GenericConvActFusion on /neck/detect2/conv3/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] GenericConvActFusion: Fusing /neck/detect2/conv3/conv/Conv with PWN(/neck/detect2/conv3/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: GenericConvActFusion on /neck/detect2/conv4/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] GenericConvActFusion: Fusing /neck/detect2/conv4/conv/Conv with PWN(/neck/detect2/conv4/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: GenericConvActFusion on /neck/detect2/conv5/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] GenericConvActFusion: Fusing /neck/detect2/conv5/conv/Conv with PWN(/neck/detect2/conv5/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: GenericConvActFusion on /neck/conv2/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] GenericConvActFusion: Fusing /neck/conv2/conv/Conv with PWN(/neck/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: GenericConvActFusion on /neck/detect2/conv6/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] GenericConvActFusion: Fusing /neck/detect2/conv6/conv/Conv with PWN(/neck/detect2/conv6/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: GenericConvActFusion on /neck/detect3/conv1/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] GenericConvActFusion: Fusing /neck/detect3/conv1/conv/Conv with PWN(/neck/detect3/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: GenericConvActFusion on /neck/detect3/conv2/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] GenericConvActFusion: Fusing /neck/detect3/conv2/conv/Conv with PWN(/neck/detect3/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: GenericConvActFusion on /neck/detect3/conv3/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] GenericConvActFusion: Fusing /neck/detect3/conv3/conv/Conv with PWN(/neck/detect3/conv3/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: GenericConvActFusion on /neck/detect3/conv4/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] GenericConvActFusion: Fusing /neck/detect3/conv4/conv/Conv with PWN(/neck/detect3/conv4/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: GenericConvActFusion on /neck/detect3/conv5/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] GenericConvActFusion: Fusing /neck/detect3/conv5/conv/Conv with PWN(/neck/detect3/conv5/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: GenericConvActFusion on /neck/detect3/conv6/conv/Conv
[10/31/2023-17:53:36] [V] [TRT] GenericConvActFusion: Fusing /neck/detect3/conv6/conv/Conv with PWN(/neck/detect3/conv6/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /backbone/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/backbone/conv1/_input_quantizer/DequantizeLinear and /backbone/conv1/_weight_quantizer/DequantizeLinear) into /backbone/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/conv1/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/conv1/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /backbone/layer1/layer1.0/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /backbone/layer1/layer1.0/conv2/_input_quantizer/QuantizeLinear into /backbone/layer1/layer1.0/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/backbone/layer1/layer1.0/conv1/_input_quantizer/DequantizeLinear and /backbone/layer1/layer1.0/conv1/_weight_quantizer/DequantizeLinear) into /backbone/layer1/layer1.0/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer1/layer1.0/conv2/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer1/layer1.0/conv1/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer1/layer1.0/conv1/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /backbone/layer1/layer1.0/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/backbone/layer1/layer1.0/conv2/_input_quantizer/DequantizeLinear and /backbone/layer1/layer1.0/conv2/_weight_quantizer/DequantizeLinear) into /backbone/layer1/layer1.0/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer1/layer1.0/conv2/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer1/layer1.0/conv2/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /backbone/layer1/layer1.1/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /backbone/layer1/layer1.1/conv2/_input_quantizer/QuantizeLinear into /backbone/layer1/layer1.1/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/backbone/layer1/layer1.1/conv1/_input_quantizer/DequantizeLinear and /backbone/layer1/layer1.1/conv1/_weight_quantizer/DequantizeLinear) into /backbone/layer1/layer1.1/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer1/layer1.1/conv2/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer1/layer1.1/conv1/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer1/layer1.1/conv1/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /backbone/layer1/layer1.1/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/backbone/layer1/layer1.1/conv2/_input_quantizer/DequantizeLinear and /backbone/layer1/layer1.1/conv2/_weight_quantizer/DequantizeLinear) into /backbone/layer1/layer1.1/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer1/layer1.1/conv2/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer1/layer1.1/conv2/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /backbone/layer2/layer2.0/downsample/downsample.0/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/DequantizeLinear and /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear) into /backbone/layer2/layer2.0/downsample/downsample.0/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /backbone/layer2/layer2.0/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /backbone/layer2/layer2.0/conv2/_input_quantizer/QuantizeLinear into /backbone/layer2/layer2.0/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/backbone/layer2/layer2.0/conv1/_input_quantizer/DequantizeLinear and /backbone/layer2/layer2.0/conv1/_weight_quantizer/DequantizeLinear) into /backbone/layer2/layer2.0/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer2/layer2.0/conv2/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer2/layer2.0/conv1/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer2/layer2.0/conv1/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /backbone/layer2/layer2.0/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/backbone/layer2/layer2.0/conv2/_input_quantizer/DequantizeLinear and /backbone/layer2/layer2.0/conv2/_weight_quantizer/DequantizeLinear) into /backbone/layer2/layer2.0/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer2/layer2.0/conv2/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer2/layer2.0/conv2/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /backbone/layer2/layer2.1/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /backbone/layer2/layer2.1/conv2/_input_quantizer/QuantizeLinear into /backbone/layer2/layer2.1/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/backbone/layer2/layer2.1/conv1/_input_quantizer/DequantizeLinear and /backbone/layer2/layer2.1/conv1/_weight_quantizer/DequantizeLinear) into /backbone/layer2/layer2.1/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer2/layer2.1/conv2/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer2/layer2.1/conv1/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer2/layer2.1/conv1/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /backbone/layer2/layer2.1/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/backbone/layer2/layer2.1/conv2/_input_quantizer/DequantizeLinear and /backbone/layer2/layer2.1/conv2/_weight_quantizer/DequantizeLinear) into /backbone/layer2/layer2.1/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer2/layer2.1/conv2/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer2/layer2.1/conv2/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /backbone/layer3/layer3.0/downsample/downsample.0/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/DequantizeLinear and /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear) into /backbone/layer3/layer3.0/downsample/downsample.0/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /backbone/layer3/layer3.0/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /backbone/layer3/layer3.0/conv2/_input_quantizer/QuantizeLinear into /backbone/layer3/layer3.0/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/backbone/layer3/layer3.0/conv1/_input_quantizer/DequantizeLinear and /backbone/layer3/layer3.0/conv1/_weight_quantizer/DequantizeLinear) into /backbone/layer3/layer3.0/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer3/layer3.0/conv2/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer3/layer3.0/conv1/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer3/layer3.0/conv1/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /backbone/layer3/layer3.0/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/backbone/layer3/layer3.0/conv2/_input_quantizer/DequantizeLinear and /backbone/layer3/layer3.0/conv2/_weight_quantizer/DequantizeLinear) into /backbone/layer3/layer3.0/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer3/layer3.0/conv2/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer3/layer3.0/conv2/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /backbone/layer3/layer3.1/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /backbone/layer3/layer3.1/conv2/_input_quantizer/QuantizeLinear into /backbone/layer3/layer3.1/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/backbone/layer3/layer3.1/conv1/_input_quantizer/DequantizeLinear and /backbone/layer3/layer3.1/conv1/_weight_quantizer/DequantizeLinear) into /backbone/layer3/layer3.1/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer3/layer3.1/conv2/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer3/layer3.1/conv1/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer3/layer3.1/conv1/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /backbone/layer3/layer3.1/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/backbone/layer3/layer3.1/conv2/_input_quantizer/DequantizeLinear and /backbone/layer3/layer3.1/conv2/_weight_quantizer/DequantizeLinear) into /backbone/layer3/layer3.1/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer3/layer3.1/conv2/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer3/layer3.1/conv2/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /backbone/layer4/layer4.0/downsample/downsample.0/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/DequantizeLinear and /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear) into /backbone/layer4/layer4.0/downsample/downsample.0/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /backbone/layer4/layer4.0/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /backbone/layer4/layer4.0/conv2/_input_quantizer/QuantizeLinear into /backbone/layer4/layer4.0/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/backbone/layer4/layer4.0/conv1/_input_quantizer/DequantizeLinear and /backbone/layer4/layer4.0/conv1/_weight_quantizer/DequantizeLinear) into /backbone/layer4/layer4.0/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer4/layer4.0/conv2/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer4/layer4.0/conv1/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer4/layer4.0/conv1/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /backbone/layer4/layer4.0/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/backbone/layer4/layer4.0/conv2/_input_quantizer/DequantizeLinear and /backbone/layer4/layer4.0/conv2/_weight_quantizer/DequantizeLinear) into /backbone/layer4/layer4.0/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer4/layer4.0/conv2/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer4/layer4.0/conv2/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /backbone/layer4/layer4.1/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /backbone/layer4/layer4.1/conv2/_input_quantizer/QuantizeLinear into /backbone/layer4/layer4.1/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/backbone/layer4/layer4.1/conv1/_input_quantizer/DequantizeLinear and /backbone/layer4/layer4.1/conv1/_weight_quantizer/DequantizeLinear) into /backbone/layer4/layer4.1/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer4/layer4.1/conv2/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer4/layer4.1/conv1/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer4/layer4.1/conv1/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /backbone/layer4/layer4.1/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/backbone/layer4/layer4.1/conv2/_input_quantizer/DequantizeLinear and /backbone/layer4/layer4.1/conv2/_weight_quantizer/DequantizeLinear) into /backbone/layer4/layer4.1/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer4/layer4.1/conv2/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /backbone/layer4/layer4.1/conv2/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /neck/detect1/conv2/conv/_input_quantizer/QuantizeLinear into /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/detect1/conv1/conv/_input_quantizer/DequantizeLinear and /neck/detect1/conv1/conv/_weight_quantizer/DequantizeLinear) into /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv2/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv1/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv1/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /neck/detect1/conv3/conv/_input_quantizer/QuantizeLinear into /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/detect1/conv2/conv/_input_quantizer/DequantizeLinear and /neck/detect1/conv2/conv/_weight_quantizer/DequantizeLinear) into /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv3/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv2/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv2/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/detect1/conv3/conv/Conv + PWN(/neck/detect1/conv3/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /neck/detect1/conv4/conv/_input_quantizer/QuantizeLinear into /neck/detect1/conv3/conv/Conv + PWN(/neck/detect1/conv3/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/detect1/conv3/conv/_input_quantizer/DequantizeLinear and /neck/detect1/conv3/conv/_weight_quantizer/DequantizeLinear) into /neck/detect1/conv3/conv/Conv + PWN(/neck/detect1/conv3/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv4/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv3/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv3/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/detect1/conv4/conv/Conv + PWN(/neck/detect1/conv4/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /neck/detect1/conv5/conv/_input_quantizer/QuantizeLinear into /neck/detect1/conv4/conv/Conv + PWN(/neck/detect1/conv4/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/detect1/conv4/conv/_input_quantizer/DequantizeLinear and /neck/detect1/conv4/conv/_weight_quantizer/DequantizeLinear) into /neck/detect1/conv4/conv/Conv + PWN(/neck/detect1/conv4/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv5/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv4/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv4/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/detect1/conv5/conv/Conv + PWN(/neck/detect1/conv5/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /neck/conv1/conv/_input_quantizer/QuantizeLinear into /neck/detect1/conv5/conv/Conv + PWN(/neck/detect1/conv5/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/detect1/conv5/conv/_input_quantizer/DequantizeLinear and /neck/detect1/conv5/conv/_weight_quantizer/DequantizeLinear) into /neck/detect1/conv5/conv/Conv + PWN(/neck/detect1/conv5/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/conv1/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv5/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv5/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_clone_0 into /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/conv1/conv/_input_quantizer/DequantizeLinear and /neck/conv1/conv/_weight_quantizer/DequantizeLinear) into /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_clone_0
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/conv1/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/conv1/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/detect1/conv6/conv/Conv + PWN(/neck/detect1/conv6/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /neck/detect1/conv7/_input_quantizer/QuantizeLinear into /neck/detect1/conv6/conv/Conv + PWN(/neck/detect1/conv6/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/detect1/conv6/conv/_input_quantizer/DequantizeLinear and /neck/detect1/conv6/conv/_weight_quantizer/DequantizeLinear) into /neck/detect1/conv6/conv/Conv + PWN(/neck/detect1/conv6/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv7/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv6/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv6/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/detect1/conv7/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/detect1/conv7/_input_quantizer/DequantizeLinear and /neck/detect1/conv7/_weight_quantizer/DequantizeLinear) into /neck/detect1/conv7/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv7/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect1/conv7/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /neck/detect2/conv2/conv/_input_quantizer/QuantizeLinear into /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/detect2/conv1/conv/_input_quantizer/DequantizeLinear and /neck/detect2/conv1/conv/_weight_quantizer/DequantizeLinear) into /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv2/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv1/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv1/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /neck/detect2/conv3/conv/_input_quantizer/QuantizeLinear into /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/detect2/conv2/conv/_input_quantizer/DequantizeLinear and /neck/detect2/conv2/conv/_weight_quantizer/DequantizeLinear) into /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv3/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv2/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv2/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /neck/detect2/conv4/conv/_input_quantizer/QuantizeLinear into /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/detect2/conv3/conv/_input_quantizer/DequantizeLinear and /neck/detect2/conv3/conv/_weight_quantizer/DequantizeLinear) into /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv4/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv3/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv3/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/detect2/conv4/conv/Conv + PWN(/neck/detect2/conv4/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /neck/detect2/conv5/conv/_input_quantizer/QuantizeLinear into /neck/detect2/conv4/conv/Conv + PWN(/neck/detect2/conv4/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/detect2/conv4/conv/_input_quantizer/DequantizeLinear and /neck/detect2/conv4/conv/_weight_quantizer/DequantizeLinear) into /neck/detect2/conv4/conv/Conv + PWN(/neck/detect2/conv4/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv5/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv4/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv4/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/detect2/conv5/conv/Conv + PWN(/neck/detect2/conv5/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /neck/conv2/conv/_input_quantizer/QuantizeLinear into /neck/detect2/conv5/conv/Conv + PWN(/neck/detect2/conv5/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/detect2/conv5/conv/_input_quantizer/DequantizeLinear and /neck/detect2/conv5/conv/_weight_quantizer/DequantizeLinear) into /neck/detect2/conv5/conv/Conv + PWN(/neck/detect2/conv5/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/conv2/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv5/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv5/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_clone_0 into /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/conv2/conv/_input_quantizer/DequantizeLinear and /neck/conv2/conv/_weight_quantizer/DequantizeLinear) into /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_clone_0
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/conv2/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/conv2/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/detect2/conv6/conv/Conv + PWN(/neck/detect2/conv6/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /neck/detect2/conv7/_input_quantizer/QuantizeLinear into /neck/detect2/conv6/conv/Conv + PWN(/neck/detect2/conv6/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/detect2/conv6/conv/_input_quantizer/DequantizeLinear and /neck/detect2/conv6/conv/_weight_quantizer/DequantizeLinear) into /neck/detect2/conv6/conv/Conv + PWN(/neck/detect2/conv6/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv7/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv6/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv6/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/detect2/conv7/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/detect2/conv7/_input_quantizer/DequantizeLinear and /neck/detect2/conv7/_weight_quantizer/DequantizeLinear) into /neck/detect2/conv7/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv7/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect2/conv7/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /neck/detect3/conv2/conv/_input_quantizer/QuantizeLinear into /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/detect3/conv1/conv/_input_quantizer/DequantizeLinear and /neck/detect3/conv1/conv/_weight_quantizer/DequantizeLinear) into /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv2/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv1/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv1/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /neck/detect3/conv3/conv/_input_quantizer/QuantizeLinear into /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/detect3/conv2/conv/_input_quantizer/DequantizeLinear and /neck/detect3/conv2/conv/_weight_quantizer/DequantizeLinear) into /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv3/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv2/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv2/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /neck/detect3/conv4/conv/_input_quantizer/QuantizeLinear into /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/detect3/conv3/conv/_input_quantizer/DequantizeLinear and /neck/detect3/conv3/conv/_weight_quantizer/DequantizeLinear) into /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv4/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv3/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv3/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/detect3/conv4/conv/Conv + PWN(/neck/detect3/conv4/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /neck/detect3/conv5/conv/_input_quantizer/QuantizeLinear into /neck/detect3/conv4/conv/Conv + PWN(/neck/detect3/conv4/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/detect3/conv4/conv/_input_quantizer/DequantizeLinear and /neck/detect3/conv4/conv/_weight_quantizer/DequantizeLinear) into /neck/detect3/conv4/conv/Conv + PWN(/neck/detect3/conv4/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv5/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv4/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv4/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/detect3/conv5/conv/Conv + PWN(/neck/detect3/conv5/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /neck/detect3/conv6/conv/_input_quantizer/QuantizeLinear into /neck/detect3/conv5/conv/Conv + PWN(/neck/detect3/conv5/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/detect3/conv5/conv/_input_quantizer/DequantizeLinear and /neck/detect3/conv5/conv/_weight_quantizer/DequantizeLinear) into /neck/detect3/conv5/conv/Conv + PWN(/neck/detect3/conv5/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv6/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv5/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv5/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/detect3/conv6/conv/Conv + PWN(/neck/detect3/conv6/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing /neck/detect3/conv7/_input_quantizer/QuantizeLinear into /neck/detect3/conv6/conv/Conv + PWN(/neck/detect3/conv6/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/detect3/conv6/conv/_input_quantizer/DequantizeLinear and /neck/detect3/conv6/conv/_weight_quantizer/DequantizeLinear) into /neck/detect3/conv6/conv/Conv + PWN(/neck/detect3/conv6/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv7/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv6/conv/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv6/conv/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: QuantizeDoubleInputNodes on /neck/detect3/conv7/Conv
[10/31/2023-17:53:36] [V] [TRT] QuantizeDoubleInputNodes: fusing (/neck/detect3/conv7/_input_quantizer/DequantizeLinear and /neck/detect3/conv7/_weight_quantizer/DequantizeLinear) into /neck/detect3/conv7/Conv
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv7/_input_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Removing /neck/detect3/conv7/_weight_quantizer/DequantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear with /backbone/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear with /backbone/layer1/layer1.0/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear with /backbone/layer1/layer1.0/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on backbone.layer1.1.conv1.weight + /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing backbone.layer1.1.conv1.weight + /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear with /backbone/layer1/layer1.1/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on backbone.layer1.1.conv2.weight + /backbone/layer1/layer1.1/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing backbone.layer1.1.conv2.weight + /backbone/layer1/layer1.1/conv2/_weight_quantizer/QuantizeLinear with /backbone/layer1/layer1.1/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear with /backbone/layer2/layer2.0/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear with /backbone/layer2/layer2.0/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear with /backbone/layer2/layer2.0/downsample/downsample.0/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear with /backbone/layer2/layer2.1/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on backbone.layer2.1.conv2.weight + /backbone/layer2/layer2.1/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing backbone.layer2.1.conv2.weight + /backbone/layer2/layer2.1/conv2/_weight_quantizer/QuantizeLinear with /backbone/layer2/layer2.1/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear with /backbone/layer3/layer3.0/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear with /backbone/layer3/layer3.0/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear with /backbone/layer3/layer3.0/downsample/downsample.0/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear with /backbone/layer3/layer3.1/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on backbone.layer3.1.conv2.weight + /backbone/layer3/layer3.1/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing backbone.layer3.1.conv2.weight + /backbone/layer3/layer3.1/conv2/_weight_quantizer/QuantizeLinear with /backbone/layer3/layer3.1/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear with /backbone/layer4/layer4.0/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear with /backbone/layer4/layer4.0/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear with /backbone/layer4/layer4.0/downsample/downsample.0/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear with /backbone/layer4/layer4.1/conv1/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on backbone.layer4.1.conv2.weight + /backbone/layer4/layer4.1/conv2/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing backbone.layer4.1.conv2.weight + /backbone/layer4/layer4.1/conv2/_weight_quantizer/QuantizeLinear with /backbone/layer4/layer4.1/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear with /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear with /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.detect1.conv3.conv.weight + /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.detect1.conv3.conv.weight + /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear with /neck/detect1/conv3/conv/Conv + PWN(/neck/detect1/conv3/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.detect1.conv4.conv.weight + /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.detect1.conv4.conv.weight + /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear with /neck/detect1/conv4/conv/Conv + PWN(/neck/detect1/conv4/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.detect1.conv5.conv.weight + /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.detect1.conv5.conv.weight + /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear with /neck/detect1/conv5/conv/Conv + PWN(/neck/detect1/conv5/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.detect1.conv6.conv.weight + /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.detect1.conv6.conv.weight + /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear with /neck/detect1/conv6/conv/Conv + PWN(/neck/detect1/conv6/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear with /neck/detect1/conv7/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear with /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear with /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear with /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear with /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.detect2.conv4.conv.weight + /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.detect2.conv4.conv.weight + /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear with /neck/detect2/conv4/conv/Conv + PWN(/neck/detect2/conv4/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.detect2.conv5.conv.weight + /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.detect2.conv5.conv.weight + /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear with /neck/detect2/conv5/conv/Conv + PWN(/neck/detect2/conv5/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.detect2.conv6.conv.weight + /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.detect2.conv6.conv.weight + /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear with /neck/detect2/conv6/conv/Conv + PWN(/neck/detect2/conv6/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear with /neck/detect2/conv7/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear with /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear with /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear with /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear with /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.detect3.conv4.conv.weight + /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.detect3.conv4.conv.weight + /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear with /neck/detect3/conv4/conv/Conv + PWN(/neck/detect3/conv4/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.detect3.conv5.conv.weight + /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.detect3.conv5.conv.weight + /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear with /neck/detect3/conv5/conv/Conv + PWN(/neck/detect3/conv5/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.detect3.conv6.conv.weight + /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.detect3.conv6.conv.weight + /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear with /neck/detect3/conv6/conv/Conv + PWN(/neck/detect3/conv6/relu/LeakyRelu)
[10/31/2023-17:53:36] [V] [TRT] Running: ConstWeightsFusion on neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] ConstWeightsFusion: Fusing neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear with /neck/detect3/conv7/Conv
[10/31/2023-17:53:36] [V] [TRT] Running: ConvEltwiseSumFusion on backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] ConvEltwiseSumFusion: Fusing backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv with /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: ConvEltwiseSumFusion on backbone.layer1.1.conv2.weight + /backbone/layer1/layer1.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.1/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] ConvEltwiseSumFusion: Fusing backbone.layer1.1.conv2.weight + /backbone/layer1/layer1.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.1/conv2/Conv with /backbone/layer1/layer1.1/Add + /backbone/layer1/layer1.1/relu_1/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: ConvEltwiseSumFusion on backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] ConvEltwiseSumFusion: Fusing backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv with /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: ConvEltwiseSumFusion on backbone.layer2.1.conv2.weight + /backbone/layer2/layer2.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] ConvEltwiseSumFusion: Fusing backbone.layer2.1.conv2.weight + /backbone/layer2/layer2.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv2/Conv with /backbone/layer2/layer2.1/Add + /backbone/layer2/layer2.1/relu_1/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: ConvEltwiseSumFusion on backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] ConvEltwiseSumFusion: Fusing backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv with /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: ConvEltwiseSumFusion on backbone.layer3.1.conv2.weight + /backbone/layer3/layer3.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] ConvEltwiseSumFusion: Fusing backbone.layer3.1.conv2.weight + /backbone/layer3/layer3.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv2/Conv with /backbone/layer3/layer3.1/Add + /backbone/layer3/layer3.1/relu_1/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: ConvEltwiseSumFusion on backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] ConvEltwiseSumFusion: Fusing backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv with /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu
[10/31/2023-17:53:36] [V] [TRT] Running: ConvEltwiseSumFusion on backbone.layer4.1.conv2.weight + /backbone/layer4/layer4.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv2/Conv
[10/31/2023-17:53:36] [V] [TRT] ConvEltwiseSumFusion: Fusing backbone.layer4.1.conv2.weight + /backbone/layer4/layer4.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv2/Conv with /backbone/layer4/layer4.1/Add + /backbone/layer4/layer4.1/relu_1/Relu
[10/31/2023-17:53:36] [V] [TRT] After dupe layer removal: 82 layers
[10/31/2023-17:53:36] [V] [TRT] After final dead-layer removal: 82 layers
[10/31/2023-17:53:36] [V] [TRT] After tensor merging: 82 layers
[10/31/2023-17:53:36] [V] [TRT] QDQ graph optimizer quantization epilogue pass
[10/31/2023-17:53:36] [V] [TRT] QDQ optimization pass
[10/31/2023-17:53:36] [V] [TRT] QDQ graph optimizer constant fold dangling QDQ pass
[10/31/2023-17:53:36] [V] [TRT] Running: QDQToCopy on /backbone/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /backbone/conv1/_input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[10/31/2023-17:53:36] [V] [TRT] Running: QDQToCopy on /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[10/31/2023-17:53:36] [V] [TRT] Running: QDQToCopy on /backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[10/31/2023-17:53:36] [V] [TRT] Running: QDQToCopy on /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[10/31/2023-17:53:36] [V] [TRT] Running: QDQToCopy on /backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[10/31/2023-17:53:36] [V] [TRT] Running: QDQToCopy on /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[10/31/2023-17:53:36] [V] [TRT] Running: QDQToCopy on /backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[10/31/2023-17:53:36] [V] [TRT] Running: QDQToCopy on /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[10/31/2023-17:53:36] [V] [TRT] Running: QDQToCopy on /backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[10/31/2023-17:53:36] [V] [TRT] Running: QDQToCopy on /neck/detect1/conv1/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/detect1/conv1/conv/_input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[10/31/2023-17:53:36] [V] [TRT] Running: QDQToCopy on /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_clone_1
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_clone_1 from QUANTIZE to kQDQ
[10/31/2023-17:53:36] [V] [TRT] Running: QDQToCopy on /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_clone_1
[10/31/2023-17:53:36] [V] [TRT] Swap the layer type of /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_clone_1 from QUANTIZE to kQDQ
[10/31/2023-17:53:36] [V] [TRT] Running: ScaleScaleFusion on backbone.layer2.0.downsample.1.weight + (Unnamed Layer* 105) [Shuffle] + (Unnamed Layer* 114) [ElementWise]
[10/31/2023-17:53:36] [V] [TRT] ScaleScaleFusion: Fusing backbone.layer2.0.downsample.1.weight + (Unnamed Layer* 105) [Shuffle] + (Unnamed Layer* 114) [ElementWise] with backbone.layer2.0.bn2.bias + Identity_2 + (Unnamed Layer* 106) [Shuffle] + /backbone/layer2/layer2.0/downsample/downsample.1/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: ScaleScaleFusion on backbone.layer3.0.downsample.1.weight + (Unnamed Layer* 179) [Shuffle] + (Unnamed Layer* 188) [ElementWise]
[10/31/2023-17:53:36] [V] [TRT] ScaleScaleFusion: Fusing backbone.layer3.0.downsample.1.weight + (Unnamed Layer* 179) [Shuffle] + (Unnamed Layer* 188) [ElementWise] with backbone.layer3.0.bn2.bias + Identity_1 + (Unnamed Layer* 180) [Shuffle] + /backbone/layer3/layer3.0/downsample/downsample.1/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] Running: ScaleScaleFusion on backbone.layer4.0.downsample.1.weight + (Unnamed Layer* 253) [Shuffle] + (Unnamed Layer* 262) [ElementWise]
[10/31/2023-17:53:36] [V] [TRT] ScaleScaleFusion: Fusing backbone.layer4.0.downsample.1.weight + (Unnamed Layer* 253) [Shuffle] + (Unnamed Layer* 262) [ElementWise] with backbone.layer4.0.bn2.bias + Identity_0 + (Unnamed Layer* 254) [Shuffle] + /backbone/layer4/layer4.0/downsample/downsample.1/BatchNormalization
[10/31/2023-17:53:36] [V] [TRT] After dupe layer removal: 79 layers
[10/31/2023-17:53:36] [V] [TRT] After final dead-layer removal: 79 layers
[10/31/2023-17:53:36] [V] [TRT] After tensor merging: 79 layers
[10/31/2023-17:53:36] [V] [TRT] After vertical fusions: 79 layers
[10/31/2023-17:53:36] [V] [TRT] After dupe layer removal: 79 layers
[10/31/2023-17:53:36] [V] [TRT] After final dead-layer removal: 79 layers
[10/31/2023-17:53:36] [V] [TRT] After tensor merging: 79 layers
[10/31/2023-17:53:36] [V] [TRT] After slice removal: 79 layers
[10/31/2023-17:53:36] [V] [TRT] Eliminating concatenation /neck/Concat_1
[10/31/2023-17:53:36] [V] [TRT] Generating copy for /neck/Concat_1_/neck/Resize_1_output_0_clone_0 to /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_output_0 because input does not support striding.
[10/31/2023-17:53:36] [V] [TRT] Retargeting /neck/Concat_1_/backbone/layer2/layer2.1/relu_1/Relu_output_0_clone_1 to /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:36] [V] [TRT] Eliminating concatenation /neck/Concat
[10/31/2023-17:53:36] [V] [TRT] Generating copy for /neck/Concat_/neck/Resize_output_0_clone_0 to /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_output_0 because input does not support striding.
[10/31/2023-17:53:36] [V] [TRT] Retargeting /neck/Concat_/backbone/layer3/layer3.1/relu_1/Relu_output_0_clone_1 to /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_output_0
[10/31/2023-17:53:36] [V] [TRT] After concat removal: 79 layers
[10/31/2023-17:53:36] [V] [TRT] Trying to split Reshape and strided tensor
[10/31/2023-17:53:36] [V] [TRT] Graph construction and optimization completed in 0.831256 seconds.
[10/31/2023-17:53:36] [I] [TRT] ---------- Layers Running on DLA ----------
[10/31/2023-17:53:36] [I] [TRT] ---------- Layers Running on GPU ----------
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] TRAIN_STATION: [trainStation1]
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] COPY: /backbone/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONSTANT: backbone.layer2.0.downsample.1.running_var + (Unnamed Layer* 108) [Shuffle]
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONSTANT: backbone.layer2.0.downsample.1.running_mean + (Unnamed Layer* 107) [Shuffle]
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONSTANT: backbone.layer3.0.downsample.1.running_var + (Unnamed Layer* 182) [Shuffle]
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONSTANT: backbone.layer3.0.downsample.1.running_mean + (Unnamed Layer* 181) [Shuffle]
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONSTANT: backbone.layer4.0.downsample.1.running_var + (Unnamed Layer* 256) [Shuffle]
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONSTANT: backbone.layer4.0.downsample.1.running_mean + (Unnamed Layer* 255) [Shuffle]
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONSTANT: (Unnamed Layer* 714) [Constant]
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONSTANT: (Unnamed Layer* 715) [Constant]
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONSTANT: (Unnamed Layer* 716) [Constant]
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear + /backbone/conv1/Conv
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] POOLING: /backbone/maxpool/MaxPool
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] COPY: /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] COPY: /backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: backbone.layer1.1.conv1.weight + /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.1/conv1/Conv
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: backbone.layer1.1.conv2.weight + /backbone/layer1/layer1.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.1/conv2/Conv + /backbone/layer1/layer1.1/Add + /backbone/layer1/layer1.1/relu_1/Relu
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] COPY: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] POINTWISE: PWN((Unnamed Layer* 112) [ElementWise], PWN(PWN((Unnamed Layer* 109) [Constant] + (Unnamed Layer* 110) [ElementWise], (Unnamed Layer* 111) [Unary]), (Unnamed Layer* 113) [ElementWise]))
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] SCALE: backbone.layer2.0.downsample.1.weight + (Unnamed Layer* 105) [Shuffle] + (Unnamed Layer* 114) [ElementWise] + backbone.layer2.0.bn2.bias + Identity_2 + (Unnamed Layer* 106) [Shuffle] + /backbone/layer2/layer2.0/downsample/downsample.1/BatchNormalization
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] COPY: /backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: backbone.layer2.1.conv2.weight + /backbone/layer2/layer2.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv2/Conv + /backbone/layer2/layer2.1/Add + /backbone/layer2/layer2.1/relu_1/Relu
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] COPY: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] COPY: /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_clone_1
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] POINTWISE: PWN((Unnamed Layer* 186) [ElementWise], PWN(PWN((Unnamed Layer* 183) [Constant] + (Unnamed Layer* 184) [ElementWise], (Unnamed Layer* 185) [Unary]), (Unnamed Layer* 187) [ElementWise]))
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] SCALE: backbone.layer3.0.downsample.1.weight + (Unnamed Layer* 179) [Shuffle] + (Unnamed Layer* 188) [ElementWise] + backbone.layer3.0.bn2.bias + Identity_1 + (Unnamed Layer* 180) [Shuffle] + /backbone/layer3/layer3.0/downsample/downsample.1/BatchNormalization
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] COPY: /backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: backbone.layer3.1.conv2.weight + /backbone/layer3/layer3.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv2/Conv + /backbone/layer3/layer3.1/Add + /backbone/layer3/layer3.1/relu_1/Relu
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] COPY: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] COPY: /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_clone_1
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] POINTWISE: PWN((Unnamed Layer* 260) [ElementWise], PWN(PWN((Unnamed Layer* 257) [Constant] + (Unnamed Layer* 258) [ElementWise], (Unnamed Layer* 259) [Unary]), (Unnamed Layer* 261) [ElementWise]))
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] SCALE: backbone.layer4.0.downsample.1.weight + (Unnamed Layer* 253) [Shuffle] + (Unnamed Layer* 262) [ElementWise] + backbone.layer4.0.bn2.bias + Identity_0 + (Unnamed Layer* 254) [Shuffle] + /backbone/layer4/layer4.0/downsample/downsample.1/BatchNormalization
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] COPY: /backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: backbone.layer4.1.conv2.weight + /backbone/layer4/layer4.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv2/Conv + /backbone/layer4/layer4.1/Add + /backbone/layer4/layer4.1/relu_1/Relu
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] COPY: /neck/detect1/conv1/conv/_input_quantizer/QuantizeLinear
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.detect1.conv3.conv.weight + /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv3/conv/Conv + PWN(/neck/detect1/conv3/relu/LeakyRelu)
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.detect1.conv4.conv.weight + /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv4/conv/Conv + PWN(/neck/detect1/conv4/relu/LeakyRelu)
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.detect1.conv5.conv.weight + /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv5/conv/Conv + PWN(/neck/detect1/conv5/relu/LeakyRelu)
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.detect1.conv6.conv.weight + /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv6/conv/Conv + PWN(/neck/detect1/conv6/relu/LeakyRelu)
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] RESIZE: /neck/Resize
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] COPY: /neck/Concat_/neck/Resize_output_0_clone_0 copy
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu)
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.detect2.conv4.conv.weight + /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv4/conv/Conv + PWN(/neck/detect2/conv4/relu/LeakyRelu)
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.detect2.conv5.conv.weight + /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv5/conv/Conv + PWN(/neck/detect2/conv5/relu/LeakyRelu)
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.detect2.conv6.conv.weight + /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv6/conv/Conv + PWN(/neck/detect2/conv6/relu/LeakyRelu)
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] RESIZE: /neck/Resize_1
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] COPY: /neck/Concat_1_/neck/Resize_1_output_0_clone_0 copy
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu)
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu)
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu)
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.detect3.conv4.conv.weight + /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv4/conv/Conv + PWN(/neck/detect3/conv4/relu/LeakyRelu)
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.detect3.conv5.conv.weight + /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv5/conv/Conv + PWN(/neck/detect3/conv5/relu/LeakyRelu)
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.detect3.conv6.conv.weight + /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv6/conv/Conv + PWN(/neck/detect3/conv6/relu/LeakyRelu)
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] CONVOLUTION: neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] MYELIN: {ForeignNode[/head/Constant_2_output_0.../Unsqueeze_2]}
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] NMS: /NonMaxSuppression_246
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] DEVICE_TO_SHAPE_HOST: (Unnamed Layer* 718) [NMS]_1_output[DevicetoShapeHostCopy]
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] TRAIN_STATION: [trainStation2]
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] MYELIN: {ForeignNode[/Split_2.../Concat_1]}
[10/31/2023-17:53:36] [I] [TRT] [GpuLayer] TRAIN_STATION: [trainStation3]
[10/31/2023-17:53:36] [V] [TRT] Trying to load shared library libcublas.so.11
[10/31/2023-17:53:36] [V] [TRT] Loaded shared library libcublas.so.11
[10/31/2023-17:53:37] [V] [TRT] Using cublas as plugin tactic source
[10/31/2023-17:53:37] [V] [TRT] Trying to load shared library libcublasLt.so.11
[10/31/2023-17:53:37] [V] [TRT] Loaded shared library libcublasLt.so.11
[10/31/2023-17:53:37] [V] [TRT] Using cublasLt as core library tactic source
[10/31/2023-17:53:37] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +535, GPU +802, now: CPU 1240, GPU 3966 (MiB)
[10/31/2023-17:53:37] [V] [TRT] Trying to load shared library libcudnn.so.8
[10/31/2023-17:53:37] [V] [TRT] Loaded shared library libcudnn.so.8
[10/31/2023-17:53:37] [V] [TRT] Using cuDNN as plugin tactic source
[10/31/2023-17:53:38] [V] [TRT] Using cuDNN as core library tactic source
[10/31/2023-17:53:38] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +82, GPU +105, now: CPU 1322, GPU 4071 (MiB)
[10/31/2023-17:53:38] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.
[10/31/2023-17:53:38] [V] [TRT] Constructing optimization profile number 0 [1/1].
[10/31/2023-17:53:38] [V] [TRT] Reserving memory for host IO tensors. Host: 0 bytes
[10/31/2023-17:53:38] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:53:38] [V] [TRT] *************** Autotuning Reformat: Float(25648128,8549376,3872,1) -> Int8(8549376,8549376:4,3872,1) ***************
[10/31/2023-17:53:38] [V] [TRT] --------------- Timing Runner: /backbone/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:53:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 18.5897
[10/31/2023-17:53:38] [V] [TRT] Tactic: 0x00000000000003ea Time: 9.91533
[10/31/2023-17:53:38] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.73212
[10/31/2023-17:53:38] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 3.73212
[10/31/2023-17:53:38] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:53:38] [V] [TRT] *************** Autotuning Reformat: Float(25648128,8549376,3872,1) -> Int8(8549376,8549376:32,3872,1) ***************
[10/31/2023-17:53:38] [V] [TRT] --------------- Timing Runner: /backbone/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:53:40] [V] [TRT] Tactic: 0x00000000000003e8 Time: 210.224
[10/31/2023-17:53:40] [V] [TRT] Tactic: 0x00000000000003ea Time: 60.464
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 205.994
[10/31/2023-17:53:42] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 60.464
[10/31/2023-17:53:42] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:53:42] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(128,1,1,1) -> Float(128,1,128,128) ***************
[10/31/2023-17:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> (Unnamed Layer* 108) [Shuffle]_output) (Reformat)
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00906914
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0200251
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00972526
[10/31/2023-17:53:42] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00906914
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(128,1,1,1) -> Float(32,1:4,32,32) ***************
[10/31/2023-17:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> (Unnamed Layer* 108) [Shuffle]_output) (Reformat)
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.009316
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0209829
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00937143
[10/31/2023-17:53:42] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.009316
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(128,1,1,1) -> Float(4,1:32,1,1) ***************
[10/31/2023-17:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> (Unnamed Layer* 108) [Shuffle]_output) (Reformat)
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00817498
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0206648
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00855664
[10/31/2023-17:53:42] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00817498
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(128,1,1,1) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> (Unnamed Layer* 108) [Shuffle]_output) (Reformat)
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00825118
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0180537
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00834997
[10/31/2023-17:53:42] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00825118
[10/31/2023-17:53:42] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(128,1,1,1) -> Float(128,1,128,128) ***************
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(128,1,1,1) -> Float(32,1:4,32,32) ***************
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(128,1,1,1) -> Float(4,1:32,1,1) ***************
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(128,1,1,1) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:53:42] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(256,1,1,1) -> Float(256,1,256,256) ***************
[10/31/2023-17:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> (Unnamed Layer* 182) [Shuffle]_output) (Reformat)
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00823873
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0178943
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00835352
[10/31/2023-17:53:42] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00823873
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(256,1,1,1) -> Float(64,1:4,64,64) ***************
[10/31/2023-17:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> (Unnamed Layer* 182) [Shuffle]_output) (Reformat)
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00853862
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0192954
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0084125
[10/31/2023-17:53:42] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0084125
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(256,1,1,1) -> Float(8,1:32,1,1) ***************
[10/31/2023-17:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> (Unnamed Layer* 182) [Shuffle]_output) (Reformat)
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00788451
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0183526
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00872686
[10/31/2023-17:53:42] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00788451
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(256,1,1,1) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> (Unnamed Layer* 182) [Shuffle]_output) (Reformat)
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00842057
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0180514
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0082433
[10/31/2023-17:53:42] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0082433
[10/31/2023-17:53:42] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(256,1,1,1) -> Float(256,1,256,256) ***************
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(256,1,1,1) -> Float(64,1:4,64,64) ***************
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(256,1,1,1) -> Float(8,1:32,1,1) ***************
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(256,1,1,1) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:53:42] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(512,1,1,1) -> Float(512,1,512,512) ***************
[10/31/2023-17:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> (Unnamed Layer* 256) [Shuffle]_output) (Reformat)
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00810006
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0187503
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00845069
[10/31/2023-17:53:42] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00810006
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(512,1,1,1) -> Float(128,1:4,128,128) ***************
[10/31/2023-17:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> (Unnamed Layer* 256) [Shuffle]_output) (Reformat)
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00817143
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.020168
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00877042
[10/31/2023-17:53:42] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00817143
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(512,1,1,1) -> Float(16,1:32,1,1) ***************
[10/31/2023-17:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> (Unnamed Layer* 256) [Shuffle]_output) (Reformat)
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00814222
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0201623
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00840901
[10/31/2023-17:53:42] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00814222
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(512,1,1,1) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> (Unnamed Layer* 256) [Shuffle]_output) (Reformat)
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00835479
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0184217
[10/31/2023-17:53:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0083487
[10/31/2023-17:53:42] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0083487
[10/31/2023-17:53:42] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(512,1,1,1) -> Float(512,1,512,512) ***************
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(512,1,1,1) -> Float(128,1:4,128,128) ***************
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(512,1,1,1) -> Float(16,1:32,1,1) ***************
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Float(512,1,1,1) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:53:42] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:53:42] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,8549376:4,3872,1) -> Int8(8549376,8549376:32,3872,1) ***************
[10/31/2023-17:53:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/conv1/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:53:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 206.881
[10/31/2023-17:53:45] [V] [TRT] Tactic: 0x00000000000003ea Time: 92.7684
[10/31/2023-17:53:45] [V] [TRT] Tactic: 0x0000000000000000 Time: 28.3966
[10/31/2023-17:53:45] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 28.3966
[10/31/2023-17:53:45] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,8549376:32,3872,1) -> Int8(8549376,8549376:4,3872,1) ***************
[10/31/2023-17:53:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/conv1/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:53:45] [V] [TRT] Tactic: 0x00000000000003e8 Time: 23.1032
[10/31/2023-17:53:46] [V] [TRT] Tactic: 0x00000000000003ea Time: 43.6506
[10/31/2023-17:53:46] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.13652
[10/31/2023-17:53:46] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 5.13652
[10/31/2023-17:53:46] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:53:46] [V] [TRT] *************** Autotuning Reformat: Float(136790016,2137344,1936,1) -> Float(34197504,1:4,30976,16) ***************
[10/31/2023-17:53:46] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/relu/Relu_output_0 -> <out>) (Reformat)
[10/31/2023-17:53:47] [V] [TRT] Tactic: 0x00000000000003e8 Time: 86.2852
[10/31/2023-17:53:47] [V] [TRT] Tactic: 0x00000000000003ea Time: 41.1033
[10/31/2023-17:53:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 87.8088
[10/31/2023-17:53:49] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 41.1033
[10/31/2023-17:53:49] [V] [TRT] *************** Autotuning Reformat: Float(34197504,1:4,30976,16) -> Float(136790016,2137344,1936,1) ***************
[10/31/2023-17:53:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/relu/Relu_output_0 -> <out>) (Reformat)
[10/31/2023-17:53:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 151.377
[10/31/2023-17:53:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 41.867
[10/31/2023-17:53:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 151.412
[10/31/2023-17:53:52] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 41.867
[10/31/2023-17:53:52] [V] [TRT] *************** Autotuning Reformat: Float(4274688,2137344:32,1936,1) -> Float(136790016,2137344,1936,1) ***************
[10/31/2023-17:53:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/relu/Relu_output_0 -> <out>) (Reformat)
[10/31/2023-17:53:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 122.23
[10/31/2023-17:53:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 41.8628
[10/31/2023-17:53:55] [V] [TRT] Tactic: 0x0000000000000000 Time: 122.183
[10/31/2023-17:53:56] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 41.8628
[10/31/2023-17:53:56] [V] [TRT] *************** Autotuning Reformat: Float(4274688,2137344:32,1936,1) -> Float(34197504,1:4,30976,16) ***************
[10/31/2023-17:53:56] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/relu/Relu_output_0 -> <out>) (Reformat)
[10/31/2023-17:53:56] [V] [TRT] Tactic: 0x00000000000003e8 Time: 83.2114
[10/31/2023-17:53:57] [V] [TRT] Tactic: 0x00000000000003ea Time: 33.4756
[10/31/2023-17:53:58] [V] [TRT] Tactic: 0x0000000000000000 Time: 83.5166
[10/31/2023-17:53:58] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 33.4756
[10/31/2023-17:53:58] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:53:58] [V] [TRT] *************** Autotuning Reformat: Float(34197504,534336,968,1) -> Float(8549376,1:4,15488,16) ***************
[10/31/2023-17:53:58] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /backbone/maxpool/MaxPool_output_0) (Reformat)
[10/31/2023-17:53:58] [V] [TRT] Tactic: 0x00000000000003e8 Time: 21.3731
[10/31/2023-17:53:58] [V] [TRT] Tactic: 0x00000000000003ea Time: 10.2757
[10/31/2023-17:53:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 21.3814
[10/31/2023-17:53:59] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 10.2757
[10/31/2023-17:53:59] [V] [TRT] *************** Autotuning Reformat: Float(34197504,534336,968,1) -> Float(1068672,534336:32,968,1) ***************
[10/31/2023-17:53:59] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /backbone/maxpool/MaxPool_output_0) (Reformat)
[10/31/2023-17:53:59] [V] [TRT] Tactic: 0x00000000000003e8 Time: 43.7914
[10/31/2023-17:53:59] [V] [TRT] Tactic: 0x00000000000003ea Time: 10.273
[10/31/2023-17:53:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 37.9656
[10/31/2023-17:53:59] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 10.273
[10/31/2023-17:53:59] [V] [TRT] *************** Autotuning Reformat: Float(8549376,1:4,15488,16) -> Float(34197504,534336,968,1) ***************
[10/31/2023-17:53:59] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /backbone/maxpool/MaxPool_output_0) (Reformat)
[10/31/2023-17:54:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 29.0065
[10/31/2023-17:54:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 10.4576
[10/31/2023-17:54:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 28.845
[10/31/2023-17:54:00] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 10.4576
[10/31/2023-17:54:00] [V] [TRT] *************** Autotuning Reformat: Float(8549376,1:4,15488,16) -> Float(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /backbone/maxpool/MaxPool_output_0) (Reformat)
[10/31/2023-17:54:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 53.0264
[10/31/2023-17:54:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.29629
[10/31/2023-17:54:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 52.9501
[10/31/2023-17:54:01] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 8.29629
[10/31/2023-17:54:01] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:01] [V] [TRT] *************** Autotuning Reformat: Float(34197504,534336,968,1) -> Int8(34197504,534336,968,1) ***************
[10/31/2023-17:54:01] [V] [TRT] --------------- Timing Runner: /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 6.69157
[10/31/2023-17:54:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.93676
[10/31/2023-17:54:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.67811
[10/31/2023-17:54:01] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 6.67811
[10/31/2023-17:54:01] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:54:01] [V] [TRT] *************** Autotuning Reformat: Float(34197504,534336,968,1) -> Int8(8549376,534336:4,968,1) ***************
[10/31/2023-17:54:01] [V] [TRT] --------------- Timing Runner: /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:02] [V] [TRT] Tactic: 0x00000000000003e8 Time: 19.0571
[10/31/2023-17:54:02] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.19683
[10/31/2023-17:54:02] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.60112
[10/31/2023-17:54:02] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 3.60112
[10/31/2023-17:54:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:54:02] [V] [TRT] *************** Autotuning Reformat: Float(34197504,534336,968,1) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:02] [V] [TRT] --------------- Timing Runner: /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:02] [V] [TRT] Tactic: 0x00000000000003e8 Time: 26.6652
[10/31/2023-17:54:02] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.18646
[10/31/2023-17:54:02] [V] [TRT] Tactic: 0x0000000000000000 Time: 26.5843
[10/31/2023-17:54:02] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 8.18646
[10/31/2023-17:54:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:02] [V] [TRT] *************** Autotuning Reformat: Float(8549376,1:4,15488,16) -> Int8(34197504,534336,968,1) ***************
[10/31/2023-17:54:02] [V] [TRT] --------------- Timing Runner: /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:03] [V] [TRT] Tactic: 0x00000000000003e8 Time: 28.6805
[10/31/2023-17:54:03] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.14613
[10/31/2023-17:54:03] [V] [TRT] Tactic: 0x0000000000000000 Time: 28.8181
[10/31/2023-17:54:03] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 8.14613
[10/31/2023-17:54:03] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:03] [V] [TRT] *************** Autotuning Reformat: Float(8549376,1:4,15488,16) -> Int8(8549376,534336:4,968,1) ***************
[10/31/2023-17:54:03] [V] [TRT] --------------- Timing Runner: /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:03] [V] [TRT] Tactic: 0x00000000000003e8 Time: 28.7524
[10/31/2023-17:54:03] [V] [TRT] Tactic: 0x00000000000003ea Time: 9.98937
[10/31/2023-17:54:03] [V] [TRT] Tactic: 0x0000000000000000 Time: 28.9498
[10/31/2023-17:54:03] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 9.98937
[10/31/2023-17:54:03] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:03] [V] [TRT] *************** Autotuning Reformat: Float(8549376,1:4,15488,16) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:03] [V] [TRT] --------------- Timing Runner: /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:04] [V] [TRT] Tactic: 0x00000000000003e8 Time: 42.9586
[10/31/2023-17:54:04] [V] [TRT] Tactic: 0x00000000000003ea Time: 9.98627
[10/31/2023-17:54:04] [V] [TRT] Tactic: 0x0000000000000000 Time: 43.0763
[10/31/2023-17:54:04] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 9.98627
[10/31/2023-17:54:04] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:04] [V] [TRT] *************** Autotuning Reformat: Float(1068672,534336:32,968,1) -> Int8(34197504,534336,968,1) ***************
[10/31/2023-17:54:04] [V] [TRT] --------------- Timing Runner: /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 28.7272
[10/31/2023-17:54:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.1423
[10/31/2023-17:54:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 28.8266
[10/31/2023-17:54:05] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 8.1423
[10/31/2023-17:54:05] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:05] [V] [TRT] *************** Autotuning Reformat: Float(1068672,534336:32,968,1) -> Int8(8549376,534336:4,968,1) ***************
[10/31/2023-17:54:05] [V] [TRT] --------------- Timing Runner: /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 28.8421
[10/31/2023-17:54:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.95959
[10/31/2023-17:54:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 28.7387
[10/31/2023-17:54:06] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 8.95959
[10/31/2023-17:54:06] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:06] [V] [TRT] *************** Autotuning Reformat: Float(1068672,534336:32,968,1) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:06] [V] [TRT] --------------- Timing Runner: /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 43.2042
[10/31/2023-17:54:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.95617
[10/31/2023-17:54:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 43.3907
[10/31/2023-17:54:06] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 8.95617
[10/31/2023-17:54:06] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:06] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:06] [V] [TRT] *************** Autotuning Reformat: Int8(34197504,534336,968,1) -> Int8(8549376,534336:4,968,1) ***************
[10/31/2023-17:54:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:07] [V] [TRT] Tactic: 0x00000000000003e8 Time: 21.7626
[10/31/2023-17:54:07] [V] [TRT] Tactic: 0x00000000000003ea Time: 9.12423
[10/31/2023-17:54:07] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.04461
[10/31/2023-17:54:07] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 7.04461
[10/31/2023-17:54:07] [V] [TRT] *************** Autotuning Reformat: Int8(34197504,534336,968,1) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:07] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:07] [V] [TRT] Tactic: 0x00000000000003e8 Time: 25.6424
[10/31/2023-17:54:07] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.34778
[10/31/2023-17:54:07] [V] [TRT] Tactic: 0x0000000000000000 Time: 25.6572
[10/31/2023-17:54:07] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 8.34778
[10/31/2023-17:54:07] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,534336:4,968,1) -> Int8(34197504,534336,968,1) ***************
[10/31/2023-17:54:07] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:07] [V] [TRT] Tactic: 0x00000000000003e8 Time: 21.8809
[10/31/2023-17:54:07] [V] [TRT] Tactic: 0x00000000000003ea Time: 9.38222
[10/31/2023-17:54:08] [V] [TRT] Tactic: 0x0000000000000000 Time: 21.8806
[10/31/2023-17:54:08] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 9.38222
[10/31/2023-17:54:08] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,534336:4,968,1) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:08] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:08] [V] [TRT] Tactic: 0x00000000000003e8 Time: 16.6041
[10/31/2023-17:54:08] [V] [TRT] Tactic: 0x00000000000003ea Time: 11.1819
[10/31/2023-17:54:08] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.67272
[10/31/2023-17:54:08] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 3.67272
[10/31/2023-17:54:08] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,534336:32,968,1) -> Int8(34197504,534336,968,1) ***************
[10/31/2023-17:54:08] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:08] [V] [TRT] Tactic: 0x00000000000003e8 Time: 22.1263
[10/31/2023-17:54:08] [V] [TRT] Tactic: 0x00000000000003ea Time: 24.0805
[10/31/2023-17:54:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 22.1378
[10/31/2023-17:54:09] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 22.1263
[10/31/2023-17:54:09] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,534336:32,968,1) -> Int8(8549376,534336:4,968,1) ***************
[10/31/2023-17:54:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 6.69674
[10/31/2023-17:54:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 11.1955
[10/31/2023-17:54:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.11436
[10/31/2023-17:54:09] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.11436
[10/31/2023-17:54:09] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:09] [V] [TRT] *************** Autotuning Reformat: Int8(34197504,534336,968,1) -> Int8(8549376,534336:4,968,1) ***************
[10/31/2023-17:54:09] [V] [TRT] *************** Autotuning Reformat: Int8(34197504,534336,968,1) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:09] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,534336:4,968,1) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:09] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,534336:32,968,1) -> Int8(8549376,534336:4,968,1) ***************
[10/31/2023-17:54:09] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:09] [V] [TRT] *************** Autotuning Reformat: Float(34197504,534336,968,1) -> Float(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/maxpool/MaxPool_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 37.8169
[10/31/2023-17:54:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 10.2864
[10/31/2023-17:54:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 37.7882
[10/31/2023-17:54:10] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 10.2864
[10/31/2023-17:54:10] [V] [TRT] *************** Autotuning Reformat: Float(8549376,1:4,15488,16) -> Float(34197504,534336,968,1) ***************
[10/31/2023-17:54:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/maxpool/MaxPool_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:10] [V] [TRT] Tactic: 0x00000000000003e8 Time: 28.9811
[10/31/2023-17:54:10] [V] [TRT] Tactic: 0x00000000000003ea Time: 10.4798
[10/31/2023-17:54:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 28.7444
[10/31/2023-17:54:10] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 10.4798
[10/31/2023-17:54:10] [V] [TRT] *************** Autotuning Reformat: Float(8549376,1:4,15488,16) -> Float(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/maxpool/MaxPool_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:11] [V] [TRT] Tactic: 0x00000000000003e8 Time: 53.0626
[10/31/2023-17:54:11] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.34016
[10/31/2023-17:54:11] [V] [TRT] Tactic: 0x0000000000000000 Time: 53.0695
[10/31/2023-17:54:11] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 8.34016
[10/31/2023-17:54:11] [V] [TRT] *************** Autotuning Reformat: Float(1068672,534336:32,968,1) -> Float(34197504,534336,968,1) ***************
[10/31/2023-17:54:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/maxpool/MaxPool_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:11] [V] [TRT] Tactic: 0x00000000000003e8 Time: 28.7673
[10/31/2023-17:54:12] [V] [TRT] Tactic: 0x00000000000003ea Time: 10.4449
[10/31/2023-17:54:12] [V] [TRT] Tactic: 0x0000000000000000 Time: 28.7396
[10/31/2023-17:54:12] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 10.4449
[10/31/2023-17:54:12] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:12] [V] [TRT] *************** Autotuning Reformat: Float(34197504,534336,968,1) -> Float(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:12] [V] [TRT] *************** Autotuning Reformat: Float(1068672,534336:32,968,1) -> Float(34197504,534336,968,1) ***************
[10/31/2023-17:54:12] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /backbone/layer1/layer1.0/relu_1/Relu_output_0) (Reformat)
[10/31/2023-17:54:12] [V] [TRT] Tactic: 0x00000000000003e8 Time: 28.6792
[10/31/2023-17:54:12] [V] [TRT] Tactic: 0x00000000000003ea Time: 10.4696
[10/31/2023-17:54:13] [V] [TRT] Tactic: 0x0000000000000000 Time: 28.7574
[10/31/2023-17:54:13] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 10.4696
[10/31/2023-17:54:13] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:13] [V] [TRT] *************** Autotuning Reformat: Float(34197504,534336,968,1) -> Int8(34197504,534336,968,1) ***************
[10/31/2023-17:54:13] [V] [TRT] --------------- Timing Runner: /backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:13] [V] [TRT] Tactic: 0x00000000000003e8 Time: 6.69311
[10/31/2023-17:54:13] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.91758
[10/31/2023-17:54:13] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.68025
[10/31/2023-17:54:13] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 6.68025
[10/31/2023-17:54:13] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:54:13] [V] [TRT] *************** Autotuning Reformat: Float(34197504,534336,968,1) -> Int8(8549376,534336:4,968,1) ***************
[10/31/2023-17:54:13] [V] [TRT] --------------- Timing Runner: /backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:13] [V] [TRT] Tactic: 0x00000000000003e8 Time: 19.0241
[10/31/2023-17:54:13] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.16917
[10/31/2023-17:54:13] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.59079
[10/31/2023-17:54:13] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 3.59079
[10/31/2023-17:54:13] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:54:13] [V] [TRT] *************** Autotuning Reformat: Float(34197504,534336,968,1) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:13] [V] [TRT] --------------- Timing Runner: /backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:13] [V] [TRT] Tactic: 0x00000000000003e8 Time: 26.6731
[10/31/2023-17:54:13] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.1785
[10/31/2023-17:54:14] [V] [TRT] Tactic: 0x0000000000000000 Time: 26.5215
[10/31/2023-17:54:14] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 8.1785
[10/31/2023-17:54:14] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:14] [V] [TRT] *************** Autotuning Reformat: Float(1068672,534336:32,968,1) -> Int8(34197504,534336,968,1) ***************
[10/31/2023-17:54:14] [V] [TRT] --------------- Timing Runner: /backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:14] [V] [TRT] Tactic: 0x00000000000003e8 Time: 28.9657
[10/31/2023-17:54:14] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.15373
[10/31/2023-17:54:14] [V] [TRT] Tactic: 0x0000000000000000 Time: 28.9015
[10/31/2023-17:54:14] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 8.15373
[10/31/2023-17:54:14] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:14] [V] [TRT] *************** Autotuning Reformat: Float(1068672,534336:32,968,1) -> Int8(8549376,534336:4,968,1) ***************
[10/31/2023-17:54:14] [V] [TRT] --------------- Timing Runner: /backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:14] [V] [TRT] Tactic: 0x00000000000003e8 Time: 28.8715
[10/31/2023-17:54:15] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.9525
[10/31/2023-17:54:15] [V] [TRT] Tactic: 0x0000000000000000 Time: 28.7149
[10/31/2023-17:54:15] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 8.9525
[10/31/2023-17:54:15] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:15] [V] [TRT] *************** Autotuning Reformat: Float(1068672,534336:32,968,1) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:15] [V] [TRT] --------------- Timing Runner: /backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:15] [V] [TRT] Tactic: 0x00000000000003e8 Time: 43.3614
[10/31/2023-17:54:15] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.96993
[10/31/2023-17:54:16] [V] [TRT] Tactic: 0x0000000000000000 Time: 43.5206
[10/31/2023-17:54:16] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 8.96993
[10/31/2023-17:54:16] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:16] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:16] [V] [TRT] *************** Autotuning Reformat: Int8(34197504,534336,968,1) -> Int8(8549376,534336:4,968,1) ***************
[10/31/2023-17:54:16] [V] [TRT] *************** Autotuning Reformat: Int8(34197504,534336,968,1) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:16] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,534336:4,968,1) -> Int8(34197504,534336,968,1) ***************
[10/31/2023-17:54:16] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,534336:4,968,1) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:16] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,534336:32,968,1) -> Int8(34197504,534336,968,1) ***************
[10/31/2023-17:54:16] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,534336:32,968,1) -> Int8(8549376,534336:4,968,1) ***************
[10/31/2023-17:54:16] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:16] [V] [TRT] *************** Autotuning Reformat: Int8(34197504,534336,968,1) -> Int8(8549376,534336:4,968,1) ***************
[10/31/2023-17:54:16] [V] [TRT] *************** Autotuning Reformat: Int8(34197504,534336,968,1) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:16] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,534336:4,968,1) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:16] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,534336:32,968,1) -> Int8(8549376,534336:4,968,1) ***************
[10/31/2023-17:54:16] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:16] [V] [TRT] *************** Autotuning Reformat: Float(34197504,534336,968,1) -> Float(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:16] [V] [TRT] *************** Autotuning Reformat: Float(1068672,534336:32,968,1) -> Float(34197504,534336,968,1) ***************
[10/31/2023-17:54:16] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:16] [V] [TRT] *************** Autotuning Reformat: Float(34197504,534336,968,1) -> Int8(34197504,534336,968,1) ***************
[10/31/2023-17:54:16] [V] [TRT] --------------- Timing Runner: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:16] [V] [TRT] Tactic: 0x00000000000003e8 Time: 6.6865
[10/31/2023-17:54:16] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.92897
[10/31/2023-17:54:16] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.68938
[10/31/2023-17:54:16] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 6.6865
[10/31/2023-17:54:16] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[10/31/2023-17:54:16] [V] [TRT] *************** Autotuning Reformat: Float(34197504,534336,968,1) -> Int8(8549376,534336:4,968,1) ***************
[10/31/2023-17:54:16] [V] [TRT] --------------- Timing Runner: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:16] [V] [TRT] Tactic: 0x00000000000003e8 Time: 19.0676
[10/31/2023-17:54:16] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.19151
[10/31/2023-17:54:16] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.5996
[10/31/2023-17:54:16] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 3.5996
[10/31/2023-17:54:16] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:54:16] [V] [TRT] *************** Autotuning Reformat: Float(34197504,534336,968,1) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:16] [V] [TRT] --------------- Timing Runner: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:16] [V] [TRT] Tactic: 0x00000000000003e8 Time: 26.6041
[10/31/2023-17:54:17] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.18092
[10/31/2023-17:54:17] [V] [TRT] Tactic: 0x0000000000000000 Time: 26.6215
[10/31/2023-17:54:17] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 8.18092
[10/31/2023-17:54:17] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:17] [V] [TRT] *************** Autotuning Reformat: Float(1068672,534336:32,968,1) -> Int8(34197504,534336,968,1) ***************
[10/31/2023-17:54:17] [V] [TRT] --------------- Timing Runner: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:17] [V] [TRT] Tactic: 0x00000000000003e8 Time: 29.0717
[10/31/2023-17:54:17] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.13916
[10/31/2023-17:54:17] [V] [TRT] Tactic: 0x0000000000000000 Time: 28.9873
[10/31/2023-17:54:17] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 8.13916
[10/31/2023-17:54:17] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:17] [V] [TRT] *************** Autotuning Reformat: Float(1068672,534336:32,968,1) -> Int8(8549376,534336:4,968,1) ***************
[10/31/2023-17:54:17] [V] [TRT] --------------- Timing Runner: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:18] [V] [TRT] Tactic: 0x00000000000003e8 Time: 28.8867
[10/31/2023-17:54:18] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.96047
[10/31/2023-17:54:18] [V] [TRT] Tactic: 0x0000000000000000 Time: 28.9816
[10/31/2023-17:54:18] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 8.96047
[10/31/2023-17:54:18] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:18] [V] [TRT] *************** Autotuning Reformat: Float(1068672,534336:32,968,1) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:18] [V] [TRT] --------------- Timing Runner: /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:18] [V] [TRT] Tactic: 0x00000000000003e8 Time: 43.3235
[10/31/2023-17:54:18] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.9553
[10/31/2023-17:54:19] [V] [TRT] Tactic: 0x0000000000000000 Time: 43.7017
[10/31/2023-17:54:19] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 8.9553
[10/31/2023-17:54:19] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:19] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:19] [V] [TRT] *************** Autotuning Reformat: Int8(34197504,534336,968,1) -> Int8(8549376,534336:4,968,1) ***************
[10/31/2023-17:54:19] [V] [TRT] *************** Autotuning Reformat: Int8(34197504,534336,968,1) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:19] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,534336:4,968,1) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:19] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,534336:32,968,1) -> Int8(8549376,534336:4,968,1) ***************
[10/31/2023-17:54:19] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:19] [V] [TRT] *************** Autotuning Reformat: Int8(34197504,534336,968,1) -> Int8(8549376,534336:4,968,1) ***************
[10/31/2023-17:54:19] [V] [TRT] *************** Autotuning Reformat: Int8(34197504,534336,968,1) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:19] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,534336:4,968,1) -> Int8(34197504,534336,968,1) ***************
[10/31/2023-17:54:19] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,534336:4,968,1) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:54:19] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,534336:32,968,1) -> Int8(34197504,534336,968,1) ***************
[10/31/2023-17:54:19] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,534336:32,968,1) -> Int8(8549376,534336:4,968,1) ***************
[10/31/2023-17:54:19] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:19] [V] [TRT] *************** Autotuning Reformat: Float(17098752,133584,484,1) -> Float(17098752,1,61952,128) ***************
[10/31/2023-17:54:19] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:19] [V] [TRT] Tactic: 0x00000000000003e8 Time: 11.9637
[10/31/2023-17:54:19] [V] [TRT] Tactic: 0x00000000000003ea Time: 5.14149
[10/31/2023-17:54:19] [V] [TRT] Tactic: 0x0000000000000000 Time: 11.9757
[10/31/2023-17:54:19] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 5.14149
[10/31/2023-17:54:19] [V] [TRT] *************** Autotuning Reformat: Float(17098752,133584,484,1) -> Float(4274688,1:4,15488,32) ***************
[10/31/2023-17:54:19] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:19] [V] [TRT] Tactic: 0x00000000000003e8 Time: 12.4423
[10/31/2023-17:54:19] [V] [TRT] Tactic: 0x00000000000003ea Time: 5.15652
[10/31/2023-17:54:19] [V] [TRT] Tactic: 0x0000000000000000 Time: 12.4555
[10/31/2023-17:54:19] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 5.15652
[10/31/2023-17:54:19] [V] [TRT] *************** Autotuning Reformat: Float(17098752,133584,484,1) -> Float(534336,133584:32,484,1) ***************
[10/31/2023-17:54:19] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:20] [V] [TRT] Tactic: 0x00000000000003e8 Time: 19.0456
[10/31/2023-17:54:20] [V] [TRT] Tactic: 0x00000000000003ea Time: 5.13691
[10/31/2023-17:54:20] [V] [TRT] Tactic: 0x0000000000000000 Time: 19.4809
[10/31/2023-17:54:20] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 5.13691
[10/31/2023-17:54:20] [V] [TRT] *************** Autotuning Reformat: Float(17098752,133584,484,1) -> Float(1:4,133584,484,1) ***************
[10/31/2023-17:54:20] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:20] [V] [TRT] Tactic: 0x00000000000003e8 Time: 29.0318
[10/31/2023-17:54:20] [V] [TRT] Tactic: 0x00000000000003ea Time: 36.6715
[10/31/2023-17:54:21] [V] [TRT] Tactic: 0x0000000000000000 Time: 29.0325
[10/31/2023-17:54:21] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 29.0318
[10/31/2023-17:54:21] [V] [TRT] *************** Autotuning Reformat: Float(17098752,1,61952,128) -> Float(17098752,133584,484,1) ***************
[10/31/2023-17:54:21] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:21] [V] [TRT] Tactic: 0x00000000000003e8 Time: 14.3696
[10/31/2023-17:54:21] [V] [TRT] Tactic: 0x00000000000003ea Time: 5.23609
[10/31/2023-17:54:21] [V] [TRT] Tactic: 0x0000000000000000 Time: 14.4874
[10/31/2023-17:54:21] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 5.23609
[10/31/2023-17:54:21] [V] [TRT] *************** Autotuning Reformat: Float(17098752,1,61952,128) -> Float(4274688,1:4,15488,32) ***************
[10/31/2023-17:54:21] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:21] [V] [TRT] Tactic: 0x00000000000003e8 Time: 8.67827
[10/31/2023-17:54:21] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.1804
[10/31/2023-17:54:21] [V] [TRT] Tactic: 0x0000000000000000 Time: 8.6824
[10/31/2023-17:54:21] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 4.1804
[10/31/2023-17:54:21] [V] [TRT] *************** Autotuning Reformat: Float(17098752,1,61952,128) -> Float(534336,133584:32,484,1) ***************
[10/31/2023-17:54:21] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:22] [V] [TRT] Tactic: 0x00000000000003e8 Time: 29.7993
[10/31/2023-17:54:22] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.1664
[10/31/2023-17:54:22] [V] [TRT] Tactic: 0x0000000000000000 Time: 28.5262
[10/31/2023-17:54:22] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 4.1664
[10/31/2023-17:54:22] [V] [TRT] *************** Autotuning Reformat: Float(17098752,1,61952,128) -> Float(1:4,133584,484,1) ***************
[10/31/2023-17:54:22] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:22] [V] [TRT] Tactic: 0x00000000000003e8 Time: 54.1291
[10/31/2023-17:54:23] [V] [TRT] Tactic: 0x00000000000003ea Time: 36.883
[10/31/2023-17:54:23] [V] [TRT] Tactic: 0x0000000000000000 Time: 54.0287
[10/31/2023-17:54:23] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 36.883
[10/31/2023-17:54:23] [V] [TRT] *************** Autotuning Reformat: Float(4274688,1:4,15488,32) -> Float(17098752,133584,484,1) ***************
[10/31/2023-17:54:23] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:23] [V] [TRT] Tactic: 0x00000000000003e8 Time: 14.4394
[10/31/2023-17:54:23] [V] [TRT] Tactic: 0x00000000000003ea Time: 5.24344
[10/31/2023-17:54:23] [V] [TRT] Tactic: 0x0000000000000000 Time: 14.3923
[10/31/2023-17:54:23] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 5.24344
[10/31/2023-17:54:23] [V] [TRT] *************** Autotuning Reformat: Float(4274688,1:4,15488,32) -> Float(17098752,1,61952,128) ***************
[10/31/2023-17:54:23] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 8.58666
[10/31/2023-17:54:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.17683
[10/31/2023-17:54:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 8.58268
[10/31/2023-17:54:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 4.17683
[10/31/2023-17:54:24] [V] [TRT] *************** Autotuning Reformat: Float(4274688,1:4,15488,32) -> Float(534336,133584:32,484,1) ***************
[10/31/2023-17:54:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:24] [V] [TRT] Tactic: 0x00000000000003e8 Time: 29.0629
[10/31/2023-17:54:24] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.16463
[10/31/2023-17:54:24] [V] [TRT] Tactic: 0x0000000000000000 Time: 27.6764
[10/31/2023-17:54:24] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 4.16463
[10/31/2023-17:54:24] [V] [TRT] *************** Autotuning Reformat: Float(4274688,1:4,15488,32) -> Float(1:4,133584,484,1) ***************
[10/31/2023-17:54:24] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:25] [V] [TRT] Tactic: 0x00000000000003e8 Time: 54.0696
[10/31/2023-17:54:25] [V] [TRT] Tactic: 0x00000000000003ea Time: 36.8579
[10/31/2023-17:54:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 54.12
[10/31/2023-17:54:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 36.8579
[10/31/2023-17:54:26] [V] [TRT] *************** Autotuning Reformat: Float(534336,133584:32,484,1) -> Float(17098752,133584,484,1) ***************
[10/31/2023-17:54:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 14.3798
[10/31/2023-17:54:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 5.25105
[10/31/2023-17:54:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 14.4631
[10/31/2023-17:54:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 5.25105
[10/31/2023-17:54:26] [V] [TRT] *************** Autotuning Reformat: Float(534336,133584:32,484,1) -> Float(17098752,1,61952,128) ***************
[10/31/2023-17:54:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 8.59468
[10/31/2023-17:54:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.16925
[10/31/2023-17:54:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 8.59926
[10/31/2023-17:54:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 4.16925
[10/31/2023-17:54:26] [V] [TRT] *************** Autotuning Reformat: Float(534336,133584:32,484,1) -> Float(4274688,1:4,15488,32) ***************
[10/31/2023-17:54:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:26] [V] [TRT] Tactic: 0x00000000000003e8 Time: 10.4334
[10/31/2023-17:54:26] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.17567
[10/31/2023-17:54:26] [V] [TRT] Tactic: 0x0000000000000000 Time: 10.4328
[10/31/2023-17:54:26] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 4.17567
[10/31/2023-17:54:26] [V] [TRT] *************** Autotuning Reformat: Float(534336,133584:32,484,1) -> Float(1:4,133584,484,1) ***************
[10/31/2023-17:54:26] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:27] [V] [TRT] Tactic: 0x00000000000003e8 Time: 53.2103
[10/31/2023-17:54:27] [V] [TRT] Tactic: 0x00000000000003ea Time: 36.8928
[10/31/2023-17:54:28] [V] [TRT] Tactic: 0x0000000000000000 Time: 53.3261
[10/31/2023-17:54:28] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 36.8928
[10/31/2023-17:54:28] [V] [TRT] *************** Autotuning Reformat: Float(1:4,133584,484,1) -> Float(17098752,133584,484,1) ***************
[10/31/2023-17:54:28] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:28] [V] [TRT] Tactic: 0x00000000000003e8 Time: 8.78773
[10/31/2023-17:54:31] [V] [TRT] Tactic: 0x00000000000003ea Time: 354.551
[10/31/2023-17:54:31] [V] [TRT] Tactic: 0x0000000000000000 Time: 9.46366
[10/31/2023-17:54:31] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 8.78773
[10/31/2023-17:54:31] [V] [TRT] *************** Autotuning Reformat: Float(1:4,133584,484,1) -> Float(17098752,1,61952,128) ***************
[10/31/2023-17:54:31] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:31] [V] [TRT] Tactic: 0x00000000000003e8 Time: 21.0885
[10/31/2023-17:54:34] [V] [TRT] Tactic: 0x00000000000003ea Time: 354.397
[10/31/2023-17:54:34] [V] [TRT] Tactic: 0x0000000000000000 Time: 21.112
[10/31/2023-17:54:34] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 21.0885
[10/31/2023-17:54:34] [V] [TRT] *************** Autotuning Reformat: Float(1:4,133584,484,1) -> Float(4274688,1:4,15488,32) ***************
[10/31/2023-17:54:34] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:34] [V] [TRT] Tactic: 0x00000000000003e8 Time: 22.0938
[10/31/2023-17:54:37] [V] [TRT] Tactic: 0x00000000000003ea Time: 354.11
[10/31/2023-17:54:37] [V] [TRT] Tactic: 0x0000000000000000 Time: 22.016
[10/31/2023-17:54:37] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 22.016
[10/31/2023-17:54:38] [V] [TRT] *************** Autotuning Reformat: Float(1:4,133584,484,1) -> Float(534336,133584:32,484,1) ***************
[10/31/2023-17:54:38] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:38] [V] [TRT] Tactic: 0x00000000000003e8 Time: 31.1575
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 354.655
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 31.1496
[10/31/2023-17:54:41] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 31.1496
[10/31/2023-17:54:41] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(128,1,1,1) -> Float(128,1,128,128) ***************
[10/31/2023-17:54:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 107) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00877338
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0201394
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0107706
[10/31/2023-17:54:41] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00877338
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(128,1,1,1) -> Float(32,1:4,32,32) ***************
[10/31/2023-17:54:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 107) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00950429
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0231308
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00923686
[10/31/2023-17:54:41] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00923686
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(128,1,1,1) -> Float(4,1:32,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 107) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00861903
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0188886
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00916429
[10/31/2023-17:54:41] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00861903
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(128,1,1,1) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 107) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00908343
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0189223
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00899429
[10/31/2023-17:54:41] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00899429
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(128,1,128,128) -> Float(128,1,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 107) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00944086
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0189869
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00930171
[10/31/2023-17:54:41] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00930171
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(128,1,128,128) -> Float(32,1:4,32,32) ***************
[10/31/2023-17:54:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 107) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00908057
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0199154
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.009296
[10/31/2023-17:54:41] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00908057
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(128,1,128,128) -> Float(4,1:32,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 107) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00925372
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0192211
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00932629
[10/31/2023-17:54:41] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00925372
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(128,1,128,128) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 107) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0095491
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0196057
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00957897
[10/31/2023-17:54:41] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.0095491
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(32,1:4,32,32) -> Float(128,1,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 107) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00910628
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0187006
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00911657
[10/31/2023-17:54:41] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00910628
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(32,1:4,32,32) -> Float(128,1,128,128) ***************
[10/31/2023-17:54:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 107) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00915886
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0207687
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00917371
[10/31/2023-17:54:41] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00915886
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(32,1:4,32,32) -> Float(4,1:32,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 107) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00880511
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0190354
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00952171
[10/31/2023-17:54:41] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00880511
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(32,1:4,32,32) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 107) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00908086
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0202326
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00935114
[10/31/2023-17:54:41] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00908086
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(4,1:32,1,1) -> Float(128,1,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 107) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00859617
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0187737
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00937114
[10/31/2023-17:54:41] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00859617
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(4,1:32,1,1) -> Float(128,1,128,128) ***************
[10/31/2023-17:54:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 107) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00878091
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0192474
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00909714
[10/31/2023-17:54:41] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00878091
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(4,1:32,1,1) -> Float(32,1:4,32,32) ***************
[10/31/2023-17:54:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 107) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00876289
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0191074
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.009286
[10/31/2023-17:54:41] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00876289
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(4,1:32,1,1) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 107) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00914943
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0189206
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00914
[10/31/2023-17:54:41] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00914
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(128,1,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 107) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00880403
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0199914
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00918429
[10/31/2023-17:54:41] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00880403
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(128,1,128,128) ***************
[10/31/2023-17:54:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 107) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00899229
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0196251
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00955368
[10/31/2023-17:54:41] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00899229
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(32,1:4,32,32) ***************
[10/31/2023-17:54:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 107) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00897543
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0199114
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00937114
[10/31/2023-17:54:41] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00897543
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(4,1:32,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 107) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00886615
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.019604
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00883227
[10/31/2023-17:54:41] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00883227
[10/31/2023-17:54:41] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(128,1,1,1) -> Float(128,1,128,128) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(128,1,1,1) -> Float(32,1:4,32,32) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(128,1,1,1) -> Float(4,1:32,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(128,1,1,1) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(128,1,128,128) -> Float(128,1,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(128,1,128,128) -> Float(32,1:4,32,32) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(128,1,128,128) -> Float(4,1:32,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(128,1,128,128) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(32,1:4,32,32) -> Float(128,1,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(32,1:4,32,32) -> Float(128,1,128,128) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(32,1:4,32,32) -> Float(4,1:32,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(32,1:4,32,32) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(4,1:32,1,1) -> Float(128,1,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(4,1:32,1,1) -> Float(128,1,128,128) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(4,1:32,1,1) -> Float(32,1:4,32,32) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(4,1:32,1,1) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(128,1,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(128,1,128,128) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(32,1:4,32,32) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(4,1:32,1,1) ***************
[10/31/2023-17:54:41] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(17098752,133584,484,1) -> Float(17098752,1,61952,128) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(17098752,133584,484,1) -> Float(4274688,1:4,15488,32) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(17098752,1,61952,128) -> Float(17098752,133584,484,1) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(17098752,1,61952,128) -> Float(4274688,1:4,15488,32) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(4274688,1:4,15488,32) -> Float(17098752,133584,484,1) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(4274688,1:4,15488,32) -> Float(17098752,1,61952,128) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(534336,133584:32,484,1) -> Float(17098752,133584,484,1) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(534336,133584:32,484,1) -> Float(17098752,1,61952,128) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(534336,133584:32,484,1) -> Float(4274688,1:4,15488,32) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(1:4,133584,484,1) -> Float(17098752,133584,484,1) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(1:4,133584,484,1) -> Float(17098752,1,61952,128) ***************
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Float(1:4,133584,484,1) -> Float(4274688,1:4,15488,32) ***************
[10/31/2023-17:54:41] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Int8(17098752,133584,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:54:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/conv2/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003e8 Time: 10.9579
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.58066
[10/31/2023-17:54:41] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.94898
[10/31/2023-17:54:41] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 3.94898
[10/31/2023-17:54:41] [V] [TRT] *************** Autotuning Reformat: Int8(17098752,133584,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:54:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/conv2/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 18.575
[10/31/2023-17:54:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.20955
[10/31/2023-17:54:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 18.6297
[10/31/2023-17:54:42] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 4.20955
[10/31/2023-17:54:42] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,133584:4,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:54:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/conv2/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 8.29287
[10/31/2023-17:54:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 5.60211
[10/31/2023-17:54:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.16964
[10/31/2023-17:54:42] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.16964
[10/31/2023-17:54:42] [V] [TRT] *************** Autotuning Reformat: Int8(534336,133584:32,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:54:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.0/conv2/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.41938
[10/31/2023-17:54:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 5.60703
[10/31/2023-17:54:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.06395
[10/31/2023-17:54:42] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.06395
[10/31/2023-17:54:42] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:42] [V] [TRT] *************** Autotuning Reformat: Float(17098752,133584,484,1) -> Float(534336,133584:32,484,1) ***************
[10/31/2023-17:54:42] [V] [TRT] *************** Autotuning Reformat: Float(17098752,1,61952,128) -> Float(17098752,133584,484,1) ***************
[10/31/2023-17:54:42] [V] [TRT] *************** Autotuning Reformat: Float(17098752,1,61952,128) -> Float(534336,133584:32,484,1) ***************
[10/31/2023-17:54:42] [V] [TRT] *************** Autotuning Reformat: Float(4274688,1:4,15488,32) -> Float(17098752,133584,484,1) ***************
[10/31/2023-17:54:42] [V] [TRT] *************** Autotuning Reformat: Float(4274688,1:4,15488,32) -> Float(534336,133584:32,484,1) ***************
[10/31/2023-17:54:42] [V] [TRT] *************** Autotuning Reformat: Float(534336,133584:32,484,1) -> Float(17098752,133584,484,1) ***************
[10/31/2023-17:54:42] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:42] [V] [TRT] *************** Autotuning Reformat: Float(17098752,133584,484,1) -> Float(534336,133584:32,484,1) ***************
[10/31/2023-17:54:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /backbone/layer2/layer2.0/relu_1/Relu_output_0) (Reformat)
[10/31/2023-17:54:42] [V] [TRT] Tactic: 0x00000000000003e8 Time: 20.7029
[10/31/2023-17:54:42] [V] [TRT] Tactic: 0x00000000000003ea Time: 5.13798
[10/31/2023-17:54:42] [V] [TRT] Tactic: 0x0000000000000000 Time: 19.1484
[10/31/2023-17:54:42] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 5.13798
[10/31/2023-17:54:42] [V] [TRT] *************** Autotuning Reformat: Float(534336,133584:32,484,1) -> Float(17098752,133584,484,1) ***************
[10/31/2023-17:54:42] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /backbone/layer2/layer2.0/relu_1/Relu_output_0) (Reformat)
[10/31/2023-17:54:43] [V] [TRT] Tactic: 0x00000000000003e8 Time: 14.3726
[10/31/2023-17:54:43] [V] [TRT] Tactic: 0x00000000000003ea Time: 5.24177
[10/31/2023-17:54:43] [V] [TRT] Tactic: 0x0000000000000000 Time: 14.4394
[10/31/2023-17:54:43] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 5.24177
[10/31/2023-17:54:43] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:43] [V] [TRT] *************** Autotuning Reformat: Float(17098752,133584,484,1) -> Int8(17098752,133584,484,1) ***************
[10/31/2023-17:54:43] [V] [TRT] --------------- Timing Runner: /backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:43] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.35567
[10/31/2023-17:54:43] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.52768
[10/31/2023-17:54:43] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.35214
[10/31/2023-17:54:43] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 3.35214
[10/31/2023-17:54:43] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:54:43] [V] [TRT] *************** Autotuning Reformat: Float(17098752,133584,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:54:43] [V] [TRT] --------------- Timing Runner: /backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:43] [V] [TRT] Tactic: 0x00000000000003e8 Time: 9.53022
[10/31/2023-17:54:43] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.102
[10/31/2023-17:54:43] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.80654
[10/31/2023-17:54:43] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.80654
[10/31/2023-17:54:43] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:54:43] [V] [TRT] *************** Autotuning Reformat: Float(17098752,133584,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:54:43] [V] [TRT] --------------- Timing Runner: /backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:43] [V] [TRT] Tactic: 0x00000000000003e8 Time: 13.305
[10/31/2023-17:54:43] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.10392
[10/31/2023-17:54:43] [V] [TRT] Tactic: 0x0000000000000000 Time: 13.3003
[10/31/2023-17:54:43] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 4.10392
[10/31/2023-17:54:43] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:43] [V] [TRT] *************** Autotuning Reformat: Float(534336,133584:32,484,1) -> Int8(17098752,133584,484,1) ***************
[10/31/2023-17:54:43] [V] [TRT] --------------- Timing Runner: /backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:43] [V] [TRT] Tactic: 0x00000000000003e8 Time: 14.4532
[10/31/2023-17:54:43] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.08784
[10/31/2023-17:54:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 14.4178
[10/31/2023-17:54:44] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 4.08784
[10/31/2023-17:54:44] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:44] [V] [TRT] *************** Autotuning Reformat: Float(534336,133584:32,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:54:44] [V] [TRT] --------------- Timing Runner: /backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 14.4534
[10/31/2023-17:54:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.48851
[10/31/2023-17:54:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 14.4347
[10/31/2023-17:54:44] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 4.48851
[10/31/2023-17:54:44] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:44] [V] [TRT] *************** Autotuning Reformat: Float(534336,133584:32,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:54:44] [V] [TRT] --------------- Timing Runner: /backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 21.7597
[10/31/2023-17:54:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.49069
[10/31/2023-17:54:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 21.7904
[10/31/2023-17:54:44] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 4.49069
[10/31/2023-17:54:44] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:44] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:44] [V] [TRT] *************** Autotuning Reformat: Int8(17098752,133584,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:54:44] [V] [TRT] *************** Autotuning Reformat: Int8(17098752,133584,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:54:44] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,133584:4,484,1) -> Int8(17098752,133584,484,1) ***************
[10/31/2023-17:54:44] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:44] [V] [TRT] Tactic: 0x00000000000003e8 Time: 10.7856
[10/31/2023-17:54:44] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.70662
[10/31/2023-17:54:45] [V] [TRT] Tactic: 0x0000000000000000 Time: 10.7943
[10/31/2023-17:54:45] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 4.70662
[10/31/2023-17:54:45] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,133584:4,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:54:45] [V] [TRT] *************** Autotuning Reformat: Int8(534336,133584:32,484,1) -> Int8(17098752,133584,484,1) ***************
[10/31/2023-17:54:45] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:45] [V] [TRT] Tactic: 0x00000000000003e8 Time: 11.0624
[10/31/2023-17:54:45] [V] [TRT] Tactic: 0x00000000000003ea Time: 12.5311
[10/31/2023-17:54:45] [V] [TRT] Tactic: 0x0000000000000000 Time: 11.0697
[10/31/2023-17:54:45] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 11.0624
[10/31/2023-17:54:45] [V] [TRT] *************** Autotuning Reformat: Int8(534336,133584:32,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:54:45] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:45] [V] [TRT] *************** Autotuning Reformat: Int8(17098752,133584,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:54:45] [V] [TRT] *************** Autotuning Reformat: Int8(17098752,133584,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:54:45] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,133584:4,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:54:45] [V] [TRT] *************** Autotuning Reformat: Int8(534336,133584:32,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:54:45] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:45] [V] [TRT] *************** Autotuning Reformat: Float(17098752,133584,484,1) -> Float(534336,133584:32,484,1) ***************
[10/31/2023-17:54:45] [V] [TRT] *************** Autotuning Reformat: Float(534336,133584:32,484,1) -> Float(17098752,133584,484,1) ***************
[10/31/2023-17:54:45] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:45] [V] [TRT] *************** Autotuning Reformat: Float(17098752,133584,484,1) -> Float(534336,133584:32,484,1) ***************
[10/31/2023-17:54:45] [V] [TRT] *************** Autotuning Reformat: Float(534336,133584:32,484,1) -> Float(17098752,133584,484,1) ***************
[10/31/2023-17:54:45] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:45] [V] [TRT] *************** Autotuning Reformat: Float(17098752,133584,484,1) -> Int8(17098752,133584,484,1) ***************
[10/31/2023-17:54:45] [V] [TRT] --------------- Timing Runner: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:45] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.34189
[10/31/2023-17:54:45] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.52051
[10/31/2023-17:54:45] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.34682
[10/31/2023-17:54:45] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 3.34189
[10/31/2023-17:54:45] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[10/31/2023-17:54:45] [V] [TRT] *************** Autotuning Reformat: Float(17098752,133584,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:54:45] [V] [TRT] --------------- Timing Runner: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:45] [V] [TRT] Tactic: 0x00000000000003e8 Time: 9.52408
[10/31/2023-17:54:45] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.09935
[10/31/2023-17:54:45] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.80511
[10/31/2023-17:54:45] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.80511
[10/31/2023-17:54:45] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:54:45] [V] [TRT] *************** Autotuning Reformat: Float(17098752,133584,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:54:45] [V] [TRT] --------------- Timing Runner: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:45] [V] [TRT] Tactic: 0x00000000000003e8 Time: 13.3219
[10/31/2023-17:54:45] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.094
[10/31/2023-17:54:45] [V] [TRT] Tactic: 0x0000000000000000 Time: 13.3107
[10/31/2023-17:54:45] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 4.094
[10/31/2023-17:54:45] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:45] [V] [TRT] *************** Autotuning Reformat: Float(534336,133584:32,484,1) -> Int8(17098752,133584,484,1) ***************
[10/31/2023-17:54:45] [V] [TRT] --------------- Timing Runner: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:46] [V] [TRT] Tactic: 0x00000000000003e8 Time: 14.5155
[10/31/2023-17:54:46] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.08245
[10/31/2023-17:54:46] [V] [TRT] Tactic: 0x0000000000000000 Time: 14.3752
[10/31/2023-17:54:46] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 4.08245
[10/31/2023-17:54:46] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:46] [V] [TRT] *************** Autotuning Reformat: Float(534336,133584:32,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:54:46] [V] [TRT] --------------- Timing Runner: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:46] [V] [TRT] Tactic: 0x00000000000003e8 Time: 14.5197
[10/31/2023-17:54:46] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.48421
[10/31/2023-17:54:46] [V] [TRT] Tactic: 0x0000000000000000 Time: 14.39
[10/31/2023-17:54:46] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 4.48421
[10/31/2023-17:54:46] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:46] [V] [TRT] *************** Autotuning Reformat: Float(534336,133584:32,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:54:46] [V] [TRT] --------------- Timing Runner: /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:54:46] [V] [TRT] Tactic: 0x00000000000003e8 Time: 21.7607
[10/31/2023-17:54:46] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.48091
[10/31/2023-17:54:47] [V] [TRT] Tactic: 0x0000000000000000 Time: 21.6581
[10/31/2023-17:54:47] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 4.48091
[10/31/2023-17:54:47] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:47] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:47] [V] [TRT] *************** Autotuning Reformat: Float(17098752,133584,484,1) -> Int8(25648128,133584,484,1) ***************
[10/31/2023-17:54:47] [V] [TRT] --------------- Timing Runner: /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_clone_1 (Reformat)
[10/31/2023-17:54:47] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.33984
[10/31/2023-17:54:47] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.51803
[10/31/2023-17:54:47] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.34301
[10/31/2023-17:54:47] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 3.33984
[10/31/2023-17:54:47] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[10/31/2023-17:54:47] [V] [TRT] *************** Autotuning Reformat: Float(17098752,133584,484,1) -> Int8(6412032,133584:4,484,1) ***************
[10/31/2023-17:54:47] [V] [TRT] --------------- Timing Runner: /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_clone_1 (Reformat)
[10/31/2023-17:54:47] [V] [TRT] Tactic: 0x00000000000003e8 Time: 9.52347
[10/31/2023-17:54:47] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.09268
[10/31/2023-17:54:47] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.80789
[10/31/2023-17:54:47] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.80789
[10/31/2023-17:54:47] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:54:47] [V] [TRT] *************** Autotuning Reformat: Float(17098752,133584,484,1) -> Int8(801504,133584:32,484,1) ***************
[10/31/2023-17:54:47] [V] [TRT] --------------- Timing Runner: /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_clone_1 (Reformat)
[10/31/2023-17:54:47] [V] [TRT] Tactic: 0x00000000000003e8 Time: 13.3489
[10/31/2023-17:54:47] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.10319
[10/31/2023-17:54:47] [V] [TRT] Tactic: 0x0000000000000000 Time: 13.3494
[10/31/2023-17:54:47] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 4.10319
[10/31/2023-17:54:47] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:47] [V] [TRT] *************** Autotuning Reformat: Float(534336,133584:32,484,1) -> Int8(25648128,133584,484,1) ***************
[10/31/2023-17:54:47] [V] [TRT] --------------- Timing Runner: /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_clone_1 (Reformat)
[10/31/2023-17:54:47] [V] [TRT] Tactic: 0x00000000000003e8 Time: 14.4344
[10/31/2023-17:54:47] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.08326
[10/31/2023-17:54:47] [V] [TRT] Tactic: 0x0000000000000000 Time: 14.3864
[10/31/2023-17:54:47] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 4.08326
[10/31/2023-17:54:47] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:47] [V] [TRT] *************** Autotuning Reformat: Float(534336,133584:32,484,1) -> Int8(6412032,133584:4,484,1) ***************
[10/31/2023-17:54:47] [V] [TRT] --------------- Timing Runner: /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_clone_1 (Reformat)
[10/31/2023-17:54:48] [V] [TRT] Tactic: 0x00000000000003e8 Time: 14.42
[10/31/2023-17:54:48] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.48428
[10/31/2023-17:54:48] [V] [TRT] Tactic: 0x0000000000000000 Time: 14.4778
[10/31/2023-17:54:48] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 4.48428
[10/31/2023-17:54:48] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:48] [V] [TRT] *************** Autotuning Reformat: Float(534336,133584:32,484,1) -> Int8(801504,133584:32,484,1) ***************
[10/31/2023-17:54:48] [V] [TRT] --------------- Timing Runner: /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_clone_1 (Reformat)
[10/31/2023-17:54:48] [V] [TRT] Tactic: 0x00000000000003e8 Time: 21.7398
[10/31/2023-17:54:48] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.47663
[10/31/2023-17:54:48] [V] [TRT] Tactic: 0x0000000000000000 Time: 21.7276
[10/31/2023-17:54:48] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 4.47663
[10/31/2023-17:54:48] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:54:48] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:48] [V] [TRT] *************** Autotuning Reformat: Int8(17098752,133584,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:54:48] [V] [TRT] *************** Autotuning Reformat: Int8(17098752,133584,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:54:48] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,133584:4,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:54:48] [V] [TRT] *************** Autotuning Reformat: Int8(534336,133584:32,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:54:48] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:48] [V] [TRT] *************** Autotuning Reformat: Int8(17098752,133584,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:54:48] [V] [TRT] *************** Autotuning Reformat: Int8(17098752,133584,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:54:48] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,133584:4,484,1) -> Int8(17098752,133584,484,1) ***************
[10/31/2023-17:54:48] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,133584:4,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:54:48] [V] [TRT] *************** Autotuning Reformat: Int8(534336,133584:32,484,1) -> Int8(17098752,133584,484,1) ***************
[10/31/2023-17:54:48] [V] [TRT] *************** Autotuning Reformat: Int8(534336,133584:32,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:54:48] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:48] [V] [TRT] *************** Autotuning Reformat: Float(8549376,33396,242,1) -> Float(8549376,1,61952,256) ***************
[10/31/2023-17:54:48] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:48] [V] [TRT] Tactic: 0x00000000000003e8 Time: 6.22809
[10/31/2023-17:54:48] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.57543
[10/31/2023-17:54:48] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.23381
[10/31/2023-17:54:48] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.57543
[10/31/2023-17:54:48] [V] [TRT] *************** Autotuning Reformat: Float(8549376,33396,242,1) -> Float(2137344,1:4,15488,64) ***************
[10/31/2023-17:54:48] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:48] [V] [TRT] Tactic: 0x00000000000003e8 Time: 6.53839
[10/31/2023-17:54:48] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.57747
[10/31/2023-17:54:48] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.53564
[10/31/2023-17:54:48] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.57747
[10/31/2023-17:54:48] [V] [TRT] *************** Autotuning Reformat: Float(8549376,33396,242,1) -> Float(267168,33396:32,242,1) ***************
[10/31/2023-17:54:48] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 9.92468
[10/31/2023-17:54:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.57897
[10/31/2023-17:54:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 9.85178
[10/31/2023-17:54:49] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.57897
[10/31/2023-17:54:49] [V] [TRT] *************** Autotuning Reformat: Float(8549376,33396,242,1) -> Float(1:4,33396,242,1) ***************
[10/31/2023-17:54:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 14.1398
[10/31/2023-17:54:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 18.3256
[10/31/2023-17:54:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 14.1644
[10/31/2023-17:54:49] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 14.1398
[10/31/2023-17:54:49] [V] [TRT] *************** Autotuning Reformat: Float(8549376,1,61952,256) -> Float(8549376,33396,242,1) ***************
[10/31/2023-17:54:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 7.19143
[10/31/2023-17:54:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.63018
[10/31/2023-17:54:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.24784
[10/31/2023-17:54:49] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.63018
[10/31/2023-17:54:49] [V] [TRT] *************** Autotuning Reformat: Float(8549376,1,61952,256) -> Float(2137344,1:4,15488,64) ***************
[10/31/2023-17:54:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:49] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.34982
[10/31/2023-17:54:49] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.09164
[10/31/2023-17:54:49] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.34602
[10/31/2023-17:54:49] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.09164
[10/31/2023-17:54:49] [V] [TRT] *************** Autotuning Reformat: Float(8549376,1,61952,256) -> Float(267168,33396:32,242,1) ***************
[10/31/2023-17:54:49] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 14.9885
[10/31/2023-17:54:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.08844
[10/31/2023-17:54:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 14.801
[10/31/2023-17:54:50] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.08844
[10/31/2023-17:54:50] [V] [TRT] *************** Autotuning Reformat: Float(8549376,1,61952,256) -> Float(1:4,33396,242,1) ***************
[10/31/2023-17:54:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 28.9774
[10/31/2023-17:54:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 18.4028
[10/31/2023-17:54:50] [V] [TRT] Tactic: 0x0000000000000000 Time: 28.9555
[10/31/2023-17:54:50] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 18.4028
[10/31/2023-17:54:50] [V] [TRT] *************** Autotuning Reformat: Float(2137344,1:4,15488,64) -> Float(8549376,33396,242,1) ***************
[10/31/2023-17:54:50] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:50] [V] [TRT] Tactic: 0x00000000000003e8 Time: 7.1755
[10/31/2023-17:54:50] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.62632
[10/31/2023-17:54:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.21714
[10/31/2023-17:54:51] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.62632
[10/31/2023-17:54:51] [V] [TRT] *************** Autotuning Reformat: Float(2137344,1:4,15488,64) -> Float(8549376,1,61952,256) ***************
[10/31/2023-17:54:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.29204
[10/31/2023-17:54:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.09027
[10/31/2023-17:54:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.2841
[10/31/2023-17:54:51] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.09027
[10/31/2023-17:54:51] [V] [TRT] *************** Autotuning Reformat: Float(2137344,1:4,15488,64) -> Float(267168,33396:32,242,1) ***************
[10/31/2023-17:54:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 15.3152
[10/31/2023-17:54:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.0846
[10/31/2023-17:54:51] [V] [TRT] Tactic: 0x0000000000000000 Time: 15.0623
[10/31/2023-17:54:51] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.0846
[10/31/2023-17:54:51] [V] [TRT] *************** Autotuning Reformat: Float(2137344,1:4,15488,64) -> Float(1:4,33396,242,1) ***************
[10/31/2023-17:54:51] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:51] [V] [TRT] Tactic: 0x00000000000003e8 Time: 28.8984
[10/31/2023-17:54:51] [V] [TRT] Tactic: 0x00000000000003ea Time: 18.3765
[10/31/2023-17:54:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 28.9167
[10/31/2023-17:54:52] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 18.3765
[10/31/2023-17:54:52] [V] [TRT] *************** Autotuning Reformat: Float(267168,33396:32,242,1) -> Float(8549376,33396,242,1) ***************
[10/31/2023-17:54:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 7.17733
[10/31/2023-17:54:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.62711
[10/31/2023-17:54:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.18809
[10/31/2023-17:54:52] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.62711
[10/31/2023-17:54:52] [V] [TRT] *************** Autotuning Reformat: Float(267168,33396:32,242,1) -> Float(8549376,1,61952,256) ***************
[10/31/2023-17:54:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.29749
[10/31/2023-17:54:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.09406
[10/31/2023-17:54:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.29865
[10/31/2023-17:54:52] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.09406
[10/31/2023-17:54:52] [V] [TRT] *************** Autotuning Reformat: Float(267168,33396:32,242,1) -> Float(2137344,1:4,15488,64) ***************
[10/31/2023-17:54:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.20605
[10/31/2023-17:54:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.09434
[10/31/2023-17:54:52] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.20874
[10/31/2023-17:54:52] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.09434
[10/31/2023-17:54:52] [V] [TRT] *************** Autotuning Reformat: Float(267168,33396:32,242,1) -> Float(1:4,33396,242,1) ***************
[10/31/2023-17:54:52] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:52] [V] [TRT] Tactic: 0x00000000000003e8 Time: 29.1424
[10/31/2023-17:54:52] [V] [TRT] Tactic: 0x00000000000003ea Time: 18.4081
[10/31/2023-17:54:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 29.1394
[10/31/2023-17:54:53] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 18.4081
[10/31/2023-17:54:53] [V] [TRT] *************** Autotuning Reformat: Float(1:4,33396,242,1) -> Float(8549376,33396,242,1) ***************
[10/31/2023-17:54:53] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:53] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.18036
[10/31/2023-17:54:54] [V] [TRT] Tactic: 0x00000000000003ea Time: 176.975
[10/31/2023-17:54:54] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.33586
[10/31/2023-17:54:54] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 4.18036
[10/31/2023-17:54:54] [V] [TRT] *************** Autotuning Reformat: Float(1:4,33396,242,1) -> Float(8549376,1,61952,256) ***************
[10/31/2023-17:54:54] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:54] [V] [TRT] Tactic: 0x00000000000003e8 Time: 10.9118
[10/31/2023-17:54:56] [V] [TRT] Tactic: 0x00000000000003ea Time: 176.937
[10/31/2023-17:54:56] [V] [TRT] Tactic: 0x0000000000000000 Time: 10.9485
[10/31/2023-17:54:56] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 10.9118
[10/31/2023-17:54:56] [V] [TRT] *************** Autotuning Reformat: Float(1:4,33396,242,1) -> Float(2137344,1:4,15488,64) ***************
[10/31/2023-17:54:56] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:56] [V] [TRT] Tactic: 0x00000000000003e8 Time: 11.5213
[10/31/2023-17:54:58] [V] [TRT] Tactic: 0x00000000000003ea Time: 176.667
[10/31/2023-17:54:58] [V] [TRT] Tactic: 0x0000000000000000 Time: 11.5398
[10/31/2023-17:54:58] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 11.5213
[10/31/2023-17:54:58] [V] [TRT] *************** Autotuning Reformat: Float(1:4,33396,242,1) -> Float(267168,33396:32,242,1) ***************
[10/31/2023-17:54:58] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:54:58] [V] [TRT] Tactic: 0x00000000000003e8 Time: 14.9175
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003ea Time: 176.854
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 15.0753
[10/31/2023-17:54:59] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 14.9175
[10/31/2023-17:54:59] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:54:59] [V] [TRT] *************** Autotuning Reformat: Float(256,1,1,1) -> Float(256,1,256,256) ***************
[10/31/2023-17:54:59] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 181) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00943229
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0193206
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00955459
[10/31/2023-17:54:59] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00943229
[10/31/2023-17:54:59] [V] [TRT] *************** Autotuning Reformat: Float(256,1,1,1) -> Float(64,1:4,64,64) ***************
[10/31/2023-17:54:59] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 181) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00930914
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.019228
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00934229
[10/31/2023-17:54:59] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00930914
[10/31/2023-17:54:59] [V] [TRT] *************** Autotuning Reformat: Float(256,1,1,1) -> Float(8,1:32,1,1) ***************
[10/31/2023-17:54:59] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 181) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00878387
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0190817
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00916286
[10/31/2023-17:54:59] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00878387
[10/31/2023-17:54:59] [V] [TRT] *************** Autotuning Reformat: Float(256,1,1,1) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:54:59] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 181) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00904914
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0187246
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.009126
[10/31/2023-17:54:59] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00904914
[10/31/2023-17:54:59] [V] [TRT] *************** Autotuning Reformat: Float(256,1,256,256) -> Float(256,1,1,1) ***************
[10/31/2023-17:54:59] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 181) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00924971
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0193709
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0104245
[10/31/2023-17:54:59] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00924971
[10/31/2023-17:54:59] [V] [TRT] *************** Autotuning Reformat: Float(256,1,256,256) -> Float(64,1:4,64,64) ***************
[10/31/2023-17:54:59] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 181) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00935143
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0196749
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00955459
[10/31/2023-17:54:59] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00935143
[10/31/2023-17:54:59] [V] [TRT] *************** Autotuning Reformat: Float(256,1,256,256) -> Float(8,1:32,1,1) ***************
[10/31/2023-17:54:59] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 181) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.009258
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0197137
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00981669
[10/31/2023-17:54:59] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.009258
[10/31/2023-17:54:59] [V] [TRT] *************** Autotuning Reformat: Float(256,1,256,256) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:54:59] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 181) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00953021
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0197429
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00927114
[10/31/2023-17:54:59] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00927114
[10/31/2023-17:54:59] [V] [TRT] *************** Autotuning Reformat: Float(64,1:4,64,64) -> Float(256,1,1,1) ***************
[10/31/2023-17:54:59] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 181) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00908971
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0193457
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00900743
[10/31/2023-17:54:59] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00900743
[10/31/2023-17:54:59] [V] [TRT] *************** Autotuning Reformat: Float(64,1:4,64,64) -> Float(256,1,256,256) ***************
[10/31/2023-17:54:59] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 181) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0089038
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0192309
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.009318
[10/31/2023-17:54:59] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.0089038
[10/31/2023-17:54:59] [V] [TRT] *************** Autotuning Reformat: Float(64,1:4,64,64) -> Float(8,1:32,1,1) ***************
[10/31/2023-17:54:59] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 181) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00930286
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0194446
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00895543
[10/31/2023-17:54:59] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00895543
[10/31/2023-17:54:59] [V] [TRT] *************** Autotuning Reformat: Float(64,1:4,64,64) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:54:59] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 181) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00905286
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0195874
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00974964
[10/31/2023-17:54:59] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00905286
[10/31/2023-17:54:59] [V] [TRT] *************** Autotuning Reformat: Float(8,1:32,1,1) -> Float(256,1,1,1) ***************
[10/31/2023-17:54:59] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 181) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00882178
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.019112
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00909914
[10/31/2023-17:54:59] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00882178
[10/31/2023-17:54:59] [V] [TRT] *************** Autotuning Reformat: Float(8,1:32,1,1) -> Float(256,1,256,256) ***************
[10/31/2023-17:54:59] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 181) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00864968
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0198337
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00940343
[10/31/2023-17:54:59] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00864968
[10/31/2023-17:54:59] [V] [TRT] *************** Autotuning Reformat: Float(8,1:32,1,1) -> Float(64,1:4,64,64) ***************
[10/31/2023-17:54:59] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 181) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00896229
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0190726
[10/31/2023-17:54:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00926686
[10/31/2023-17:54:59] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00896229
[10/31/2023-17:54:59] [V] [TRT] *************** Autotuning Reformat: Float(8,1:32,1,1) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:54:59] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 181) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00872659
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0189097
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00948914
[10/31/2023-17:55:00] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00872659
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(256,1,1,1) ***************
[10/31/2023-17:55:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 181) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00890138
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0200137
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00915829
[10/31/2023-17:55:00] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00890138
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(256,1,256,256) ***************
[10/31/2023-17:55:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 181) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00916228
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0202171
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.009298
[10/31/2023-17:55:00] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00916228
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(64,1:4,64,64) ***************
[10/31/2023-17:55:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 181) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00922857
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0197914
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00979992
[10/31/2023-17:55:00] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00922857
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(8,1:32,1,1) ***************
[10/31/2023-17:55:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 181) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.009148
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0194869
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.009286
[10/31/2023-17:55:00] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.009148
[10/31/2023-17:55:00] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(256,1,1,1) -> Float(256,1,256,256) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(256,1,1,1) -> Float(64,1:4,64,64) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(256,1,1,1) -> Float(8,1:32,1,1) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(256,1,1,1) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(256,1,256,256) -> Float(256,1,1,1) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(256,1,256,256) -> Float(64,1:4,64,64) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(256,1,256,256) -> Float(8,1:32,1,1) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(256,1,256,256) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(64,1:4,64,64) -> Float(256,1,1,1) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(64,1:4,64,64) -> Float(256,1,256,256) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(64,1:4,64,64) -> Float(8,1:32,1,1) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(64,1:4,64,64) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(8,1:32,1,1) -> Float(256,1,1,1) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(8,1:32,1,1) -> Float(256,1,256,256) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(8,1:32,1,1) -> Float(64,1:4,64,64) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(8,1:32,1,1) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(256,1,1,1) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(256,1,256,256) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(64,1:4,64,64) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(8,1:32,1,1) ***************
[10/31/2023-17:55:00] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(8549376,33396,242,1) -> Float(8549376,1,61952,256) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(8549376,33396,242,1) -> Float(2137344,1:4,15488,64) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(8549376,1,61952,256) -> Float(8549376,33396,242,1) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(8549376,1,61952,256) -> Float(2137344,1:4,15488,64) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(2137344,1:4,15488,64) -> Float(8549376,33396,242,1) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(2137344,1:4,15488,64) -> Float(8549376,1,61952,256) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(267168,33396:32,242,1) -> Float(8549376,33396,242,1) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(267168,33396:32,242,1) -> Float(8549376,1,61952,256) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(267168,33396:32,242,1) -> Float(2137344,1:4,15488,64) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(1:4,33396,242,1) -> Float(8549376,33396,242,1) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(1:4,33396,242,1) -> Float(8549376,1,61952,256) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(1:4,33396,242,1) -> Float(2137344,1:4,15488,64) ***************
[10/31/2023-17:55:00] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,33396,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:55:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/conv2/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.48816
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.29494
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.98658
[10/31/2023-17:55:00] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.98658
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,33396,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:55:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/conv2/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 8.25002
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.1157
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 8.1269
[10/31/2023-17:55:00] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.1157
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,33396:4,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:55:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/conv2/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.15066
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.80667
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.38792
[10/31/2023-17:55:00] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.38792
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Int8(267168,33396:32,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:55:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.0/conv2/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.67249
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.8068
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.53696
[10/31/2023-17:55:00] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.53696
[10/31/2023-17:55:00] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(8549376,33396,242,1) -> Float(267168,33396:32,242,1) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(8549376,1,61952,256) -> Float(8549376,33396,242,1) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(8549376,1,61952,256) -> Float(267168,33396:32,242,1) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(2137344,1:4,15488,64) -> Float(8549376,33396,242,1) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(2137344,1:4,15488,64) -> Float(267168,33396:32,242,1) ***************
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(267168,33396:32,242,1) -> Float(8549376,33396,242,1) ***************
[10/31/2023-17:55:00] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(8549376,33396,242,1) -> Float(267168,33396:32,242,1) ***************
[10/31/2023-17:55:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /backbone/layer3/layer3.0/relu_1/Relu_output_0) (Reformat)
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 13.3014
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.57479
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 13.4745
[10/31/2023-17:55:00] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.57479
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(267168,33396:32,242,1) -> Float(8549376,33396,242,1) ***************
[10/31/2023-17:55:00] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /backbone/layer3/layer3.0/relu_1/Relu_output_0) (Reformat)
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 7.16695
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.62222
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.16005
[10/31/2023-17:55:00] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.62222
[10/31/2023-17:55:00] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(8549376,33396,242,1) -> Int8(8549376,33396,242,1) ***************
[10/31/2023-17:55:00] [V] [TRT] --------------- Timing Runner: /backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.67492
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.32139
[10/31/2023-17:55:00] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.67611
[10/31/2023-17:55:00] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 1.67492
[10/31/2023-17:55:00] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[10/31/2023-17:55:00] [V] [TRT] *************** Autotuning Reformat: Float(8549376,33396,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:55:00] [V] [TRT] --------------- Timing Runner: /backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.74607
[10/31/2023-17:55:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.04811
[10/31/2023-17:55:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.906496
[10/31/2023-17:55:01] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.906496
[10/31/2023-17:55:01] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:55:01] [V] [TRT] *************** Autotuning Reformat: Float(8549376,33396,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:55:01] [V] [TRT] --------------- Timing Runner: /backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 6.08431
[10/31/2023-17:55:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.04758
[10/31/2023-17:55:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.09523
[10/31/2023-17:55:01] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.04758
[10/31/2023-17:55:01] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:01] [V] [TRT] *************** Autotuning Reformat: Float(267168,33396:32,242,1) -> Int8(8549376,33396,242,1) ***************
[10/31/2023-17:55:01] [V] [TRT] --------------- Timing Runner: /backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 7.16393
[10/31/2023-17:55:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.04889
[10/31/2023-17:55:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.17675
[10/31/2023-17:55:01] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.04889
[10/31/2023-17:55:01] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:01] [V] [TRT] *************** Autotuning Reformat: Float(267168,33396:32,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:55:01] [V] [TRT] --------------- Timing Runner: /backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 7.17915
[10/31/2023-17:55:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.23963
[10/31/2023-17:55:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.17373
[10/31/2023-17:55:01] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.23963
[10/31/2023-17:55:01] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:01] [V] [TRT] *************** Autotuning Reformat: Float(267168,33396:32,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:55:01] [V] [TRT] --------------- Timing Runner: /backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 10.8444
[10/31/2023-17:55:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.24568
[10/31/2023-17:55:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 10.7892
[10/31/2023-17:55:01] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.24568
[10/31/2023-17:55:01] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:01] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:01] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,33396,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:55:01] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,33396,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:55:01] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,33396:4,242,1) -> Int8(8549376,33396,242,1) ***************
[10/31/2023-17:55:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.38902
[10/31/2023-17:55:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.35481
[10/31/2023-17:55:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.38363
[10/31/2023-17:55:01] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.35481
[10/31/2023-17:55:01] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,33396:4,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:55:01] [V] [TRT] *************** Autotuning Reformat: Int8(267168,33396:32,242,1) -> Int8(8549376,33396,242,1) ***************
[10/31/2023-17:55:01] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.36643
[10/31/2023-17:55:01] [V] [TRT] Tactic: 0x00000000000003ea Time: 6.24441
[10/31/2023-17:55:01] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.3642
[10/31/2023-17:55:01] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 5.3642
[10/31/2023-17:55:01] [V] [TRT] *************** Autotuning Reformat: Int8(267168,33396:32,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:55:01] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:01] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,33396,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:55:01] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,33396,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:55:01] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,33396:4,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:55:01] [V] [TRT] *************** Autotuning Reformat: Int8(267168,33396:32,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:55:01] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:01] [V] [TRT] *************** Autotuning Reformat: Float(8549376,33396,242,1) -> Float(267168,33396:32,242,1) ***************
[10/31/2023-17:55:01] [V] [TRT] *************** Autotuning Reformat: Float(267168,33396:32,242,1) -> Float(8549376,33396,242,1) ***************
[10/31/2023-17:55:01] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:01] [V] [TRT] *************** Autotuning Reformat: Float(8549376,33396,242,1) -> Float(267168,33396:32,242,1) ***************
[10/31/2023-17:55:01] [V] [TRT] *************** Autotuning Reformat: Float(267168,33396:32,242,1) -> Float(8549376,33396,242,1) ***************
[10/31/2023-17:55:01] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:01] [V] [TRT] *************** Autotuning Reformat: Float(8549376,33396,242,1) -> Int8(8549376,33396,242,1) ***************
[10/31/2023-17:55:01] [V] [TRT] --------------- Timing Runner: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:01] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.66512
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.31945
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.67296
[10/31/2023-17:55:02] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 1.66512
[10/31/2023-17:55:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[10/31/2023-17:55:02] [V] [TRT] *************** Autotuning Reformat: Float(8549376,33396,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:55:02] [V] [TRT] --------------- Timing Runner: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.74446
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.04739
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.904133
[10/31/2023-17:55:02] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.904133
[10/31/2023-17:55:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:55:02] [V] [TRT] *************** Autotuning Reformat: Float(8549376,33396,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:55:02] [V] [TRT] --------------- Timing Runner: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x00000000000003e8 Time: 6.0544
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.05416
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.23846
[10/31/2023-17:55:02] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.05416
[10/31/2023-17:55:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:02] [V] [TRT] *************** Autotuning Reformat: Float(267168,33396:32,242,1) -> Int8(8549376,33396,242,1) ***************
[10/31/2023-17:55:02] [V] [TRT] --------------- Timing Runner: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x00000000000003e8 Time: 7.22743
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.04845
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.20438
[10/31/2023-17:55:02] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.04845
[10/31/2023-17:55:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:02] [V] [TRT] *************** Autotuning Reformat: Float(267168,33396:32,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:55:02] [V] [TRT] --------------- Timing Runner: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x00000000000003e8 Time: 7.17232
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.24076
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.17347
[10/31/2023-17:55:02] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.24076
[10/31/2023-17:55:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:02] [V] [TRT] *************** Autotuning Reformat: Float(267168,33396:32,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:55:02] [V] [TRT] --------------- Timing Runner: /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x00000000000003e8 Time: 10.8169
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.24381
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x0000000000000000 Time: 10.8345
[10/31/2023-17:55:02] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.24381
[10/31/2023-17:55:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:02] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:02] [V] [TRT] *************** Autotuning Reformat: Float(8549376,33396,242,1) -> Int8(12824064,33396,242,1) ***************
[10/31/2023-17:55:02] [V] [TRT] --------------- Timing Runner: /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_clone_1 (Reformat)
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.67281
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.3188
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.67648
[10/31/2023-17:55:02] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 1.67281
[10/31/2023-17:55:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[10/31/2023-17:55:02] [V] [TRT] *************** Autotuning Reformat: Float(8549376,33396,242,1) -> Int8(3206016,33396:4,242,1) ***************
[10/31/2023-17:55:02] [V] [TRT] --------------- Timing Runner: /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_clone_1 (Reformat)
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.7516
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.05628
[10/31/2023-17:55:02] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.906149
[10/31/2023-17:55:02] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.906149
[10/31/2023-17:55:02] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:55:02] [V] [TRT] *************** Autotuning Reformat: Float(8549376,33396,242,1) -> Int8(400752,33396:32,242,1) ***************
[10/31/2023-17:55:02] [V] [TRT] --------------- Timing Runner: /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_clone_1 (Reformat)
[10/31/2023-17:55:03] [V] [TRT] Tactic: 0x00000000000003e8 Time: 6.12835
[10/31/2023-17:55:03] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.05283
[10/31/2023-17:55:03] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.12587
[10/31/2023-17:55:03] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.05283
[10/31/2023-17:55:03] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:03] [V] [TRT] *************** Autotuning Reformat: Float(267168,33396:32,242,1) -> Int8(12824064,33396,242,1) ***************
[10/31/2023-17:55:03] [V] [TRT] --------------- Timing Runner: /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_clone_1 (Reformat)
[10/31/2023-17:55:03] [V] [TRT] Tactic: 0x00000000000003e8 Time: 7.16685
[10/31/2023-17:55:03] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.05268
[10/31/2023-17:55:03] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.18318
[10/31/2023-17:55:03] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.05268
[10/31/2023-17:55:03] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:03] [V] [TRT] *************** Autotuning Reformat: Float(267168,33396:32,242,1) -> Int8(3206016,33396:4,242,1) ***************
[10/31/2023-17:55:03] [V] [TRT] --------------- Timing Runner: /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_clone_1 (Reformat)
[10/31/2023-17:55:03] [V] [TRT] Tactic: 0x00000000000003e8 Time: 7.1818
[10/31/2023-17:55:03] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.24264
[10/31/2023-17:55:03] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.19148
[10/31/2023-17:55:03] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.24264
[10/31/2023-17:55:03] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:03] [V] [TRT] *************** Autotuning Reformat: Float(267168,33396:32,242,1) -> Int8(400752,33396:32,242,1) ***************
[10/31/2023-17:55:03] [V] [TRT] --------------- Timing Runner: /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_clone_1 (Reformat)
[10/31/2023-17:55:03] [V] [TRT] Tactic: 0x00000000000003e8 Time: 10.8991
[10/31/2023-17:55:03] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.24168
[10/31/2023-17:55:03] [V] [TRT] Tactic: 0x0000000000000000 Time: 10.8831
[10/31/2023-17:55:03] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.24168
[10/31/2023-17:55:03] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:03] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:03] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,33396,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:55:03] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,33396,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:55:03] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,33396:4,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:55:03] [V] [TRT] *************** Autotuning Reformat: Int8(267168,33396:32,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:55:03] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:03] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,33396,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:55:03] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,33396,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:55:03] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,33396:4,242,1) -> Int8(8549376,33396,242,1) ***************
[10/31/2023-17:55:03] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,33396:4,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:55:03] [V] [TRT] *************** Autotuning Reformat: Int8(267168,33396:32,242,1) -> Int8(8549376,33396,242,1) ***************
[10/31/2023-17:55:03] [V] [TRT] *************** Autotuning Reformat: Int8(267168,33396:32,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:55:03] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:03] [V] [TRT] *************** Autotuning Reformat: Float(4274688,8349,121,1) -> Float(4274688,1,61952,512) ***************
[10/31/2023-17:55:03] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:03] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.33904
[10/31/2023-17:55:03] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.29972
[10/31/2023-17:55:03] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.34157
[10/31/2023-17:55:03] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.29972
[10/31/2023-17:55:03] [V] [TRT] *************** Autotuning Reformat: Float(4274688,8349,121,1) -> Float(1068672,1:4,15488,128) ***************
[10/31/2023-17:55:03] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:03] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.36472
[10/31/2023-17:55:03] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.29647
[10/31/2023-17:55:03] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.35891
[10/31/2023-17:55:03] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.29647
[10/31/2023-17:55:03] [V] [TRT] *************** Autotuning Reformat: Float(4274688,8349,121,1) -> Float(133584,8349:32,121,1) ***************
[10/31/2023-17:55:03] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:03] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.30052
[10/31/2023-17:55:03] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.29668
[10/31/2023-17:55:03] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.28766
[10/31/2023-17:55:03] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.29668
[10/31/2023-17:55:03] [V] [TRT] *************** Autotuning Reformat: Float(4274688,8349,121,1) -> Float(1:4,8349,121,1) ***************
[10/31/2023-17:55:03] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:03] [V] [TRT] Tactic: 0x00000000000003e8 Time: 7.07535
[10/31/2023-17:55:04] [V] [TRT] Tactic: 0x00000000000003ea Time: 9.13579
[10/31/2023-17:55:04] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.06938
[10/31/2023-17:55:04] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 7.06938
[10/31/2023-17:55:04] [V] [TRT] *************** Autotuning Reformat: Float(4274688,1,61952,512) -> Float(4274688,8349,121,1) ***************
[10/31/2023-17:55:04] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:04] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.33631
[10/31/2023-17:55:04] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.32566
[10/31/2023-17:55:04] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.34091
[10/31/2023-17:55:04] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.32566
[10/31/2023-17:55:04] [V] [TRT] *************** Autotuning Reformat: Float(4274688,1,61952,512) -> Float(1068672,1:4,15488,128) ***************
[10/31/2023-17:55:04] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:04] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.17974
[10/31/2023-17:55:04] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.0552
[10/31/2023-17:55:04] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.17265
[10/31/2023-17:55:04] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.0552
[10/31/2023-17:55:04] [V] [TRT] *************** Autotuning Reformat: Float(4274688,1,61952,512) -> Float(133584,8349:32,121,1) ***************
[10/31/2023-17:55:04] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:04] [V] [TRT] Tactic: 0x00000000000003e8 Time: 6.07336
[10/31/2023-17:55:04] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.05292
[10/31/2023-17:55:04] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.07253
[10/31/2023-17:55:04] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.05292
[10/31/2023-17:55:04] [V] [TRT] *************** Autotuning Reformat: Float(4274688,1,61952,512) -> Float(1:4,8349,121,1) ***************
[10/31/2023-17:55:04] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:04] [V] [TRT] Tactic: 0x00000000000003e8 Time: 14.4675
[10/31/2023-17:55:04] [V] [TRT] Tactic: 0x00000000000003ea Time: 9.16645
[10/31/2023-17:55:04] [V] [TRT] Tactic: 0x0000000000000000 Time: 14.4754
[10/31/2023-17:55:04] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 9.16645
[10/31/2023-17:55:04] [V] [TRT] *************** Autotuning Reformat: Float(1068672,1:4,15488,128) -> Float(4274688,8349,121,1) ***************
[10/31/2023-17:55:04] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:04] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.37012
[10/31/2023-17:55:04] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.32693
[10/31/2023-17:55:04] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.3624
[10/31/2023-17:55:04] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.32693
[10/31/2023-17:55:04] [V] [TRT] *************** Autotuning Reformat: Float(1068672,1:4,15488,128) -> Float(4274688,1,61952,512) ***************
[10/31/2023-17:55:04] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:04] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.14979
[10/31/2023-17:55:04] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.05497
[10/31/2023-17:55:04] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.15131
[10/31/2023-17:55:04] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.05497
[10/31/2023-17:55:04] [V] [TRT] *************** Autotuning Reformat: Float(1068672,1:4,15488,128) -> Float(133584,8349:32,121,1) ***************
[10/31/2023-17:55:04] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:04] [V] [TRT] Tactic: 0x00000000000003e8 Time: 6.07636
[10/31/2023-17:55:04] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.05376
[10/31/2023-17:55:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.08177
[10/31/2023-17:55:05] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.05376
[10/31/2023-17:55:05] [V] [TRT] *************** Autotuning Reformat: Float(1068672,1:4,15488,128) -> Float(1:4,8349,121,1) ***************
[10/31/2023-17:55:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 14.4841
[10/31/2023-17:55:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 9.16752
[10/31/2023-17:55:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 14.472
[10/31/2023-17:55:05] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 9.16752
[10/31/2023-17:55:05] [V] [TRT] *************** Autotuning Reformat: Float(133584,8349:32,121,1) -> Float(4274688,8349,121,1) ***************
[10/31/2023-17:55:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.36355
[10/31/2023-17:55:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.32601
[10/31/2023-17:55:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.3656
[10/31/2023-17:55:05] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.32601
[10/31/2023-17:55:05] [V] [TRT] *************** Autotuning Reformat: Float(133584,8349:32,121,1) -> Float(4274688,1,61952,512) ***************
[10/31/2023-17:55:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.15138
[10/31/2023-17:55:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.05578
[10/31/2023-17:55:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.15074
[10/31/2023-17:55:05] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.05578
[10/31/2023-17:55:05] [V] [TRT] *************** Autotuning Reformat: Float(133584,8349:32,121,1) -> Float(1068672,1:4,15488,128) ***************
[10/31/2023-17:55:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.61291
[10/31/2023-17:55:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.05798
[10/31/2023-17:55:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.61102
[10/31/2023-17:55:05] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.05798
[10/31/2023-17:55:05] [V] [TRT] *************** Autotuning Reformat: Float(133584,8349:32,121,1) -> Float(1:4,8349,121,1) ***************
[10/31/2023-17:55:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 14.4976
[10/31/2023-17:55:05] [V] [TRT] Tactic: 0x00000000000003ea Time: 9.20347
[10/31/2023-17:55:05] [V] [TRT] Tactic: 0x0000000000000000 Time: 14.4902
[10/31/2023-17:55:05] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 9.20347
[10/31/2023-17:55:05] [V] [TRT] *************** Autotuning Reformat: Float(1:4,8349,121,1) -> Float(4274688,8349,121,1) ***************
[10/31/2023-17:55:05] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:05] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.08847
[10/31/2023-17:55:06] [V] [TRT] Tactic: 0x00000000000003ea Time: 88.5618
[10/31/2023-17:55:06] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.17042
[10/31/2023-17:55:06] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 2.08847
[10/31/2023-17:55:06] [V] [TRT] *************** Autotuning Reformat: Float(1:4,8349,121,1) -> Float(4274688,1,61952,512) ***************
[10/31/2023-17:55:06] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:06] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.7035
[10/31/2023-17:55:07] [V] [TRT] Tactic: 0x00000000000003ea Time: 88.5458
[10/31/2023-17:55:07] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.71566
[10/31/2023-17:55:07] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 5.7035
[10/31/2023-17:55:07] [V] [TRT] *************** Autotuning Reformat: Float(1:4,8349,121,1) -> Float(1068672,1:4,15488,128) ***************
[10/31/2023-17:55:07] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:07] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.98838
[10/31/2023-17:55:08] [V] [TRT] Tactic: 0x00000000000003ea Time: 88.4954
[10/31/2023-17:55:08] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.99415
[10/31/2023-17:55:08] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 5.98838
[10/31/2023-17:55:08] [V] [TRT] *************** Autotuning Reformat: Float(1:4,8349,121,1) -> Float(133584,8349:32,121,1) ***************
[10/31/2023-17:55:08] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:08] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.36066
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 88.4361
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.33323
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 5.33323
[10/31/2023-17:55:09] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(512,1,1,1) -> Float(512,1,512,512) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 255) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00902615
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0188611
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00929514
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00902615
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(512,1,1,1) -> Float(128,1:4,128,128) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 255) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00908486
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.021361
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00911572
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00908486
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(512,1,1,1) -> Float(16,1:32,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 255) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00868975
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0207687
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00892316
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00868975
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(512,1,1,1) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 255) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.008988
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.019396
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00906343
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.008988
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(512,1,512,512) -> Float(512,1,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 255) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00900114
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0194
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00934628
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00900114
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(512,1,512,512) -> Float(128,1:4,128,128) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 255) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00913029
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0206029
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00931943
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00913029
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(512,1,512,512) -> Float(16,1:32,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 255) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00884571
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0210142
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00947914
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00884571
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(512,1,512,512) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 255) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0103638
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0199783
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0105783
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.0103638
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(128,1:4,128,128) -> Float(512,1,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 255) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00958689
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0208274
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00975695
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00958689
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(128,1:4,128,128) -> Float(512,1,512,512) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 255) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0100809
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0197966
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00919343
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00919343
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(128,1:4,128,128) -> Float(16,1:32,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 255) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.008988
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0206962
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00971703
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.008988
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(128,1:4,128,128) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 255) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0108689
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0197046
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.011008
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.0108689
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(16,1:32,1,1) -> Float(512,1,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 255) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00886615
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0204408
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00887691
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00886615
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(16,1:32,1,1) -> Float(512,1,512,512) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 255) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00882851
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0210018
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00923286
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00882851
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(16,1:32,1,1) -> Float(128,1:4,128,128) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 255) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.009326
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.02051
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.00912943
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.00912943
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(16,1:32,1,1) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 255) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0108418
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0201051
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0109623
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.0108418
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(512,1,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 255) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0100017
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0223693
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0104346
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.0100017
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(512,1,512,512) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 255) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.0104738
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0231249
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0106426
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.0104738
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(128,1:4,128,128) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 255) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00918914
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0218776
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0101778
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00918914
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(16,1:32,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat((Unnamed Layer* 255) [Shuffle]_output -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.00986453
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.0217659
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0103814
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 0.00986453
[10/31/2023-17:55:09] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(512,1,1,1) -> Float(512,1,512,512) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(512,1,1,1) -> Float(128,1:4,128,128) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(512,1,1,1) -> Float(16,1:32,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(512,1,1,1) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(512,1,512,512) -> Float(512,1,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(512,1,512,512) -> Float(128,1:4,128,128) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(512,1,512,512) -> Float(16,1:32,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(512,1,512,512) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(128,1:4,128,128) -> Float(512,1,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(128,1:4,128,128) -> Float(512,1,512,512) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(128,1:4,128,128) -> Float(16,1:32,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(128,1:4,128,128) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(16,1:32,1,1) -> Float(512,1,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(16,1:32,1,1) -> Float(512,1,512,512) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(16,1:32,1,1) -> Float(128,1:4,128,128) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(16,1:32,1,1) -> Float(1:4,1,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(512,1,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(512,1,512,512) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(128,1:4,128,128) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(1:4,1,1,1) -> Float(16,1:32,1,1) ***************
[10/31/2023-17:55:09] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(4274688,8349,121,1) -> Float(4274688,1,61952,512) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(4274688,8349,121,1) -> Float(1068672,1:4,15488,128) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(4274688,1,61952,512) -> Float(4274688,8349,121,1) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(4274688,1,61952,512) -> Float(1068672,1:4,15488,128) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(1068672,1:4,15488,128) -> Float(4274688,8349,121,1) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(1068672,1:4,15488,128) -> Float(4274688,1,61952,512) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(133584,8349:32,121,1) -> Float(4274688,8349,121,1) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(133584,8349:32,121,1) -> Float(4274688,1,61952,512) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(133584,8349:32,121,1) -> Float(1068672,1:4,15488,128) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(1:4,8349,121,1) -> Float(4274688,8349,121,1) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(1:4,8349,121,1) -> Float(4274688,1,61952,512) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(1:4,8349,121,1) -> Float(1068672,1:4,15488,128) ***************
[10/31/2023-17:55:09] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,8349,121,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/conv2/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.78037
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.16069
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.999922
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.999922
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,8349,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/conv2/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 3.0803
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.08501
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.0805
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.08501
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,8349:4,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/conv2/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.12691
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.40993
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.919863
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.919863
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Int8(133584,8349:32,121,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.0/conv2/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.871026
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.40888
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.27648
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.27648
[10/31/2023-17:55:09] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(4274688,8349,121,1) -> Float(133584,8349:32,121,1) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(4274688,1,61952,512) -> Float(4274688,8349,121,1) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(4274688,1,61952,512) -> Float(133584,8349:32,121,1) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(1068672,1:4,15488,128) -> Float(4274688,8349,121,1) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(1068672,1:4,15488,128) -> Float(133584,8349:32,121,1) ***************
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(133584,8349:32,121,1) -> Float(4274688,8349,121,1) ***************
[10/31/2023-17:55:09] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(4274688,8349,121,1) -> Float(133584,8349:32,121,1) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /backbone/layer4/layer4.0/relu_1/Relu_output_0) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.79002
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.30149
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.82346
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.30149
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(133584,8349:32,121,1) -> Float(4274688,8349,121,1) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /backbone/layer4/layer4.0/relu_1/Relu_output_0) (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.38272
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.33259
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.37665
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.33259
[10/31/2023-17:55:09] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(4274688,8349,121,1) -> Int8(4274688,8349,121,1) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: /backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.82656
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.36388
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.82937
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.36388
[10/31/2023-17:55:09] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(4274688,8349,121,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: /backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.41461
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.0416
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.490679
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.490679
[10/31/2023-17:55:09] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(4274688,8349,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: /backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.63712
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.04253
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.64196
[10/31/2023-17:55:09] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.04253
[10/31/2023-17:55:09] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:09] [V] [TRT] *************** Autotuning Reformat: Float(133584,8349:32,121,1) -> Int8(4274688,8349,121,1) ***************
[10/31/2023-17:55:09] [V] [TRT] --------------- Timing Runner: /backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.52548
[10/31/2023-17:55:09] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.03842
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.51853
[10/31/2023-17:55:10] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.03842
[10/31/2023-17:55:10] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Float(133584,8349:32,121,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] --------------- Timing Runner: /backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.83308
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.13869
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.83461
[10/31/2023-17:55:10] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.13869
[10/31/2023-17:55:10] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Float(133584,8349:32,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] --------------- Timing Runner: /backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.55415
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.14014
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.54672
[10/31/2023-17:55:10] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.14014
[10/31/2023-17:55:10] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:10] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,8349,121,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,8349,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,8349:4,121,1) -> Int8(4274688,8349,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.74518
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.19259
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.75162
[10/31/2023-17:55:10] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.19259
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,8349:4,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(133584,8349:32,121,1) -> Int8(4274688,8349,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.71698
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.898
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.72043
[10/31/2023-17:55:10] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 2.71698
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(133584,8349:32,121,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,8349,121,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,8349,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,8349:4,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(133584,8349:32,121,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Float(4274688,8349,121,1) -> Float(133584,8349:32,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Float(133584,8349:32,121,1) -> Float(4274688,8349,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Float(4274688,8349,121,1) -> Int8(4274688,8349,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] --------------- Timing Runner: /neck/detect1/conv1/conv/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.82542
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.36166
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.82184
[10/31/2023-17:55:10] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.36166
[10/31/2023-17:55:10] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Float(4274688,8349,121,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] --------------- Timing Runner: /neck/detect1/conv1/conv/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.41206
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.03755
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.493243
[10/31/2023-17:55:10] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.493243
[10/31/2023-17:55:10] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Float(4274688,8349,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] --------------- Timing Runner: /neck/detect1/conv1/conv/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.64517
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.04001
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.64142
[10/31/2023-17:55:10] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.04001
[10/31/2023-17:55:10] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Float(133584,8349:32,121,1) -> Int8(4274688,8349,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] --------------- Timing Runner: /neck/detect1/conv1/conv/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.524
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.04156
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.52272
[10/31/2023-17:55:10] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.04156
[10/31/2023-17:55:10] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Float(133584,8349:32,121,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] --------------- Timing Runner: /neck/detect1/conv1/conv/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.83164
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.13878
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.83702
[10/31/2023-17:55:10] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.13878
[10/31/2023-17:55:10] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Float(133584,8349:32,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] --------------- Timing Runner: /neck/detect1/conv1/conv/_input_quantizer/QuantizeLinear (Reformat)
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.54263
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.13636
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.54189
[10/31/2023-17:55:10] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.13636
[10/31/2023-17:55:10] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:10] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,8349,121,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,8349,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,8349:4,121,1) -> Int8(4274688,8349,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,8349:4,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(133584,8349:32,121,1) -> Int8(4274688,8349,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(133584,8349:32,121,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,8349,121,1) -> Int8(534336,8349:4,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect1/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.39243
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.588247
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.506363
[10/31/2023-17:55:10] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.506363
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,8349,121,1) -> Int8(66792,8349:32,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect1/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.54699
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.54981
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.54771
[10/31/2023-17:55:10] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.54981
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(534336,8349:4,121,1) -> Int8(2137344,8349,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect1/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.37753
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.60421
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.38051
[10/31/2023-17:55:10] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.60421
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(534336,8349:4,121,1) -> Int8(66792,8349:32,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect1/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.07088
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.714866
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.37024
[10/31/2023-17:55:10] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.37024
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(66792,8349:32,121,1) -> Int8(2137344,8349,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect1/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.36525
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.43945
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.36183
[10/31/2023-17:55:10] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.36183
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(66792,8349:32,121,1) -> Int8(534336,8349:4,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect1/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.442697
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.712549
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.144279
[10/31/2023-17:55:10] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.144279
[10/31/2023-17:55:10] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,8349,121,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,8349,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,8349:4,121,1) -> Int8(4274688,8349,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,8349:4,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(133584,8349:32,121,1) -> Int8(4274688,8349,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(133584,8349:32,121,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,8349,121,1) -> Int8(534336,8349:4,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,8349,121,1) -> Int8(66792,8349:32,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(534336,8349:4,121,1) -> Int8(2137344,8349,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(534336,8349:4,121,1) -> Int8(66792,8349:32,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(66792,8349:32,121,1) -> Int8(2137344,8349,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(66792,8349:32,121,1) -> Int8(534336,8349:4,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,8349,121,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,8349,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,8349:4,121,1) -> Int8(4274688,8349,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,8349:4,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(133584,8349:32,121,1) -> Int8(4274688,8349,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(133584,8349:32,121,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,8349,121,1) -> Int8(534336,8349:4,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /neck/conv1/conv/_input_quantizer/QuantizeLinear_output_0) (Reformat)
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.39704
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.589481
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.506459
[10/31/2023-17:55:10] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.506459
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,8349,121,1) -> Int8(66792,8349:32,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /neck/conv1/conv/_input_quantizer/QuantizeLinear_output_0) (Reformat)
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.54753
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.548695
[10/31/2023-17:55:10] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.54454
[10/31/2023-17:55:10] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.548695
[10/31/2023-17:55:10] [V] [TRT] *************** Autotuning Reformat: Int8(534336,8349:4,121,1) -> Int8(2137344,8349,121,1) ***************
[10/31/2023-17:55:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /neck/conv1/conv/_input_quantizer/QuantizeLinear_output_0) (Reformat)
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.38037
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.608091
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.38001
[10/31/2023-17:55:11] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.608091
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(534336,8349:4,121,1) -> Int8(66792,8349:32,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /neck/conv1/conv/_input_quantizer/QuantizeLinear_output_0) (Reformat)
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.0681
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.714281
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.368791
[10/31/2023-17:55:11] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.368791
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(66792,8349:32,121,1) -> Int8(2137344,8349,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /neck/conv1/conv/_input_quantizer/QuantizeLinear_output_0) (Reformat)
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.36407
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.45887
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.36203
[10/31/2023-17:55:11] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.36203
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(66792,8349:32,121,1) -> Int8(534336,8349:4,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /neck/conv1/conv/_input_quantizer/QuantizeLinear_output_0) (Reformat)
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.441486
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.711195
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.143735
[10/31/2023-17:55:11] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.143735
[10/31/2023-17:55:11] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,8349,121,1) -> Int8(534336,8349:4,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,8349,121,1) -> Int8(66792,8349:32,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(534336,8349:4,121,1) -> Int8(2137344,8349,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(534336,8349:4,121,1) -> Int8(66792,8349:32,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(66792,8349:32,121,1) -> Int8(2137344,8349,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(66792,8349:32,121,1) -> Int8(534336,8349:4,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,8349,121,1) -> Int8(534336,8349:4,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,8349,121,1) -> Int8(66792,8349:32,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(534336,8349:4,121,1) -> Int8(2137344,8349,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(534336,8349:4,121,1) -> Int8(66792,8349:32,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(66792,8349:32,121,1) -> Int8(2137344,8349,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(66792,8349:32,121,1) -> Int8(534336,8349:4,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,8349,121,1) -> Int8(33396,8349:32,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/Resize_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.75355
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.27797
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.762661
[10/31/2023-17:55:11] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.27797
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(267168,8349:4,121,1) -> Int8(1068672,8349,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/Resize_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.682464
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.312151
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.683392
[10/31/2023-17:55:11] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.312151
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(267168,8349:4,121,1) -> Int8(33396,8349:32,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/Resize_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.528914
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.362185
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.139033
[10/31/2023-17:55:11] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.139033
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(33396,8349:32,121,1) -> Int8(1068672,8349,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/Resize_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.681641
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.752626
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.678007
[10/31/2023-17:55:11] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.678007
[10/31/2023-17:55:11] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,8349,121,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,8349,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,8349:4,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(133584,8349:32,121,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:55:11] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,33396,242,1) -> Int8(12824064,33396,242,1) ***************
[10/31/2023-17:55:11] [V] [TRT] --------------- Timing Runner: /neck/Concat_/neck/Resize_output_0_clone_0 copy (Reformat)
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.15306
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.36633
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.15542
[10/31/2023-17:55:11] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.36633
[10/31/2023-17:55:11] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,33396,242,1) -> Int8(3206016,33396:4,242,1) ***************
[10/31/2023-17:55:11] [V] [TRT] --------------- Timing Runner: /neck/Concat_/neck/Resize_output_0_clone_0 copy (Reformat)
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.75281
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.15544
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.00266
[10/31/2023-17:55:11] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.00266
[10/31/2023-17:55:11] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,33396,242,1) -> Int8(400752,33396:32,242,1) ***************
[10/31/2023-17:55:11] [V] [TRT] --------------- Timing Runner: /neck/Concat_/neck/Resize_output_0_clone_0 copy (Reformat)
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.20208
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.06544
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.19855
[10/31/2023-17:55:11] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.06544
[10/31/2023-17:55:11] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(133584,33396:32,242,1) -> Int8(12824064,33396,242,1) ***************
[10/31/2023-17:55:11] [V] [TRT] --------------- Timing Runner: /neck/Concat_/neck/Resize_output_0_clone_0 copy (Reformat)
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.71178
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.12459
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.70678
[10/31/2023-17:55:11] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.70678
[10/31/2023-17:55:11] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(133584,33396:32,242,1) -> Int8(3206016,33396:4,242,1) ***************
[10/31/2023-17:55:11] [V] [TRT] --------------- Timing Runner: /neck/Concat_/neck/Resize_output_0_clone_0 copy (Reformat)
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.841102
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.41017
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.274885
[10/31/2023-17:55:11] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.274885
[10/31/2023-17:55:11] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(133584,33396:32,242,1) -> Int8(400752,33396:32,242,1) ***************
[10/31/2023-17:55:11] [V] [TRT] --------------- Timing Runner: /neck/Concat_/neck/Resize_output_0_clone_0 copy (Reformat)
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.1814
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.906775
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.43669
[10/31/2023-17:55:11] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.43669
[10/31/2023-17:55:11] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:55:11] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(12824064,33396,242,1) -> Int8(3206016,33396:4,242,1) ***************
[10/31/2023-17:55:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003e8 Time: 8.22845
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.43743
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.97605
[10/31/2023-17:55:11] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.97605
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(12824064,33396,242,1) -> Int8(400752,33396:32,242,1) ***************
[10/31/2023-17:55:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003e8 Time: 11.9415
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.16466
[10/31/2023-17:55:11] [V] [TRT] Tactic: 0x0000000000000000 Time: 12.3354
[10/31/2023-17:55:11] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 3.16466
[10/31/2023-17:55:11] [V] [TRT] *************** Autotuning Reformat: Int8(3206016,33396:4,242,1) -> Int8(12824064,33396,242,1) ***************
[10/31/2023-17:55:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x00000000000003e8 Time: 8.13029
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.53373
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x0000000000000000 Time: 8.13351
[10/31/2023-17:55:12] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 3.53373
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(3206016,33396:4,242,1) -> Int8(400752,33396:32,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x00000000000003e8 Time: 6.24243
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.20242
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.4463
[10/31/2023-17:55:12] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.4463
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(400752,33396:32,242,1) -> Int8(12824064,33396,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x00000000000003e8 Time: 8.07333
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x00000000000003ea Time: 9.37187
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x0000000000000000 Time: 8.09305
[10/31/2023-17:55:12] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 8.07333
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(400752,33396:32,242,1) -> Int8(3206016,33396:4,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.49637
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x00000000000003ea Time: 4.20049
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.796905
[10/31/2023-17:55:12] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.796905
[10/31/2023-17:55:12] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,33396,242,1) -> Int8(1068672,33396:4,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect2/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.75215
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.15568
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.999406
[10/31/2023-17:55:12] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.999406
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,33396,242,1) -> Int8(133584,33396:32,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect2/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.01155
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.06106
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.91641
[10/31/2023-17:55:12] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.06106
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,33396:4,242,1) -> Int8(4274688,33396,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect2/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.72216
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.18898
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.7083
[10/31/2023-17:55:12] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.18898
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,33396:4,242,1) -> Int8(133584,33396:32,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect2/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.0839
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.40694
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.505353
[10/31/2023-17:55:12] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.505353
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(133584,33396:32,242,1) -> Int8(4274688,33396,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect2/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.70297
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.11797
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.70304
[10/31/2023-17:55:12] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 2.70297
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(133584,33396:32,242,1) -> Int8(1068672,33396:4,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect2/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.842341
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.41
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.275387
[10/31/2023-17:55:12] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.275387
[10/31/2023-17:55:12] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,33396,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,33396,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,33396:4,242,1) -> Int8(8549376,33396,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,33396:4,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(267168,33396:32,242,1) -> Int8(8549376,33396,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(267168,33396:32,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,33396,242,1) -> Int8(1068672,33396:4,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,33396,242,1) -> Int8(133584,33396:32,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,33396:4,242,1) -> Int8(4274688,33396,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,33396:4,242,1) -> Int8(133584,33396:32,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(133584,33396:32,242,1) -> Int8(4274688,33396,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(133584,33396:32,242,1) -> Int8(1068672,33396:4,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,33396,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,33396,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,33396:4,242,1) -> Int8(8549376,33396,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,33396:4,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(267168,33396:32,242,1) -> Int8(8549376,33396,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(267168,33396:32,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,33396,242,1) -> Int8(1068672,33396:4,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /neck/conv2/conv/_input_quantizer/QuantizeLinear_output_0) (Reformat)
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.74879
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.15554
[10/31/2023-17:55:12] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.997755
[10/31/2023-17:55:12] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.997755
[10/31/2023-17:55:12] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,33396,242,1) -> Int8(133584,33396:32,242,1) ***************
[10/31/2023-17:55:12] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /neck/conv2/conv/_input_quantizer/QuantizeLinear_output_0) (Reformat)
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.18624
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.06286
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.95611
[10/31/2023-17:55:13] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.06286
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,33396:4,242,1) -> Int8(4274688,33396,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /neck/conv2/conv/_input_quantizer/QuantizeLinear_output_0) (Reformat)
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.71145
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.19588
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.71237
[10/31/2023-17:55:13] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 1.19588
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,33396:4,242,1) -> Int8(133584,33396:32,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /neck/conv2/conv/_input_quantizer/QuantizeLinear_output_0) (Reformat)
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.08837
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.40695
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.503163
[10/31/2023-17:55:13] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.503163
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(133584,33396:32,242,1) -> Int8(4274688,33396,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /neck/conv2/conv/_input_quantizer/QuantizeLinear_output_0) (Reformat)
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.69818
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.16945
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.70507
[10/31/2023-17:55:13] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 2.69818
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(133584,33396:32,242,1) -> Int8(1068672,33396:4,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(<in> -> /neck/conv2/conv/_input_quantizer/QuantizeLinear_output_0) (Reformat)
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.838944
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.41283
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.274258
[10/31/2023-17:55:13] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.274258
[10/31/2023-17:55:13] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,33396,242,1) -> Int8(1068672,33396:4,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,33396,242,1) -> Int8(133584,33396:32,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,33396:4,242,1) -> Int8(4274688,33396,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,33396:4,242,1) -> Int8(133584,33396:32,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(133584,33396:32,242,1) -> Int8(4274688,33396,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(133584,33396:32,242,1) -> Int8(1068672,33396:4,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,33396,242,1) -> Int8(1068672,33396:4,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,33396,242,1) -> Int8(133584,33396:32,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,33396:4,242,1) -> Int8(4274688,33396,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(1068672,33396:4,242,1) -> Int8(133584,33396:32,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(133584,33396:32,242,1) -> Int8(4274688,33396,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(133584,33396:32,242,1) -> Int8(1068672,33396:4,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,33396,242,1) -> Int8(66792,33396:32,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/Resize_1_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003e8 Time: 2.0206
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.53995
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.16352
[10/31/2023-17:55:13] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.53995
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(534336,33396:4,242,1) -> Int8(2137344,33396,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/Resize_1_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.36294
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.602469
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.36236
[10/31/2023-17:55:13] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.602469
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(534336,33396:4,242,1) -> Int8(66792,33396:32,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/Resize_1_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.04987
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.71072
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.243095
[10/31/2023-17:55:13] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.243095
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(66792,33396:32,242,1) -> Int8(2137344,33396,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/Resize_1_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.35911
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.55508
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.35819
[10/31/2023-17:55:13] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.35819
[10/31/2023-17:55:13] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,33396,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,33396,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,33396:4,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(267168,33396:32,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:55:13] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,133584,484,1) -> Int8(25648128,133584,484,1) ***************
[10/31/2023-17:55:13] [V] [TRT] --------------- Timing Runner: /neck/Concat_1_/neck/Resize_1_output_0_clone_0 copy (Reformat)
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.808247
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.7035
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.807337
[10/31/2023-17:55:13] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.807337
[10/31/2023-17:55:13] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,133584,484,1) -> Int8(6412032,133584:4,484,1) ***************
[10/31/2023-17:55:13] [V] [TRT] --------------- Timing Runner: /neck/Concat_1_/neck/Resize_1_output_0_clone_0 copy (Reformat)
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.47241
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.2915
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.98494
[10/31/2023-17:55:13] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.98494
[10/31/2023-17:55:13] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,133584,484,1) -> Int8(801504,133584:32,484,1) ***************
[10/31/2023-17:55:13] [V] [TRT] --------------- Timing Runner: /neck/Concat_1_/neck/Resize_1_output_0_clone_0 copy (Reformat)
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003e8 Time: 9.38279
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.1147
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x0000000000000000 Time: 9.34292
[10/31/2023-17:55:13] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.1147
[10/31/2023-17:55:13] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(267168,133584:32,484,1) -> Int8(25648128,133584,484,1) ***************
[10/31/2023-17:55:13] [V] [TRT] --------------- Timing Runner: /neck/Concat_1_/neck/Resize_1_output_0_clone_0 copy (Reformat)
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.63937
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003ea Time: 6.22268
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.62768
[10/31/2023-17:55:13] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 5.62768
[10/31/2023-17:55:13] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(267168,133584:32,484,1) -> Int8(6412032,133584:4,484,1) ***************
[10/31/2023-17:55:13] [V] [TRT] --------------- Timing Runner: /neck/Concat_1_/neck/Resize_1_output_0_clone_0 copy (Reformat)
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.68419
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.79759
[10/31/2023-17:55:13] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.534021
[10/31/2023-17:55:13] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.534021
[10/31/2023-17:55:13] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:55:13] [V] [TRT] *************** Autotuning Reformat: Int8(267168,133584:32,484,1) -> Int8(801504,133584:32,484,1) ***************
[10/31/2023-17:55:13] [V] [TRT] --------------- Timing Runner: /neck/Concat_1_/neck/Resize_1_output_0_clone_0 copy (Reformat)
[10/31/2023-17:55:14] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.38466
[10/31/2023-17:55:14] [V] [TRT] Tactic: 0x00000000000003ea Time: 1.79261
[10/31/2023-17:55:14] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.788014
[10/31/2023-17:55:14] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.788014
[10/31/2023-17:55:14] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[10/31/2023-17:55:14] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:14] [V] [TRT] *************** Autotuning Reformat: Int8(25648128,133584,484,1) -> Int8(6412032,133584:4,484,1) ***************
[10/31/2023-17:55:14] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:14] [V] [TRT] Tactic: 0x00000000000003e8 Time: 16.3244
[10/31/2023-17:55:14] [V] [TRT] Tactic: 0x00000000000003ea Time: 6.85179
[10/31/2023-17:55:14] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.91109
[10/31/2023-17:55:14] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 5.91109
[10/31/2023-17:55:14] [V] [TRT] *************** Autotuning Reformat: Int8(25648128,133584,484,1) -> Int8(801504,133584:32,484,1) ***************
[10/31/2023-17:55:14] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:14] [V] [TRT] Tactic: 0x00000000000003e8 Time: 22.4982
[10/31/2023-17:55:14] [V] [TRT] Tactic: 0x00000000000003ea Time: 6.26916
[10/31/2023-17:55:14] [V] [TRT] Tactic: 0x0000000000000000 Time: 19.2402
[10/31/2023-17:55:14] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 6.26916
[10/31/2023-17:55:14] [V] [TRT] *************** Autotuning Reformat: Int8(6412032,133584:4,484,1) -> Int8(25648128,133584,484,1) ***************
[10/31/2023-17:55:14] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:14] [V] [TRT] Tactic: 0x00000000000003e8 Time: 16.1345
[10/31/2023-17:55:14] [V] [TRT] Tactic: 0x00000000000003ea Time: 7.01972
[10/31/2023-17:55:15] [V] [TRT] Tactic: 0x0000000000000000 Time: 16.1473
[10/31/2023-17:55:15] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 7.01972
[10/31/2023-17:55:15] [V] [TRT] *************** Autotuning Reformat: Int8(6412032,133584:4,484,1) -> Int8(801504,133584:32,484,1) ***************
[10/31/2023-17:55:15] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:15] [V] [TRT] Tactic: 0x00000000000003e8 Time: 12.0928
[10/31/2023-17:55:15] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.39171
[10/31/2023-17:55:15] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.63964
[10/31/2023-17:55:15] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 3.63964
[10/31/2023-17:55:15] [V] [TRT] *************** Autotuning Reformat: Int8(801504,133584:32,484,1) -> Int8(25648128,133584,484,1) ***************
[10/31/2023-17:55:15] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:15] [V] [TRT] Tactic: 0x00000000000003e8 Time: 16.5794
[10/31/2023-17:55:15] [V] [TRT] Tactic: 0x00000000000003ea Time: 18.6279
[10/31/2023-17:55:15] [V] [TRT] Tactic: 0x0000000000000000 Time: 16.5571
[10/31/2023-17:55:15] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 16.5571
[10/31/2023-17:55:15] [V] [TRT] *************** Autotuning Reformat: Int8(801504,133584:32,484,1) -> Int8(6412032,133584:4,484,1) ***************
[10/31/2023-17:55:15] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:15] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.01336
[10/31/2023-17:55:15] [V] [TRT] Tactic: 0x00000000000003ea Time: 8.36895
[10/31/2023-17:55:15] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.58491
[10/31/2023-17:55:15] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.58491
[10/31/2023-17:55:15] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:15] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,133584,484,1) -> Int8(2137344,133584:4,484,1) ***************
[10/31/2023-17:55:15] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect3/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:15] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.42577
[10/31/2023-17:55:15] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.28483
[10/31/2023-17:55:15] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.76224
[10/31/2023-17:55:15] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.76224
[10/31/2023-17:55:15] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,133584,484,1) -> Int8(267168,133584:32,484,1) ***************
[10/31/2023-17:55:15] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect3/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x00000000000003e8 Time: 6.43551
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.10023
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.43355
[10/31/2023-17:55:16] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.10023
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,133584:4,484,1) -> Int8(8549376,133584,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect3/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.39323
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.3551
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.38752
[10/31/2023-17:55:16] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 2.3551
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,133584:4,484,1) -> Int8(267168,133584:32,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect3/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x00000000000003e8 Time: 4.05541
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.80222
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.910171
[10/31/2023-17:55:16] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.910171
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(267168,133584:32,484,1) -> Int8(8549376,133584,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect3/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x00000000000003e8 Time: 5.53486
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x00000000000003ea Time: 6.21065
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x0000000000000000 Time: 5.53828
[10/31/2023-17:55:16] [V] [TRT] Fastest Tactic: 0x00000000000003e8 Time: 5.53486
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(267168,133584:32,484,1) -> Int8(2137344,133584:4,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect3/conv2/conv/_input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.68268
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x00000000000003ea Time: 2.80298
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.535506
[10/31/2023-17:55:16] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.535506
[10/31/2023-17:55:16] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(17098752,133584,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(17098752,133584,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,133584:4,484,1) -> Int8(17098752,133584,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,133584:4,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(534336,133584:32,484,1) -> Int8(17098752,133584,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(534336,133584:32,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,133584,484,1) -> Int8(2137344,133584:4,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,133584,484,1) -> Int8(267168,133584:32,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,133584:4,484,1) -> Int8(8549376,133584,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,133584:4,484,1) -> Int8(267168,133584:32,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(267168,133584:32,484,1) -> Int8(8549376,133584,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(267168,133584:32,484,1) -> Int8(2137344,133584:4,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(17098752,133584,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(17098752,133584,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,133584:4,484,1) -> Int8(17098752,133584,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,133584:4,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(534336,133584:32,484,1) -> Int8(17098752,133584,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(534336,133584:32,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,133584,484,1) -> Int8(2137344,133584:4,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(8549376,133584,484,1) -> Int8(267168,133584:32,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,133584:4,484,1) -> Int8(8549376,133584,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(2137344,133584:4,484,1) -> Int8(267168,133584:32,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(267168,133584:32,484,1) -> Int8(8549376,133584,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(267168,133584:32,484,1) -> Int8(2137344,133584:4,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(17098752,133584,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(17098752,133584,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(4274688,133584:4,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Int8(534336,133584:32,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Float(267168,133584:32,484,1) -> Float(8415792,133584,484,1) ***************
[10/31/2023-17:55:16] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect3/conv7/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x00000000000003e8 Time: 7.0737
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x00000000000003ea Time: 3.02537
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x0000000000000000 Time: 7.05763
[10/31/2023-17:55:16] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 3.02537
[10/31/2023-17:55:16] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Float(66792,33396:32,242,1) -> Float(2103948,33396,242,1) ***************
[10/31/2023-17:55:16] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect2/conv7/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x00000000000003e8 Time: 1.76891
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.765856
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.76839
[10/31/2023-17:55:16] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.765856
[10/31/2023-17:55:16] [V] [TRT] =============== Computing reformatting costs: 
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning Reformat: Float(16698,8349:32,121,1) -> Float(525987,8349,121,1) ***************
[10/31/2023-17:55:16] [V] [TRT] --------------- Timing Runner: Optimizer Reformat(/neck/detect1/conv7/Conv_output_0 -> <out>) (Reformat)
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x00000000000003e8 Time: 0.30181
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x00000000000003ea Time: 0.205225
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.299991
[10/31/2023-17:55:16] [V] [TRT] Fastest Tactic: 0x00000000000003ea Time: 0.205225
[10/31/2023-17:55:16] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning format combination:  ->  ***************
[10/31/2023-17:55:16] [V] [TRT] --------------- Timing Runner: [trainStation1] (TrainStation)
[10/31/2023-17:55:16] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.000275633
[10/31/2023-17:55:16] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.000275633
[10/31/2023-17:55:16] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: TrainStation Tactic: 0x0000000000000000
[10/31/2023-17:55:16] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning format combination:  -> Float(128,1,1,1) ***************
[10/31/2023-17:55:16] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning format combination:  -> Float(128,1,1,1) ***************
[10/31/2023-17:55:16] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning format combination:  -> Float(256,1,1,1) ***************
[10/31/2023-17:55:16] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning format combination:  -> Float(256,1,1,1) ***************
[10/31/2023-17:55:16] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning format combination:  -> Float(512,1,1,1) ***************
[10/31/2023-17:55:16] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning format combination:  -> Float(512,1,1,1) ***************
[10/31/2023-17:55:16] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning format combination:  -> Int32() ***************
[10/31/2023-17:55:16] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning format combination:  -> Float() ***************
[10/31/2023-17:55:16] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning format combination:  -> Float() ***************
[10/31/2023-17:55:16] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:55:16] [V] [TRT] *************** Autotuning format combination: Int8(8549376,8549376:4,3872,1) -> Float(136790016,2137344,1936,1) ***************
[10/31/2023-17:55:16] [V] [TRT] --------------- Timing Runner: backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear + /backbone/conv1/Conv (CudaDepthwiseConvolution)
[10/31/2023-17:55:16] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:55:16] [V] [TRT] --------------- Timing Runner: backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear + /backbone/conv1/Conv (CaskConvolution)
[10/31/2023-17:55:16] [V] [TRT] backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear + /backbone/conv1/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 0xb29abdd00304c881
[10/31/2023-17:55:17] [V] [TRT] Tactic: 0xb29abdd00304c881 Time: 58.8474
[10/31/2023-17:55:17] [V] [TRT] backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear + /backbone/conv1/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: 0x127de12a1f1a4809
[10/31/2023-17:55:18] [V] [TRT] Tactic: 0x127de12a1f1a4809 Time: 178.031
[10/31/2023-17:55:19] [V] [TRT] backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear + /backbone/conv1/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 0xec391424db39a74f
[10/31/2023-17:55:19] [V] [TRT] Tactic: 0xec391424db39a74f Time: 64.524
[10/31/2023-17:55:19] [V] [TRT] backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear + /backbone/conv1/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 0xe01ccc14da28fa6f
[10/31/2023-17:55:20] [V] [TRT] Tactic: 0xe01ccc14da28fa6f Time: 60.2465
[10/31/2023-17:55:20] [V] [TRT] backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear + /backbone/conv1/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 0x1b0534177b414e71
[10/31/2023-17:55:20] [V] [TRT] Tactic: 0x1b0534177b414e71 Time: 60.3834
[10/31/2023-17:55:21] [V] [TRT] backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear + /backbone/conv1/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 0x2cf4271504d30e29
[10/31/2023-17:55:22] [V] [TRT] Tactic: 0x2cf4271504d30e29 Time: 112.39
[10/31/2023-17:55:22] [V] [TRT] Fastest Tactic: 0xb29abdd00304c881 Time: 58.8474
[10/31/2023-17:55:22] [V] [TRT] --------------- Timing Runner: backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear + /backbone/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:55:22] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:55:22] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xb29abdd00304c881
[10/31/2023-17:55:22] [V] [TRT] *************** Autotuning format combination: Int8(8549376,8549376:32,3872,1) -> Float(136790016,2137344,1936,1) ***************
[10/31/2023-17:55:22] [V] [TRT] --------------- Timing Runner: backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear + /backbone/conv1/Conv (CaskConvolution)
[10/31/2023-17:55:22] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:55:22] [V] [TRT] --------------- Timing Runner: backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear + /backbone/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:55:22] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:55:22] [V] [TRT] *************** Autotuning format combination: Int8(8549376,8549376:32,3872,1) -> Float(4274688,2137344:32,1936,1) ***************
[10/31/2023-17:55:22] [V] [TRT] --------------- Timing Runner: backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear + /backbone/conv1/Conv (CaskConvolution)
[10/31/2023-17:55:22] [V] [TRT] backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear + /backbone/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87
[10/31/2023-17:55:23] [V] [TRT] Tactic: 0xdc1f355deb032b87 Time: 156.479
[10/31/2023-17:55:23] [V] [TRT] backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear + /backbone/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e
[10/31/2023-17:55:24] [V] [TRT] Tactic: 0x6986b0c6a136276e Time: 100.14
[10/31/2023-17:55:24] [V] [TRT] backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear + /backbone/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec
[10/31/2023-17:55:26] [V] [TRT] Tactic: 0xa2f15b0a75b7dcec Time: 146.887
[10/31/2023-17:55:26] [V] [TRT] Fastest Tactic: 0x6986b0c6a136276e Time: 100.14
[10/31/2023-17:55:26] [V] [TRT] --------------- Timing Runner: backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear + /backbone/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:55:26] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:55:26] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6986b0c6a136276e
[10/31/2023-17:55:26] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:55:26] [V] [TRT] *************** Autotuning format combination: Float(136790016,2137344,1936,1) -> Float(34197504,534336,968,1) ***************
[10/31/2023-17:55:26] [V] [TRT] --------------- Timing Runner: /backbone/maxpool/MaxPool (TiledPooling)
[10/31/2023-17:55:26] [V] [TRT] TiledPooling has no valid tactics for this config, skipping
[10/31/2023-17:55:26] [V] [TRT] --------------- Timing Runner: /backbone/maxpool/MaxPool (CudnnPooling)
[10/31/2023-17:55:26] [V] [TRT] Tactic: 0xffffffffffffffff Time: 31.4119
[10/31/2023-17:55:26] [V] [TRT] Fastest Tactic: 0xffffffffffffffff Time: 31.4119
[10/31/2023-17:55:26] [V] [TRT] --------------- Timing Runner: /backbone/maxpool/MaxPool (CaskPooling)
[10/31/2023-17:55:26] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll5_tThreads1017 Tactic: 0x7b9e5e445528b90a
[10/31/2023-17:55:27] [V] [TRT] Tactic: 0x7b9e5e445528b90a Time: 48.6545
[10/31/2023-17:55:27] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll3_tThreads513 Tactic: 0xcb3875826530ea38
[10/31/2023-17:55:27] [V] [TRT] Tactic: 0xcb3875826530ea38 Time: 58.6958
[10/31/2023-17:55:27] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll5_tThreads791 Tactic: 0x7f97b1a406293c0b
[10/31/2023-17:55:28] [V] [TRT] Tactic: 0x7f97b1a406293c0b Time: 56.9185
[10/31/2023-17:55:28] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll5_tThreads841 Tactic: 0x7bd883ae684d33e0
[10/31/2023-17:55:29] [V] [TRT] Tactic: 0x7bd883ae684d33e0 Time: 81.7622
[10/31/2023-17:55:29] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll1_tThreads855 Tactic: 0xf86a4e1f189f4821
[10/31/2023-17:55:31] [V] [TRT] Tactic: 0xf86a4e1f189f4821 Time: 185.752
[10/31/2023-17:55:31] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll6_tThreads855 Tactic: 0x5f5e601b4a0531ed
[10/31/2023-17:55:31] [V] [TRT] Tactic: 0x5f5e601b4a0531ed Time: 50.4251
[10/31/2023-17:55:31] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll1_tThreads1017 Tactic: 0x4cf88ed475f74f6e
[10/31/2023-17:55:33] [V] [TRT] Tactic: 0x4cf88ed475f74f6e Time: 159.521
[10/31/2023-17:55:33] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll6_tThreads513 Tactic: 0x4587105059a26a2b
[10/31/2023-17:55:33] [V] [TRT] Tactic: 0x4587105059a26a2b Time: 38.9475
[10/31/2023-17:55:33] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll4_tThreads855 Tactic: 0x76d52bcd240dc832
[10/31/2023-17:55:34] [V] [TRT] Tactic: 0x76d52bcd240dc832 Time: 60.2975
[10/31/2023-17:55:34] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll3_tThreads855 Tactic: 0xd1e105c97697b1fe
[10/31/2023-17:55:35] [V] [TRT] Tactic: 0xd1e105c97697b1fe Time: 77.2953
[10/31/2023-17:55:35] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll6_tThreads791 Tactic: 0x8bb5080c88a2b679
[10/31/2023-17:55:35] [V] [TRT] Tactic: 0x8bb5080c88a2b679 Time: 51.4192
[10/31/2023-17:55:35] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll4_tThreads513 Tactic: 0x6c0c5b8637aa93f4
[10/31/2023-17:55:36] [V] [TRT] Tactic: 0x6c0c5b8637aa93f4 Time: 45.5574
[10/31/2023-17:55:36] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll1_tThreads791 Tactic: 0x2c812608da38cfb5
[10/31/2023-17:55:38] [V] [TRT] Tactic: 0x2c812608da38cfb5 Time: 188.47
[10/31/2023-17:55:38] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll4_tThreads791 Tactic: 0xa23e43dae6aa4fa6
[10/31/2023-17:55:39] [V] [TRT] Tactic: 0xa23e43dae6aa4fa6 Time: 67.0217
[10/31/2023-17:55:39] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_custom_tP4_tQ32_tRS3_tUV2 Tactic: 0x2639d3932b27ac67
[10/31/2023-17:55:39] [V] [TRT] Tactic: 0x2639d3932b27ac67 Time: 17.5134
[10/31/2023-17:55:39] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll4_tThreads255 Tactic: 0xfcb5fcaa68fff7ac
[10/31/2023-17:55:40] [V] [TRT] Tactic: 0xfcb5fcaa68fff7ac Time: 96.1599
[10/31/2023-17:55:40] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll4_tThreads225 Tactic: 0xf21b9b7ab2973d3e
[10/31/2023-17:55:41] [V] [TRT] Tactic: 0xf21b9b7ab2973d3e Time: 109.744
[10/31/2023-17:55:41] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll3_tThreads791 Tactic: 0x050a6ddeb430366a
[10/31/2023-17:55:42] [V] [TRT] Tactic: 0x050a6ddeb430366a Time: 85.2034
[10/31/2023-17:55:42] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll3_tThreads841 Tactic: 0x01455fd4da543981
[10/31/2023-17:55:43] [V] [TRT] Tactic: 0x01455fd4da543981 Time: 89.3733
[10/31/2023-17:55:43] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll5_tThreads255 Tactic: 0x211c0ed4887c8401
[10/31/2023-17:55:44] [V] [TRT] Tactic: 0x211c0ed4887c8401 Time: 93.036
[10/31/2023-17:55:44] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll6_tThreads1017 Tactic: 0x6df482284d70bfa1
[10/31/2023-17:55:44] [V] [TRT] Tactic: 0x6df482284d70bfa1 Time: 43.6299
[10/31/2023-17:55:44] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NCHW_Max Tactic: 0xb59f9cfb90407c92
[10/31/2023-17:55:45] [V] [TRT] Tactic: 0xb59f9cfb90407c92 Time: 32.1537
[10/31/2023-17:55:45] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll1_tThreads513 Tactic: 0xe2b33e540b3813e7
[10/31/2023-17:55:46] [V] [TRT] Tactic: 0xe2b33e540b3813e7 Time: 146.105
[10/31/2023-17:55:46] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll5_tThreads513 Tactic: 0xb1a5a9f8d729e059
[10/31/2023-17:55:47] [V] [TRT] Tactic: 0xb1a5a9f8d729e059 Time: 41.641
[10/31/2023-17:55:47] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll6_tThreads255 Tactic: 0xd53eb77c06f70e73
[10/31/2023-17:55:48] [V] [TRT] Tactic: 0xd53eb77c06f70e73 Time: 91.6854
[10/31/2023-17:55:48] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll2_tThreads513 Tactic: 0x169187fc85b39995
[10/31/2023-17:55:49] [V] [TRT] Tactic: 0x169187fc85b39995 Time: 80.2385
[10/31/2023-17:55:49] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll2_tThreads841 Tactic: 0xdcecadaa3ad74a2c
[10/31/2023-17:55:50] [V] [TRT] Tactic: 0xdcecadaa3ad74a2c Time: 103.522
[10/31/2023-17:55:50] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll3_tThreads225 Tactic: 0x552fb57ee00d44f2
[10/31/2023-17:55:51] [V] [TRT] Tactic: 0x552fb57ee00d44f2 Time: 111.467
[10/31/2023-17:55:51] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_nd_NCDHW_kMAX_kGENERIC_3D_POOLING_MODE_kFLOAT_0 Tactic: 0x5faf4a0a8a5670ed
[10/31/2023-17:55:51] [V] [TRT] Tactic: 0x5faf4a0a8a5670ed Time: 65.1529
[10/31/2023-17:55:51] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll5_tThreads225 Tactic: 0x2fb2690452144e93
[10/31/2023-17:55:53] [V] [TRT] Tactic: 0x2fb2690452144e93 Time: 107.048
[10/31/2023-17:55:53] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll3_tThreads255 Tactic: 0x5b81d2ae3a658e60
[10/31/2023-17:55:54] [V] [TRT] Tactic: 0x5b81d2ae3a658e60 Time: 96.1501
[10/31/2023-17:55:54] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll3_tThreads1017 Tactic: 0x574be69c6598b45c
[10/31/2023-17:55:54] [V] [TRT] Tactic: 0x574be69c6598b45c Time: 66.1777
[10/31/2023-17:55:54] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll6_tThreads225 Tactic: 0xdb90d0acdc9fc4e1
[10/31/2023-17:55:55] [V] [TRT] Tactic: 0xdb90d0acdc9fc4e1 Time: 104.634
[10/31/2023-17:55:55] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll2_tThreads791 Tactic: 0xd8a39fa054b345c7
[10/31/2023-17:55:57] [V] [TRT] Tactic: 0xd8a39fa054b345c7 Time: 114.049
[10/31/2023-17:55:57] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll1_tThreads841 Tactic: 0x28ce1402b45cc05e
[10/31/2023-17:55:58] [V] [TRT] Tactic: 0x28ce1402b45cc05e Time: 195.807
[10/31/2023-17:55:58] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll5_tThreads855 Tactic: 0xab7cd9b3c48ebb9f
[10/31/2023-17:55:59] [V] [TRT] Tactic: 0xab7cd9b3c48ebb9f Time: 53.3006
[10/31/2023-17:55:59] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll2_tThreads1017 Tactic: 0x5a9252b86daf49c5
[10/31/2023-17:56:00] [V] [TRT] Tactic: 0x5a9252b86daf49c5 Time: 90.1264
[10/31/2023-17:56:00] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll4_tThreads841 Tactic: 0xa67171d088ce404d
[10/31/2023-17:56:01] [V] [TRT] Tactic: 0xa67171d088ce404d Time: 89.0494
[10/31/2023-17:56:01] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll1_tThreads225 Tactic: 0x7ca4fea88e05bd2d
[10/31/2023-17:56:02] [V] [TRT] Tactic: 0x7ca4fea88e05bd2d Time: 125.336
[10/31/2023-17:56:02] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll2_tThreads255 Tactic: 0x862820d0dae6fdcd
[10/31/2023-17:56:03] [V] [TRT] Tactic: 0x862820d0dae6fdcd Time: 100.865
[10/31/2023-17:56:03] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll4_tThreads1017 Tactic: 0x7647ea605d1f4493
[10/31/2023-17:56:04] [V] [TRT] Tactic: 0x7647ea605d1f4493 Time: 55.3684
[10/31/2023-17:56:04] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll1_tThreads255 Tactic: 0x720a9978546d77bf
[10/31/2023-17:56:05] [V] [TRT] Tactic: 0x720a9978546d77bf Time: 108.879
[10/31/2023-17:56:05] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll2_tThreads225 Tactic: 0x88864700008e375f
[10/31/2023-17:56:06] [V] [TRT] Tactic: 0x88864700008e375f Time: 115.868
[10/31/2023-17:56:06] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll2_tThreads855 Tactic: 0x0c48f7b79614c253
[10/31/2023-17:56:07] [V] [TRT] Tactic: 0x0c48f7b79614c253 Time: 98.765
[10/31/2023-17:56:07] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll6_tThreads841 Tactic: 0x8ffa3a06e6c6b992
[10/31/2023-17:56:08] [V] [TRT] Tactic: 0x8ffa3a06e6c6b992 Time: 75.3095
[10/31/2023-17:56:08] [V] [TRT] Fastest Tactic: 0x2639d3932b27ac67 Time: 17.5134
[10/31/2023-17:56:08] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x2639d3932b27ac67
[10/31/2023-17:56:08] [V] [TRT] *************** Autotuning format combination: Float(34197504,1:4,30976,16) -> Float(8549376,1:4,15488,16) ***************
[10/31/2023-17:56:08] [V] [TRT] --------------- Timing Runner: /backbone/maxpool/MaxPool (CaskPooling)
[10/31/2023-17:56:08] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_3_NOT_PROPAGATE_NAN_2D Tactic: 0x789b2859f2e03e79
[10/31/2023-17:56:08] [V] [TRT] Tactic: 0x789b2859f2e03e79 Time: 20.4841
[10/31/2023-17:56:08] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_0_NOT_PROPAGATE_NAN_3D Tactic: 0xfa211b1cdd504de0
[10/31/2023-17:56:09] [V] [TRT] Tactic: 0xfa211b1cdd504de0 Time: 41.9155
[10/31/2023-17:56:09] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_3_PROPAGATE_NAN_3D Tactic: 0xe9d01a2a900075cb
[10/31/2023-17:56:09] [V] [TRT] Tactic: 0xe9d01a2a900075cb Time: 56.9896
[10/31/2023-17:56:09] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_0_NOT_PROPAGATE_NAN_2D Tactic: 0xaec8628e8180bced
[10/31/2023-17:56:10] [V] [TRT] Tactic: 0xaec8628e8180bced Time: 40.1768
[10/31/2023-17:56:10] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_3_PROPAGATE_NAN_2D Tactic: 0xbd3963b8ccd084c6
[10/31/2023-17:56:10] [V] [TRT] Tactic: 0xbd3963b8ccd084c6 Time: 21.9379
[10/31/2023-17:56:10] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_0_PROPAGATE_NAN_2D Tactic: 0x8382d5c464539e87
[10/31/2023-17:56:11] [V] [TRT] Tactic: 0x8382d5c464539e87 Time: 47.9008
[10/31/2023-17:56:11] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_3_NOT_PROPAGATE_NAN_3D Tactic: 0x2c7251cbae30cf74
[10/31/2023-17:56:12] [V] [TRT] Tactic: 0x2c7251cbae30cf74 Time: 56.9146
[10/31/2023-17:56:12] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NHWC_Max_CAlign4 Tactic: 0x22fb1bb4a70e340d
[10/31/2023-17:56:12] [V] [TRT] Tactic: 0x22fb1bb4a70e340d Time: 54.5693
[10/31/2023-17:56:12] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_max_nhwc_FP32FP32_WINDOWSIZE_0_PROPAGATE_NAN_3D Tactic: 0xd76bac5638836f8a
[10/31/2023-17:56:13] [V] [TRT] Tactic: 0xd76bac5638836f8a Time: 49.8061
[10/31/2023-17:56:13] [V] [TRT] Fastest Tactic: 0x789b2859f2e03e79 Time: 20.4841
[10/31/2023-17:56:13] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x789b2859f2e03e79
[10/31/2023-17:56:13] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:56:13] [V] [TRT] *************** Autotuning format combination: Int8(34197504,534336,968,1) -> Int8(34197504,534336,968,1) ***************
[10/31/2023-17:56:13] [V] [TRT] --------------- Timing Runner: backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv (CaskConvolution)
[10/31/2023-17:56:13] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:13] [V] [TRT] --------------- Timing Runner: backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:56:13] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:13] [V] [TRT] *************** Autotuning format combination: Int8(8549376,534336:4,968,1) -> Int8(8549376,534336:4,968,1) ***************
[10/31/2023-17:56:13] [V] [TRT] --------------- Timing Runner: backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv (CudaDepthwiseConvolution)
[10/31/2023-17:56:13] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:13] [V] [TRT] --------------- Timing Runner: backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv (FusedConvActConvolution)
[10/31/2023-17:56:13] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:13] [V] [TRT] --------------- Timing Runner: backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv (CaskConvolution)
[10/31/2023-17:56:13] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0x8846c5916f34f7e9
[10/31/2023-17:56:13] [V] [TRT] Tactic: 0x8846c5916f34f7e9 Time: 35.7146
[10/31/2023-17:56:13] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 0xf6833cac33ebf690
[10/31/2023-17:56:14] [V] [TRT] Tactic: 0xf6833cac33ebf690 Time: 66.3688
[10/31/2023-17:56:14] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 0xa49b04dbd5d9448a
[10/31/2023-17:56:14] [V] [TRT] Tactic: 0xa49b04dbd5d9448a Time: 66.3147
[10/31/2023-17:56:14] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: 0xe0658ca00f36ea3e
[10/31/2023-17:56:15] [V] [TRT] Tactic: 0xe0658ca00f36ea3e Time: 77.9607
[10/31/2023-17:56:15] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 0x0270c2170caa55f8
[10/31/2023-17:56:15] [V] [TRT] Tactic: 0x0270c2170caa55f8 Time: 36.0212
[10/31/2023-17:56:15] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 0xcc3c5501ab7b5867
[10/31/2023-17:56:15] [V] [TRT] Tactic: 0xcc3c5501ab7b5867 Time: 33.5095
[10/31/2023-17:56:15] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 0x50f6b3d3d5866716
[10/31/2023-17:56:16] [V] [TRT] Tactic: 0x50f6b3d3d5866716 Time: 34.1489
[10/31/2023-17:56:16] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 0x1e2179aecb150578
[10/31/2023-17:56:16] [V] [TRT] Tactic: 0x1e2179aecb150578 Time: 34.7706
[10/31/2023-17:56:16] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x1f0263042c683192
[10/31/2023-17:56:16] [V] [TRT] Tactic: 0x1f0263042c683192 Time: 34.1003
[10/31/2023-17:56:16] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 0x9aa46c15c2a1d8a7
[10/31/2023-17:56:17] [V] [TRT] Tactic: 0x9aa46c15c2a1d8a7 Time: 33.7797
[10/31/2023-17:56:17] [V] [TRT] Fastest Tactic: 0xcc3c5501ab7b5867 Time: 33.5095
[10/31/2023-17:56:17] [V] [TRT] --------------- Timing Runner: backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:56:17] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:17] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xcc3c5501ab7b5867
[10/31/2023-17:56:17] [V] [TRT] *************** Autotuning format combination: Int8(8549376,534336:4,968,1) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:56:17] [V] [TRT] --------------- Timing Runner: backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv (CaskConvolution)
[10/31/2023-17:56:17] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4
[10/31/2023-17:56:17] [V] [TRT] Tactic: 0xeb494e2e67f079d4 Time: 35.9082
[10/31/2023-17:56:17] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18
[10/31/2023-17:56:17] [V] [TRT] Tactic: 0x0f47434ace2a7d18 Time: 35.6597
[10/31/2023-17:56:17] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 0x0da8f7df8cfd2e37
[10/31/2023-17:56:18] [V] [TRT] Tactic: 0x0da8f7df8cfd2e37 Time: 33.4678
[10/31/2023-17:56:18] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: 0x2cf0cc5b8a069ae7
[10/31/2023-17:56:18] [V] [TRT] Tactic: 0x2cf0cc5b8a069ae7 Time: 77.4097
[10/31/2023-17:56:18] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: 0x320c30eda409da30
[10/31/2023-17:56:18] [V] [TRT] Tactic: 0x320c30eda409da30 Time: 34.0498
[10/31/2023-17:56:18] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: 0xf3f27c40da3380fe
[10/31/2023-17:56:19] [V] [TRT] Tactic: 0xf3f27c40da3380fe Time: 34.0294
[10/31/2023-17:56:19] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: 0xc4f09503ebafdf51
[10/31/2023-17:56:19] [V] [TRT] Tactic: 0xc4f09503ebafdf51 Time: 66.2268
[10/31/2023-17:56:19] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 0xd4d010e83172b290
[10/31/2023-17:56:20] [V] [TRT] Tactic: 0xd4d010e83172b290 Time: 34.6605
[10/31/2023-17:56:20] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: 0x3c0834dbcd849973
[10/31/2023-17:56:20] [V] [TRT] Tactic: 0x3c0834dbcd849973 Time: 66.2133
[10/31/2023-17:56:20] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: 0xc911b61f7a5c0315
[10/31/2023-17:56:20] [V] [TRT] Tactic: 0xc911b61f7a5c0315 Time: 33.6957
[10/31/2023-17:56:20] [V] [TRT] Fastest Tactic: 0x0da8f7df8cfd2e37 Time: 33.4678
[10/31/2023-17:56:20] [V] [TRT] --------------- Timing Runner: backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:56:20] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:20] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0da8f7df8cfd2e37
[10/31/2023-17:56:20] [V] [TRT] *************** Autotuning format combination: Int8(1068672,534336:32,968,1) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:56:20] [V] [TRT] --------------- Timing Runner: backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv (CudaGroupConvolution)
[10/31/2023-17:56:20] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:20] [V] [TRT] --------------- Timing Runner: backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv (CudaDepthwiseConvolution)
[10/31/2023-17:56:20] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:20] [V] [TRT] --------------- Timing Runner: backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv (FusedConvActConvolution)
[10/31/2023-17:56:20] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:20] [V] [TRT] --------------- Timing Runner: backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv (CaskConvolution)
[10/31/2023-17:56:20] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18
[10/31/2023-17:56:21] [V] [TRT] Tactic: 0x4cf18ad3e295ea18 Time: 26.8467
[10/31/2023-17:56:21] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5
[10/31/2023-17:56:21] [V] [TRT] Tactic: 0x89c1d75fe77808d5 Time: 19.2197
[10/31/2023-17:56:21] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363
[10/31/2023-17:56:21] [V] [TRT] Tactic: 0xf56c0ac895d82363 Time: 10.9423
[10/31/2023-17:56:21] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673
[10/31/2023-17:56:21] [V] [TRT] Tactic: 0xf33711e7c9ed4673 Time: 12.5183
[10/31/2023-17:56:21] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780
[10/31/2023-17:56:21] [V] [TRT] Tactic: 0xeb33cf0799237780 Time: 10.8965
[10/31/2023-17:56:21] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50
[10/31/2023-17:56:21] [V] [TRT] Tactic: 0x3818ca0093333b50 Time: 16.1926
[10/31/2023-17:56:21] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7
[10/31/2023-17:56:21] [V] [TRT] Tactic: 0xee2fce9480a52be7 Time: 17.0669
[10/31/2023-17:56:21] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x048dc59a1807b5bb
[10/31/2023-17:56:22] [V] [TRT] Tactic: 0x048dc59a1807b5bb Time: 21.6125
[10/31/2023-17:56:22] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af
[10/31/2023-17:56:22] [V] [TRT] Tactic: 0x26a29d5b8b3f62af Time: 6.97498
[10/31/2023-17:56:22] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee
[10/31/2023-17:56:22] [V] [TRT] Tactic: 0xd277f13d771603ee Time: 9.62353
[10/31/2023-17:56:22] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52
[10/31/2023-17:56:22] [V] [TRT] Tactic: 0x11b75d98d540ee52 Time: 13.3461
[10/31/2023-17:56:22] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071
[10/31/2023-17:56:22] [V] [TRT] Tactic: 0x2f34f689bfca5071 Time: 13.5751
[10/31/2023-17:56:22] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4
[10/31/2023-17:56:22] [V] [TRT] Tactic: 0x9f6920f4a40549f4 Time: 19.1987
[10/31/2023-17:56:22] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e
[10/31/2023-17:56:22] [V] [TRT] Tactic: 0x2ff8bc85c8ced89e Time: 10.0825
[10/31/2023-17:56:22] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0
[10/31/2023-17:56:22] [V] [TRT] Tactic: 0x5e1b696f91f572c0 Time: 8.8869
[10/31/2023-17:56:22] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26
[10/31/2023-17:56:23] [V] [TRT] Tactic: 0x623894f465c90f26 Time: 14.179
[10/31/2023-17:56:23] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423
[10/31/2023-17:56:23] [V] [TRT] Tactic: 0x955d593b1135a423 Time: 14.5091
[10/31/2023-17:56:23] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d
[10/31/2023-17:56:23] [V] [TRT] Tactic: 0x2728bbf79375f71d Time: 13.0157
[10/31/2023-17:56:23] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df
[10/31/2023-17:56:23] [V] [TRT] Tactic: 0x87620679fb5f37df Time: 13.1035
[10/31/2023-17:56:23] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b
[10/31/2023-17:56:23] [V] [TRT] Tactic: 0x705baf38e41eee0b Time: 8.29674
[10/31/2023-17:56:23] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd
[10/31/2023-17:56:23] [V] [TRT] Tactic: 0xd30e9f770878c3fd Time: 15.7777
[10/31/2023-17:56:23] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0
[10/31/2023-17:56:23] [V] [TRT] Tactic: 0x41b4640eb250ffd0 Time: 26.8781
[10/31/2023-17:56:23] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6
[10/31/2023-17:56:23] [V] [TRT] Tactic: 0xd3d41ef6de22d9b6 Time: 6.54252
[10/31/2023-17:56:23] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4805f65acfc5cf25
[10/31/2023-17:56:24] [V] [TRT] Tactic: 0x4805f65acfc5cf25 Time: 20.4052
[10/31/2023-17:56:24] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d
[10/31/2023-17:56:24] [V] [TRT] Tactic: 0x8d50646eff0cde6d Time: 12.2964
[10/31/2023-17:56:24] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c
[10/31/2023-17:56:24] [V] [TRT] Tactic: 0xf8d4389f60adfa3c Time: 13.0422
[10/31/2023-17:56:24] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163
[10/31/2023-17:56:24] [V] [TRT] Tactic: 0x80cce6d8f75dc163 Time: 13.7605
[10/31/2023-17:56:24] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96
[10/31/2023-17:56:24] [V] [TRT] Tactic: 0xea88b51105501f96 Time: 11.8457
[10/31/2023-17:56:24] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468
[10/31/2023-17:56:24] [V] [TRT] Tactic: 0x53554c607d072468 Time: 9.52877
[10/31/2023-17:56:24] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab
[10/31/2023-17:56:24] [V] [TRT] Tactic: 0x70ccdad7e8ced9ab Time: 7.25215
[10/31/2023-17:56:24] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9
[10/31/2023-17:56:24] [V] [TRT] Tactic: 0x99901a83d7176ba9 Time: 7.66373
[10/31/2023-17:56:24] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7
[10/31/2023-17:56:24] [V] [TRT] Tactic: 0x8e1e9d670448aca7 Time: 14.5606
[10/31/2023-17:56:24] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02
[10/31/2023-17:56:24] [V] [TRT] Tactic: 0x204424cb4da28c02 Time: 6.75921
[10/31/2023-17:56:24] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49
[10/31/2023-17:56:25] [V] [TRT] Tactic: 0x0fbfd5fa9864ab49 Time: 12.8604
[10/31/2023-17:56:25] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb
[10/31/2023-17:56:25] [V] [TRT] Tactic: 0x9d0f90e0cec890bb Time: 12.3064
[10/31/2023-17:56:25] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4
[10/31/2023-17:56:25] [V] [TRT] Tactic: 0xa92dc4358ef627d4 Time: 14.4232
[10/31/2023-17:56:25] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530
[10/31/2023-17:56:25] [V] [TRT] Tactic: 0x7dcb6d55062fd530 Time: 13.6787
[10/31/2023-17:56:25] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792
[10/31/2023-17:56:25] [V] [TRT] Tactic: 0x8b2f6e180c235792 Time: 19.9285
[10/31/2023-17:56:25] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986
[10/31/2023-17:56:25] [V] [TRT] Tactic: 0xd3f592fae61c7986 Time: 13.3974
[10/31/2023-17:56:25] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8
[10/31/2023-17:56:25] [V] [TRT] Tactic: 0x60edc1e1753cd5b8 Time: 13.8645
[10/31/2023-17:56:25] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393
[10/31/2023-17:56:26] [V] [TRT] Tactic: 0x0f6ba1e5b0320393 Time: 21.0588
[10/31/2023-17:56:26] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662
[10/31/2023-17:56:26] [V] [TRT] Tactic: 0x4b860619b6031662 Time: 27.9869
[10/31/2023-17:56:26] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da
[10/31/2023-17:56:26] [V] [TRT] Tactic: 0x19b79d0c4ff629da Time: 15.4276
[10/31/2023-17:56:26] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49
[10/31/2023-17:56:26] [V] [TRT] Tactic: 0x136c072084f41e49 Time: 15.5481
[10/31/2023-17:56:26] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23
[10/31/2023-17:56:26] [V] [TRT] Tactic: 0x4749124f62d8bd23 Time: 17.2753
[10/31/2023-17:56:26] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b
[10/31/2023-17:56:26] [V] [TRT] Tactic: 0x444f830f80ff098b Time: 20.2721
[10/31/2023-17:56:26] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655
[10/31/2023-17:56:27] [V] [TRT] Tactic: 0x671c943720ba8655 Time: 14.4423
[10/31/2023-17:56:27] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23
[10/31/2023-17:56:27] [V] [TRT] Tactic: 0xeb9516439d87ac23 Time: 19.2797
[10/31/2023-17:56:27] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50
[10/31/2023-17:56:27] [V] [TRT] Tactic: 0xef01fb6e433afa50 Time: 6.81575
[10/31/2023-17:56:27] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4
[10/31/2023-17:56:27] [V] [TRT] Tactic: 0xbb88763c3b0e94d4 Time: 9.97627
[10/31/2023-17:56:27] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0
[10/31/2023-17:56:27] [V] [TRT] Tactic: 0x4b476758492c15b0 Time: 10.7423
[10/31/2023-17:56:27] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd
[10/31/2023-17:56:27] [V] [TRT] Tactic: 0x554e2e252e28b3fd Time: 25.731
[10/31/2023-17:56:27] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d
[10/31/2023-17:56:27] [V] [TRT] Tactic: 0x9262f8f95beb428d Time: 9.71962
[10/31/2023-17:56:27] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac
[10/31/2023-17:56:27] [V] [TRT] Tactic: 0xd9f3bbc3e16b16ac Time: 13.6366
[10/31/2023-17:56:27] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3
[10/31/2023-17:56:27] [V] [TRT] Tactic: 0x4f6730716f78f4a3 Time: 12.4482
[10/31/2023-17:56:27] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d
[10/31/2023-17:56:28] [V] [TRT] Tactic: 0x028e842ce51fbd9d Time: 12.5932
[10/31/2023-17:56:28] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c
[10/31/2023-17:56:28] [V] [TRT] Tactic: 0xe37d64a29ca3101c Time: 15.0752
[10/31/2023-17:56:28] [V] [TRT] Fastest Tactic: 0xd3d41ef6de22d9b6 Time: 6.54252
[10/31/2023-17:56:28] [V] [TRT] --------------- Timing Runner: backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:56:28] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:28] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd3d41ef6de22d9b6
[10/31/2023-17:56:28] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:56:28] [V] [TRT] *************** Autotuning format combination: Int8(8549376,534336:4,968,1), Float(34197504,534336,968,1) -> Float(34197504,534336,968,1) ***************
[10/31/2023-17:56:28] [V] [TRT] --------------- Timing Runner: backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu (CaskConvolution)
[10/31/2023-17:56:28] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 0xb29abdd00304c881
[10/31/2023-17:56:28] [V] [TRT] Tactic: 0xb29abdd00304c881 Time: 35.2734
[10/31/2023-17:56:28] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: 0x127de12a1f1a4809
[10/31/2023-17:56:29] [V] [TRT] Tactic: 0x127de12a1f1a4809 Time: 90.2363
[10/31/2023-17:56:29] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99
[10/31/2023-17:56:29] [V] [TRT] Tactic: 0xfe80445f7bb61a99 Time: 36.9285
[10/31/2023-17:56:29] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 0x14ef32afe5695907
[10/31/2023-17:56:30] [V] [TRT] Tactic: 0x14ef32afe5695907 Time: 68.1702
[10/31/2023-17:56:30] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 0xec391424db39a74f
[10/31/2023-17:56:30] [V] [TRT] Tactic: 0xec391424db39a74f Time: 36.1531
[10/31/2023-17:56:30] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2
[10/31/2023-17:56:31] [V] [TRT] Tactic: 0x69c4e2ca38eadce2 Time: 35.0675
[10/31/2023-17:56:31] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 0x4d9d0d03129bceb1
[10/31/2023-17:56:31] [V] [TRT] Tactic: 0x4d9d0d03129bceb1 Time: 34.8507
[10/31/2023-17:56:31] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 0xe01ccc14da28fa6f
[10/31/2023-17:56:31] [V] [TRT] Tactic: 0xe01ccc14da28fa6f Time: 37.299
[10/31/2023-17:56:31] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 0x1b0534177b414e71
[10/31/2023-17:56:32] [V] [TRT] Tactic: 0x1b0534177b414e71 Time: 34.4341
[10/31/2023-17:56:32] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 0x2cf4271504d30e29
[10/31/2023-17:56:32] [V] [TRT] Tactic: 0x2cf4271504d30e29 Time: 68.3704
[10/31/2023-17:56:32] [V] [TRT] Fastest Tactic: 0x1b0534177b414e71 Time: 34.4341
[10/31/2023-17:56:32] [V] [TRT] --------------- Timing Runner: backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu (CaskFlattenConvolution)
[10/31/2023-17:56:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1b0534177b414e71
[10/31/2023-17:56:32] [V] [TRT] *************** Autotuning format combination: Int8(1068672,534336:32,968,1), Float(34197504,534336,968,1) -> Float(34197504,534336,968,1) ***************
[10/31/2023-17:56:32] [V] [TRT] --------------- Timing Runner: backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu (CaskConvolution)
[10/31/2023-17:56:32] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84
[10/31/2023-17:56:33] [V] [TRT] Tactic: 0x85c1a5f7f239cf84 Time: 21.9181
[10/31/2023-17:56:33] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10
[10/31/2023-17:56:33] [V] [TRT] Tactic: 0x2bcbba39f608bf10 Time: 17.665
[10/31/2023-17:56:33] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808
[10/31/2023-17:56:33] [V] [TRT] Tactic: 0xac914b235d066808 Time: 24.8129
[10/31/2023-17:56:33] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e
[10/31/2023-17:56:33] [V] [TRT] Tactic: 0x23b890da05937b9e Time: 14.293
[10/31/2023-17:56:33] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463
[10/31/2023-17:56:33] [V] [TRT] Tactic: 0xa8b56a226b057463 Time: 16.1138
[10/31/2023-17:56:33] [V] [TRT] Fastest Tactic: 0x23b890da05937b9e Time: 14.293
[10/31/2023-17:56:33] [V] [TRT] --------------- Timing Runner: backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu (CaskFlattenConvolution)
[10/31/2023-17:56:33] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x23b890da05937b9e
[10/31/2023-17:56:33] [V] [TRT] *************** Autotuning format combination: Int8(1068672,534336:32,968,1), Float(1068672,534336:32,968,1) -> Float(1068672,534336:32,968,1) ***************
[10/31/2023-17:56:33] [V] [TRT] --------------- Timing Runner: backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu (CaskConvolution)
[10/31/2023-17:56:33] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c
[10/31/2023-17:56:34] [V] [TRT] Tactic: 0xb936321f82fd390c Time: 15.9757
[10/31/2023-17:56:34] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa
[10/31/2023-17:56:34] [V] [TRT] Tactic: 0x55edef142e02adaa Time: 14.1418
[10/31/2023-17:56:34] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1
[10/31/2023-17:56:34] [V] [TRT] Tactic: 0xe742f4598442d2f1 Time: 12.1514
[10/31/2023-17:56:34] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e
[10/31/2023-17:56:34] [V] [TRT] Tactic: 0x3f2b14dec582741e Time: 16.2517
[10/31/2023-17:56:34] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4
[10/31/2023-17:56:34] [V] [TRT] Tactic: 0x0edd5d0285e564d4 Time: 18.6055
[10/31/2023-17:56:34] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79
[10/31/2023-17:56:35] [V] [TRT] Tactic: 0xb86b920539eb9c79 Time: 14.6991
[10/31/2023-17:56:35] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87
[10/31/2023-17:56:35] [V] [TRT] Tactic: 0xdc1f355deb032b87 Time: 18.9394
[10/31/2023-17:56:35] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb
[10/31/2023-17:56:35] [V] [TRT] Tactic: 0x45f7566cdb2b10fb Time: 16.6545
[10/31/2023-17:56:35] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1
[10/31/2023-17:56:35] [V] [TRT] Tactic: 0xea2b7420057a01c1 Time: 29.7905
[10/31/2023-17:56:35] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e
[10/31/2023-17:56:35] [V] [TRT] Tactic: 0x6986b0c6a136276e Time: 14.7743
[10/31/2023-17:56:35] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922
[10/31/2023-17:56:36] [V] [TRT] Tactic: 0xad886d4d69834922 Time: 11.8251
[10/31/2023-17:56:36] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a
[10/31/2023-17:56:36] [V] [TRT] Tactic: 0x2468d082d0ff7c9a Time: 18.0098
[10/31/2023-17:56:36] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61
[10/31/2023-17:56:36] [V] [TRT] Tactic: 0x8141573686849b61 Time: 19.6262
[10/31/2023-17:56:36] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377
[10/31/2023-17:56:36] [V] [TRT] Tactic: 0xb5f710b331ba8377 Time: 17.1428
[10/31/2023-17:56:36] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4
[10/31/2023-17:56:37] [V] [TRT] Tactic: 0x748f926574b7bdc4 Time: 30.8197
[10/31/2023-17:56:37] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f
[10/31/2023-17:56:37] [V] [TRT] Tactic: 0x0e07dc8353bf7e9f Time: 10.363
[10/31/2023-17:56:37] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800
[10/31/2023-17:56:37] [V] [TRT] Tactic: 0x47b1629a4bff4800 Time: 16.6747
[10/31/2023-17:56:37] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e
[10/31/2023-17:56:37] [V] [TRT] Tactic: 0x5f1a472d416ff35e Time: 13.8933
[10/31/2023-17:56:37] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33
[10/31/2023-17:56:37] [V] [TRT] Tactic: 0x991db9fd58152c33 Time: 10.4165
[10/31/2023-17:56:37] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e
[10/31/2023-17:56:37] [V] [TRT] Tactic: 0xd14bd6d95fefd45e Time: 19.2837
[10/31/2023-17:56:37] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9
[10/31/2023-17:56:38] [V] [TRT] Tactic: 0x2d8ab2aa0639fda9 Time: 13.2592
[10/31/2023-17:56:38] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda
[10/31/2023-17:56:38] [V] [TRT] Tactic: 0x445983715412fbda Time: 19.8922
[10/31/2023-17:56:38] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec
[10/31/2023-17:56:38] [V] [TRT] Tactic: 0xa2f15b0a75b7dcec Time: 16.5819
[10/31/2023-17:56:38] [V] [TRT] Fastest Tactic: 0x0e07dc8353bf7e9f Time: 10.363
[10/31/2023-17:56:38] [V] [TRT] --------------- Timing Runner: backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu (CaskFlattenConvolution)
[10/31/2023-17:56:38] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:38] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0e07dc8353bf7e9f
[10/31/2023-17:56:38] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:56:38] [V] [TRT] *************** Autotuning format combination: Int8(34197504,534336,968,1) -> Int8(34197504,534336,968,1) ***************
[10/31/2023-17:56:38] [V] [TRT] --------------- Timing Runner: backbone.layer1.1.conv1.weight + /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.1/conv1/Conv (CaskConvolution)
[10/31/2023-17:56:38] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:38] [V] [TRT] --------------- Timing Runner: backbone.layer1.1.conv1.weight + /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.1/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:56:38] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:38] [V] [TRT] *************** Autotuning format combination: Int8(8549376,534336:4,968,1) -> Int8(8549376,534336:4,968,1) ***************
[10/31/2023-17:56:38] [V] [TRT] *************** Autotuning format combination: Int8(8549376,534336:4,968,1) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:56:38] [V] [TRT] *************** Autotuning format combination: Int8(1068672,534336:32,968,1) -> Int8(1068672,534336:32,968,1) ***************
[10/31/2023-17:56:38] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:56:38] [V] [TRT] *************** Autotuning format combination: Int8(8549376,534336:4,968,1), Float(34197504,534336,968,1) -> Float(34197504,534336,968,1) ***************
[10/31/2023-17:56:38] [V] [TRT] *************** Autotuning format combination: Int8(1068672,534336:32,968,1), Float(34197504,534336,968,1) -> Float(34197504,534336,968,1) ***************
[10/31/2023-17:56:38] [V] [TRT] *************** Autotuning format combination: Int8(1068672,534336:32,968,1), Float(1068672,534336:32,968,1) -> Float(1068672,534336:32,968,1) ***************
[10/31/2023-17:56:38] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:56:38] [V] [TRT] *************** Autotuning format combination: Int8(8549376,534336:4,968,1) -> Float(17098752,133584,484,1) ***************
[10/31/2023-17:56:38] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv (CudaDepthwiseConvolution)
[10/31/2023-17:56:38] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:38] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv (CaskConvolution)
[10/31/2023-17:56:38] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 0xb29abdd00304c881
[10/31/2023-17:56:38] [V] [TRT] Tactic: 0xb29abdd00304c881 Time: 3.83184
[10/31/2023-17:56:38] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: 0xc25454de2efcccdc
[10/31/2023-17:56:38] [V] [TRT] Tactic: 0xc25454de2efcccdc Time: 4.49553
[10/31/2023-17:56:38] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99
[10/31/2023-17:56:38] [V] [TRT] Tactic: 0xfe80445f7bb61a99 Time: 4.1717
[10/31/2023-17:56:38] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 0x14ef32afe5695907
[10/31/2023-17:56:38] [V] [TRT] Tactic: 0x14ef32afe5695907 Time: 4.08779
[10/31/2023-17:56:38] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32
[10/31/2023-17:56:38] [V] [TRT] Tactic: 0xff6944b17d5b2e32 Time: 3.64512
[10/31/2023-17:56:38] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 0xec391424db39a74f
[10/31/2023-17:56:38] [V] [TRT] Tactic: 0xec391424db39a74f Time: 4.92122
[10/31/2023-17:56:38] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2
[10/31/2023-17:56:38] [V] [TRT] Tactic: 0x69c4e2ca38eadce2 Time: 3.79327
[10/31/2023-17:56:38] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 0x4d9d0d03129bceb1
[10/31/2023-17:56:39] [V] [TRT] Tactic: 0x4d9d0d03129bceb1 Time: 4.66053
[10/31/2023-17:56:39] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 0xe01ccc14da28fa6f
[10/31/2023-17:56:39] [V] [TRT] Tactic: 0xe01ccc14da28fa6f Time: 4.25759
[10/31/2023-17:56:39] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83
[10/31/2023-17:56:39] [V] [TRT] Tactic: 0xa4ae2d82115c3e83 Time: 4.04992
[10/31/2023-17:56:39] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: 0x1db6e8cf1382fbe0
[10/31/2023-17:56:39] [V] [TRT] Tactic: 0x1db6e8cf1382fbe0 Time: 4.04415
[10/31/2023-17:56:39] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 0x2cf4271504d30e29
[10/31/2023-17:56:39] [V] [TRT] Tactic: 0x2cf4271504d30e29 Time: 4.19802
[10/31/2023-17:56:39] [V] [TRT] Fastest Tactic: 0xff6944b17d5b2e32 Time: 3.64512
[10/31/2023-17:56:39] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv (CaskFlattenConvolution)
[10/31/2023-17:56:39] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[10/31/2023-17:56:39] [V] [TRT] *************** Autotuning format combination: Int8(1068672,534336:32,968,1) -> Float(17098752,133584,484,1) ***************
[10/31/2023-17:56:39] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv (CaskConvolution)
[10/31/2023-17:56:39] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42
[10/31/2023-17:56:39] [V] [TRT] Tactic: 0xf04572b287451f42 Time: 3.90905
[10/31/2023-17:56:39] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431
[10/31/2023-17:56:39] [V] [TRT] Tactic: 0x733ba2a91a48d431 Time: 3.17956
[10/31/2023-17:56:39] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6
[10/31/2023-17:56:39] [V] [TRT] Tactic: 0x5e4f6d7c83746fd6 Time: 4.21376
[10/31/2023-17:56:39] [V] [TRT] Fastest Tactic: 0x733ba2a91a48d431 Time: 3.17956
[10/31/2023-17:56:39] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv (CaskFlattenConvolution)
[10/31/2023-17:56:39] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431
[10/31/2023-17:56:39] [V] [TRT] *************** Autotuning format combination: Int8(1068672,534336:32,968,1) -> Float(534336,133584:32,484,1) ***************
[10/31/2023-17:56:39] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv (CaskConvolution)
[10/31/2023-17:56:39] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1
[10/31/2023-17:56:39] [V] [TRT] Tactic: 0x33a5c6dd086942c1 Time: 3.27611
[10/31/2023-17:56:39] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1
[10/31/2023-17:56:39] [V] [TRT] Tactic: 0xe742f4598442d2f1 Time: 3.4344
[10/31/2023-17:56:39] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667
[10/31/2023-17:56:39] [V] [TRT] Tactic: 0xdf7e1bd6a496d667 Time: 2.98933
[10/31/2023-17:56:39] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e
[10/31/2023-17:56:39] [V] [TRT] Tactic: 0x3f2b14dec582741e Time: 3.76512
[10/31/2023-17:56:39] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36
[10/31/2023-17:56:39] [V] [TRT] Tactic: 0xcf64a2ae51bf6b36 Time: 3.20189
[10/31/2023-17:56:39] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79
[10/31/2023-17:56:39] [V] [TRT] Tactic: 0xb86b920539eb9c79 Time: 3.07874
[10/31/2023-17:56:39] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664
[10/31/2023-17:56:39] [V] [TRT] Tactic: 0xa71946688cad8664 Time: 3.09566
[10/31/2023-17:56:39] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87
[10/31/2023-17:56:39] [V] [TRT] Tactic: 0xdc1f355deb032b87 Time: 3.39325
[10/31/2023-17:56:39] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e
[10/31/2023-17:56:39] [V] [TRT] Tactic: 0x6986b0c6a136276e Time: 4.31595
[10/31/2023-17:56:39] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef
[10/31/2023-17:56:39] [V] [TRT] Tactic: 0x271b998fe31732ef Time: 3.17741
[10/31/2023-17:56:39] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a
[10/31/2023-17:56:39] [V] [TRT] Tactic: 0x2468d082d0ff7c9a Time: 3.58337
[10/31/2023-17:56:39] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd
[10/31/2023-17:56:39] [V] [TRT] Tactic: 0xcd229658c16b33cd Time: 3.86345
[10/31/2023-17:56:39] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61
[10/31/2023-17:56:39] [V] [TRT] Tactic: 0x8141573686849b61 Time: 3.28063
[10/31/2023-17:56:39] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c
[10/31/2023-17:56:39] [V] [TRT] Tactic: 0x60b880e28fee7a0c Time: 4.67296
[10/31/2023-17:56:39] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377
[10/31/2023-17:56:40] [V] [TRT] Tactic: 0xb5f710b331ba8377 Time: 3.87227
[10/31/2023-17:56:40] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4
[10/31/2023-17:56:40] [V] [TRT] Tactic: 0x748f926574b7bdc4 Time: 4.83423
[10/31/2023-17:56:40] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e
[10/31/2023-17:56:40] [V] [TRT] Tactic: 0x5f1a472d416ff35e Time: 3.59454
[10/31/2023-17:56:40] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93
[10/31/2023-17:56:40] [V] [TRT] Tactic: 0x5bd8221bd57baf93 Time: 3.47087
[10/31/2023-17:56:40] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33
[10/31/2023-17:56:40] [V] [TRT] Tactic: 0x991db9fd58152c33 Time: 3.33044
[10/31/2023-17:56:40] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552
[10/31/2023-17:56:40] [V] [TRT] Tactic: 0x84942841d92b0552 Time: 3.26423
[10/31/2023-17:56:40] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda
[10/31/2023-17:56:40] [V] [TRT] Tactic: 0x445983715412fbda Time: 3.97019
[10/31/2023-17:56:40] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19
[10/31/2023-17:56:40] [V] [TRT] Tactic: 0x844ea9c00f711f19 Time: 3.22919
[10/31/2023-17:56:40] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec
[10/31/2023-17:56:40] [V] [TRT] Tactic: 0xa2f15b0a75b7dcec Time: 4.11732
[10/31/2023-17:56:40] [V] [TRT] Fastest Tactic: 0xdf7e1bd6a496d667 Time: 2.98933
[10/31/2023-17:56:40] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv (CaskFlattenConvolution)
[10/31/2023-17:56:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xdf7e1bd6a496d667
[10/31/2023-17:56:40] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:56:40] [V] [TRT] *************** Autotuning format combination: Int8(34197504,534336,968,1) -> Int8(17098752,133584,484,1) ***************
[10/31/2023-17:56:40] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv (CaskConvolution)
[10/31/2023-17:56:40] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:40] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:56:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:40] [V] [TRT] *************** Autotuning format combination: Int8(8549376,534336:4,968,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:56:40] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv (CudaDepthwiseConvolution)
[10/31/2023-17:56:40] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:40] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv (FusedConvActConvolution)
[10/31/2023-17:56:40] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:40] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv (CaskConvolution)
[10/31/2023-17:56:40] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0x8846c5916f34f7e9
[10/31/2023-17:56:40] [V] [TRT] Tactic: 0x8846c5916f34f7e9 Time: 17.9871
[10/31/2023-17:56:40] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 0xf6833cac33ebf690
[10/31/2023-17:56:40] [V] [TRT] Tactic: 0xf6833cac33ebf690 Time: 16.6327
[10/31/2023-17:56:40] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 0xa49b04dbd5d9448a
[10/31/2023-17:56:40] [V] [TRT] Tactic: 0xa49b04dbd5d9448a Time: 16.6267
[10/31/2023-17:56:40] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: 0xe0658ca00f36ea3e
[10/31/2023-17:56:40] [V] [TRT] Tactic: 0xe0658ca00f36ea3e Time: 19.5763
[10/31/2023-17:56:40] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 0x0270c2170caa55f8
[10/31/2023-17:56:41] [V] [TRT] Tactic: 0x0270c2170caa55f8 Time: 18.0913
[10/31/2023-17:56:41] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 0xcc3c5501ab7b5867
[10/31/2023-17:56:41] [V] [TRT] Tactic: 0xcc3c5501ab7b5867 Time: 16.7565
[10/31/2023-17:56:41] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 0x50f6b3d3d5866716
[10/31/2023-17:56:41] [V] [TRT] Tactic: 0x50f6b3d3d5866716 Time: 17.1009
[10/31/2023-17:56:41] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 0x1e2179aecb150578
[10/31/2023-17:56:41] [V] [TRT] Tactic: 0x1e2179aecb150578 Time: 17.434
[10/31/2023-17:56:41] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x1f0263042c683192
[10/31/2023-17:56:41] [V] [TRT] Tactic: 0x1f0263042c683192 Time: 17.0723
[10/31/2023-17:56:41] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 0x9aa46c15c2a1d8a7
[10/31/2023-17:56:41] [V] [TRT] Tactic: 0x9aa46c15c2a1d8a7 Time: 16.9319
[10/31/2023-17:56:41] [V] [TRT] Fastest Tactic: 0xa49b04dbd5d9448a Time: 16.6267
[10/31/2023-17:56:41] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:56:41] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:41] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa49b04dbd5d9448a
[10/31/2023-17:56:41] [V] [TRT] *************** Autotuning format combination: Int8(8549376,534336:4,968,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:56:41] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv (CaskConvolution)
[10/31/2023-17:56:41] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4
[10/31/2023-17:56:42] [V] [TRT] Tactic: 0xeb494e2e67f079d4 Time: 18.0478
[10/31/2023-17:56:42] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18
[10/31/2023-17:56:42] [V] [TRT] Tactic: 0x0f47434ace2a7d18 Time: 17.9203
[10/31/2023-17:56:42] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 0x0da8f7df8cfd2e37
[10/31/2023-17:56:42] [V] [TRT] Tactic: 0x0da8f7df8cfd2e37 Time: 16.7483
[10/31/2023-17:56:42] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: 0x2cf0cc5b8a069ae7
[10/31/2023-17:56:42] [V] [TRT] Tactic: 0x2cf0cc5b8a069ae7 Time: 19.5647
[10/31/2023-17:56:42] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: 0x320c30eda409da30
[10/31/2023-17:56:42] [V] [TRT] Tactic: 0x320c30eda409da30 Time: 17.0672
[10/31/2023-17:56:42] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: 0xf3f27c40da3380fe
[10/31/2023-17:56:42] [V] [TRT] Tactic: 0xf3f27c40da3380fe Time: 17.0494
[10/31/2023-17:56:42] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: 0xc4f09503ebafdf51
[10/31/2023-17:56:42] [V] [TRT] Tactic: 0xc4f09503ebafdf51 Time: 16.6485
[10/31/2023-17:56:42] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 0xd4d010e83172b290
[10/31/2023-17:56:43] [V] [TRT] Tactic: 0xd4d010e83172b290 Time: 17.3785
[10/31/2023-17:56:43] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: 0x3c0834dbcd849973
[10/31/2023-17:56:43] [V] [TRT] Tactic: 0x3c0834dbcd849973 Time: 16.6841
[10/31/2023-17:56:43] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: 0xc911b61f7a5c0315
[10/31/2023-17:56:43] [V] [TRT] Tactic: 0xc911b61f7a5c0315 Time: 16.9249
[10/31/2023-17:56:43] [V] [TRT] Fastest Tactic: 0xc4f09503ebafdf51 Time: 16.6485
[10/31/2023-17:56:43] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:56:43] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:43] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc4f09503ebafdf51
[10/31/2023-17:56:43] [V] [TRT] *************** Autotuning format combination: Int8(1068672,534336:32,968,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:56:43] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv (CudaGroupConvolution)
[10/31/2023-17:56:43] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:43] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv (CudaDepthwiseConvolution)
[10/31/2023-17:56:43] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:43] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv (FusedConvActConvolution)
[10/31/2023-17:56:43] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:43] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv (CaskConvolution)
[10/31/2023-17:56:43] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18
[10/31/2023-17:56:43] [V] [TRT] Tactic: 0x4cf18ad3e295ea18 Time: 7.01273
[10/31/2023-17:56:43] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5
[10/31/2023-17:56:43] [V] [TRT] Tactic: 0x89c1d75fe77808d5 Time: 5.03628
[10/31/2023-17:56:43] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363
[10/31/2023-17:56:43] [V] [TRT] Tactic: 0xf56c0ac895d82363 Time: 6.00418
[10/31/2023-17:56:43] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673
[10/31/2023-17:56:43] [V] [TRT] Tactic: 0xf33711e7c9ed4673 Time: 3.28867
[10/31/2023-17:56:43] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780
[10/31/2023-17:56:43] [V] [TRT] Tactic: 0xeb33cf0799237780 Time: 7.01555
[10/31/2023-17:56:43] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50
[10/31/2023-17:56:43] [V] [TRT] Tactic: 0x3818ca0093333b50 Time: 4.24822
[10/31/2023-17:56:43] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7
[10/31/2023-17:56:43] [V] [TRT] Tactic: 0xee2fce9480a52be7 Time: 10.0847
[10/31/2023-17:56:43] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x048dc59a1807b5bb
[10/31/2023-17:56:43] [V] [TRT] Tactic: 0x048dc59a1807b5bb Time: 12.0265
[10/31/2023-17:56:43] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af
[10/31/2023-17:56:44] [V] [TRT] Tactic: 0x26a29d5b8b3f62af Time: 3.749
[10/31/2023-17:56:44] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee
[10/31/2023-17:56:44] [V] [TRT] Tactic: 0xd277f13d771603ee Time: 5.10666
[10/31/2023-17:56:44] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52
[10/31/2023-17:56:44] [V] [TRT] Tactic: 0x11b75d98d540ee52 Time: 6.91373
[10/31/2023-17:56:44] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071
[10/31/2023-17:56:44] [V] [TRT] Tactic: 0x2f34f689bfca5071 Time: 7.87804
[10/31/2023-17:56:44] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4
[10/31/2023-17:56:44] [V] [TRT] Tactic: 0x9f6920f4a40549f4 Time: 4.94674
[10/31/2023-17:56:44] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e
[10/31/2023-17:56:44] [V] [TRT] Tactic: 0x2ff8bc85c8ced89e Time: 5.31246
[10/31/2023-17:56:44] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0
[10/31/2023-17:56:44] [V] [TRT] Tactic: 0x5e1b696f91f572c0 Time: 4.71844
[10/31/2023-17:56:44] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26
[10/31/2023-17:56:44] [V] [TRT] Tactic: 0x623894f465c90f26 Time: 3.80805
[10/31/2023-17:56:44] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423
[10/31/2023-17:56:44] [V] [TRT] Tactic: 0x955d593b1135a423 Time: 8.34689
[10/31/2023-17:56:44] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d
[10/31/2023-17:56:44] [V] [TRT] Tactic: 0x2728bbf79375f71d Time: 3.38777
[10/31/2023-17:56:44] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df
[10/31/2023-17:56:44] [V] [TRT] Tactic: 0x87620679fb5f37df Time: 3.4268
[10/31/2023-17:56:44] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b
[10/31/2023-17:56:44] [V] [TRT] Tactic: 0x705baf38e41eee0b Time: 4.49492
[10/31/2023-17:56:44] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd
[10/31/2023-17:56:44] [V] [TRT] Tactic: 0xd30e9f770878c3fd Time: 4.07254
[10/31/2023-17:56:44] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0
[10/31/2023-17:56:44] [V] [TRT] Tactic: 0x41b4640eb250ffd0 Time: 6.82957
[10/31/2023-17:56:44] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6
[10/31/2023-17:56:44] [V] [TRT] Tactic: 0xd3d41ef6de22d9b6 Time: 3.69355
[10/31/2023-17:56:44] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4805f65acfc5cf25
[10/31/2023-17:56:44] [V] [TRT] Tactic: 0x4805f65acfc5cf25 Time: 11.4641
[10/31/2023-17:56:44] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d
[10/31/2023-17:56:44] [V] [TRT] Tactic: 0x8d50646eff0cde6d Time: 3.17097
[10/31/2023-17:56:44] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c
[10/31/2023-17:56:44] [V] [TRT] Tactic: 0xf8d4389f60adfa3c Time: 3.47814
[10/31/2023-17:56:44] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163
[10/31/2023-17:56:44] [V] [TRT] Tactic: 0x80cce6d8f75dc163 Time: 3.62309
[10/31/2023-17:56:44] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96
[10/31/2023-17:56:44] [V] [TRT] Tactic: 0xea88b51105501f96 Time: 3.09032
[10/31/2023-17:56:44] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468
[10/31/2023-17:56:45] [V] [TRT] Tactic: 0x53554c607d072468 Time: 5.71209
[10/31/2023-17:56:45] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab
[10/31/2023-17:56:45] [V] [TRT] Tactic: 0x70ccdad7e8ced9ab Time: 3.98187
[10/31/2023-17:56:45] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9
[10/31/2023-17:56:45] [V] [TRT] Tactic: 0x99901a83d7176ba9 Time: 4.09206
[10/31/2023-17:56:45] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7
[10/31/2023-17:56:45] [V] [TRT] Tactic: 0x8e1e9d670448aca7 Time: 7.54854
[10/31/2023-17:56:45] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02
[10/31/2023-17:56:45] [V] [TRT] Tactic: 0x204424cb4da28c02 Time: 3.67562
[10/31/2023-17:56:45] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49
[10/31/2023-17:56:45] [V] [TRT] Tactic: 0x0fbfd5fa9864ab49 Time: 6.71521
[10/31/2023-17:56:45] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb
[10/31/2023-17:56:45] [V] [TRT] Tactic: 0x9d0f90e0cec890bb Time: 6.40258
[10/31/2023-17:56:45] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4
[10/31/2023-17:56:45] [V] [TRT] Tactic: 0xa92dc4358ef627d4 Time: 7.57199
[10/31/2023-17:56:45] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530
[10/31/2023-17:56:45] [V] [TRT] Tactic: 0x7dcb6d55062fd530 Time: 3.59336
[10/31/2023-17:56:45] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792
[10/31/2023-17:56:45] [V] [TRT] Tactic: 0x8b2f6e180c235792 Time: 5.24643
[10/31/2023-17:56:45] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986
[10/31/2023-17:56:45] [V] [TRT] Tactic: 0xd3f592fae61c7986 Time: 7.10059
[10/31/2023-17:56:45] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8
[10/31/2023-17:56:45] [V] [TRT] Tactic: 0x60edc1e1753cd5b8 Time: 7.29286
[10/31/2023-17:56:45] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393
[10/31/2023-17:56:45] [V] [TRT] Tactic: 0x0f6ba1e5b0320393 Time: 5.51989
[10/31/2023-17:56:45] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662
[10/31/2023-17:56:45] [V] [TRT] Tactic: 0x4b860619b6031662 Time: 7.33434
[10/31/2023-17:56:45] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da
[10/31/2023-17:56:45] [V] [TRT] Tactic: 0x19b79d0c4ff629da Time: 8.20417
[10/31/2023-17:56:45] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49
[10/31/2023-17:56:45] [V] [TRT] Tactic: 0x136c072084f41e49 Time: 8.26062
[10/31/2023-17:56:45] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23
[10/31/2023-17:56:46] [V] [TRT] Tactic: 0x4749124f62d8bd23 Time: 10.0646
[10/31/2023-17:56:46] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b
[10/31/2023-17:56:46] [V] [TRT] Tactic: 0x444f830f80ff098b Time: 5.22055
[10/31/2023-17:56:46] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655
[10/31/2023-17:56:46] [V] [TRT] Tactic: 0x671c943720ba8655 Time: 3.81667
[10/31/2023-17:56:46] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23
[10/31/2023-17:56:46] [V] [TRT] Tactic: 0xeb9516439d87ac23 Time: 4.97751
[10/31/2023-17:56:46] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50
[10/31/2023-17:56:46] [V] [TRT] Tactic: 0xef01fb6e433afa50 Time: 3.69582
[10/31/2023-17:56:46] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4
[10/31/2023-17:56:46] [V] [TRT] Tactic: 0xbb88763c3b0e94d4 Time: 5.32908
[10/31/2023-17:56:46] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0
[10/31/2023-17:56:46] [V] [TRT] Tactic: 0x4b476758492c15b0 Time: 6.66256
[10/31/2023-17:56:46] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd
[10/31/2023-17:56:46] [V] [TRT] Tactic: 0x554e2e252e28b3fd Time: 6.52923
[10/31/2023-17:56:46] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d
[10/31/2023-17:56:46] [V] [TRT] Tactic: 0x9262f8f95beb428d Time: 5.97455
[10/31/2023-17:56:46] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac
[10/31/2023-17:56:46] [V] [TRT] Tactic: 0xd9f3bbc3e16b16ac Time: 3.63557
[10/31/2023-17:56:46] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3
[10/31/2023-17:56:46] [V] [TRT] Tactic: 0x4f6730716f78f4a3 Time: 3.23536
[10/31/2023-17:56:46] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d
[10/31/2023-17:56:46] [V] [TRT] Tactic: 0x028e842ce51fbd9d Time: 6.54936
[10/31/2023-17:56:46] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c
[10/31/2023-17:56:46] [V] [TRT] Tactic: 0xe37d64a29ca3101c Time: 3.96127
[10/31/2023-17:56:46] [V] [TRT] Fastest Tactic: 0xea88b51105501f96 Time: 3.09032
[10/31/2023-17:56:46] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:56:46] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:56:46] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xea88b51105501f96
[10/31/2023-17:56:46] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:56:46] [V] [TRT] *************** Autotuning format combination: Float(17098752,133584,484,1), Float(128,1,1,1), Float(128,1,1,1) -> Float(17098752,133584,484,1) ***************
[10/31/2023-17:56:46] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 112) [ElementWise], PWN(PWN((Unnamed Layer* 109) [Constant] + (Unnamed Layer* 110) [ElementWise], (Unnamed Layer* 111) [Unary]), (Unnamed Layer* 113) [ElementWise])) (PointWiseV2)
[10/31/2023-17:56:47] [V] [TRT] Tactic: 0x0000000000000000 Time: 12.1186
[10/31/2023-17:56:47] [V] [TRT] Tactic: 0x0000000000000001 Time: 9.15262
[10/31/2023-17:56:48] [V] [TRT] Tactic: 0x0000000000000002 Time: 8.6465
[10/31/2023-17:56:48] [V] [TRT] Tactic: 0x0000000000000003 Time: 7.05095
[10/31/2023-17:56:49] [V] [TRT] Tactic: 0x0000000000000004 Time: 6.82827
[10/31/2023-17:56:49] [V] [TRT] Tactic: 0x0000000000000005 Time: 7.12334
[10/31/2023-17:56:50] [V] [TRT] Tactic: 0x0000000000000006 Time: 6.18453
[10/31/2023-17:56:51] [V] [TRT] Tactic: 0x0000000000000007 Time: 5.50121
[10/31/2023-17:56:51] [V] [TRT] Tactic: 0x0000000000000008 Time: 6.38109
[10/31/2023-17:56:52] [V] [TRT] Tactic: 0x0000000000000009 Time: 6.51185
[10/31/2023-17:56:52] [V] [TRT] Tactic: 0x000000000000001c Time: 11.0612
[10/31/2023-17:56:52] [V] [TRT] Fastest Tactic: 0x0000000000000007 Time: 5.50121
[10/31/2023-17:56:52] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 112) [ElementWise], PWN(PWN((Unnamed Layer* 109) [Constant] + (Unnamed Layer* 110) [ElementWise], (Unnamed Layer* 111) [Unary]), (Unnamed Layer* 113) [ElementWise])) (PointWise)
[10/31/2023-17:56:52] [V] [TRT] PointWise has no valid tactics for this config, skipping
[10/31/2023-17:56:52] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000007
[10/31/2023-17:56:52] [V] [TRT] *************** Autotuning format combination: Float(17098752,1,61952,128), Float(128,1,128,128), Float(128,1,128,128) -> Float(17098752,1,61952,128) ***************
[10/31/2023-17:56:52] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 112) [ElementWise], PWN(PWN((Unnamed Layer* 109) [Constant] + (Unnamed Layer* 110) [ElementWise], (Unnamed Layer* 111) [Unary]), (Unnamed Layer* 113) [ElementWise])) (PointWiseV2)
[10/31/2023-17:56:53] [V] [TRT] Tactic: 0x0000000000000000 Time: 13.5355
[10/31/2023-17:56:53] [V] [TRT] Tactic: 0x0000000000000001 Time: 9.50147
[10/31/2023-17:56:54] [V] [TRT] Tactic: 0x0000000000000002 Time: 9.14772
[10/31/2023-17:56:54] [V] [TRT] Tactic: 0x0000000000000003 Time: 7.07611
[10/31/2023-17:56:55] [V] [TRT] Tactic: 0x0000000000000004 Time: 6.97557
[10/31/2023-17:56:56] [V] [TRT] Tactic: 0x0000000000000005 Time: 7.33856
[10/31/2023-17:56:56] [V] [TRT] Tactic: 0x0000000000000006 Time: 8.18557
[10/31/2023-17:56:57] [V] [TRT] Tactic: 0x0000000000000007 Time: 8.05973
[10/31/2023-17:56:57] [V] [TRT] Tactic: 0x0000000000000008 Time: 8.99248
[10/31/2023-17:56:58] [V] [TRT] Tactic: 0x0000000000000009 Time: 9.04945
[10/31/2023-17:56:58] [V] [TRT] Tactic: 0x000000000000001c Time: 13.1096
[10/31/2023-17:56:58] [V] [TRT] Fastest Tactic: 0x0000000000000004 Time: 6.97557
[10/31/2023-17:56:58] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 112) [ElementWise], PWN(PWN((Unnamed Layer* 109) [Constant] + (Unnamed Layer* 110) [ElementWise], (Unnamed Layer* 111) [Unary]), (Unnamed Layer* 113) [ElementWise])) (PointWise)
[10/31/2023-17:56:58] [V] [TRT] PointWise has no valid tactics for this config, skipping
[10/31/2023-17:56:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000004
[10/31/2023-17:56:58] [V] [TRT] *************** Autotuning format combination: Float(4274688,1:4,15488,32), Float(32,1:4,32,32), Float(32,1:4,32,32) -> Float(4274688,1:4,15488,32) ***************
[10/31/2023-17:56:58] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 112) [ElementWise], PWN(PWN((Unnamed Layer* 109) [Constant] + (Unnamed Layer* 110) [ElementWise], (Unnamed Layer* 111) [Unary]), (Unnamed Layer* 113) [ElementWise])) (PointWiseV2)
[10/31/2023-17:56:59] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.45488
[10/31/2023-17:57:00] [V] [TRT] Tactic: 0x0000000000000001 Time: 7.409
[10/31/2023-17:57:00] [V] [TRT] Tactic: 0x0000000000000002 Time: 6.94848
[10/31/2023-17:57:01] [V] [TRT] Tactic: 0x0000000000000003 Time: 8.17312
[10/31/2023-17:57:01] [V] [TRT] Tactic: 0x0000000000000004 Time: 9.18439
[10/31/2023-17:57:02] [V] [TRT] Tactic: 0x0000000000000005 Time: 8.22287
[10/31/2023-17:57:03] [V] [TRT] Tactic: 0x0000000000000006 Time: 9.69716
[10/31/2023-17:57:03] [V] [TRT] Tactic: 0x0000000000000007 Time: 11.1803
[10/31/2023-17:57:04] [V] [TRT] Tactic: 0x0000000000000008 Time: 13.5346
[10/31/2023-17:57:05] [V] [TRT] Tactic: 0x0000000000000009 Time: 15.1354
[10/31/2023-17:57:05] [V] [TRT] Tactic: 0x000000000000000a Time: 9.34192
[10/31/2023-17:57:06] [V] [TRT] Tactic: 0x000000000000000b Time: 6.92264
[10/31/2023-17:57:07] [V] [TRT] Tactic: 0x000000000000000c Time: 7.01125
[10/31/2023-17:57:07] [V] [TRT] Tactic: 0x000000000000000d Time: 7.50558
[10/31/2023-17:57:08] [V] [TRT] Tactic: 0x000000000000000e Time: 8.75805
[10/31/2023-17:57:08] [V] [TRT] Tactic: 0x000000000000000f Time: 7.72966
[10/31/2023-17:57:09] [V] [TRT] Tactic: 0x0000000000000010 Time: 8.78584
[10/31/2023-17:57:10] [V] [TRT] Tactic: 0x0000000000000011 Time: 10.0775
[10/31/2023-17:57:10] [V] [TRT] Tactic: 0x0000000000000012 Time: 10.8301
[10/31/2023-17:57:11] [V] [TRT] Tactic: 0x0000000000000013 Time: 10.024
[10/31/2023-17:57:12] [V] [TRT] Tactic: 0x0000000000000014 Time: 14.5951
[10/31/2023-17:57:12] [V] [TRT] Tactic: 0x0000000000000015 Time: 10.2104
[10/31/2023-17:57:13] [V] [TRT] Tactic: 0x0000000000000016 Time: 7.45167
[10/31/2023-17:57:13] [V] [TRT] Tactic: 0x0000000000000017 Time: 8.75957
[10/31/2023-17:57:14] [V] [TRT] Tactic: 0x000000000000001c Time: 6.26251
[10/31/2023-17:57:14] [V] [TRT] Tactic: 0x000000000000001d Time: 9.33734
[10/31/2023-17:57:15] [V] [TRT] Tactic: 0x000000000000001e Time: 14.339
[10/31/2023-17:57:15] [V] [TRT] Fastest Tactic: 0x000000000000001c Time: 6.26251
[10/31/2023-17:57:15] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 112) [ElementWise], PWN(PWN((Unnamed Layer* 109) [Constant] + (Unnamed Layer* 110) [ElementWise], (Unnamed Layer* 111) [Unary]), (Unnamed Layer* 113) [ElementWise])) (PointWise)
[10/31/2023-17:57:15] [V] [TRT] PointWise has no valid tactics for this config, skipping
[10/31/2023-17:57:15] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001c
[10/31/2023-17:57:15] [V] [TRT] *************** Autotuning format combination: Float(534336,133584:32,484,1), Float(4,1:32,1,1), Float(4,1:32,1,1) -> Float(534336,133584:32,484,1) ***************
[10/31/2023-17:57:15] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 112) [ElementWise], PWN(PWN((Unnamed Layer* 109) [Constant] + (Unnamed Layer* 110) [ElementWise], (Unnamed Layer* 111) [Unary]), (Unnamed Layer* 113) [ElementWise])) (PointWiseV2)
[10/31/2023-17:57:15] [V] [TRT] Tactic: 0x0000000000000018 Time: 6.16034
[10/31/2023-17:57:16] [V] [TRT] Tactic: 0x0000000000000019 Time: 5.46372
[10/31/2023-17:57:17] [V] [TRT] Tactic: 0x000000000000001a Time: 4.90285
[10/31/2023-17:57:17] [V] [TRT] Tactic: 0x000000000000001b Time: 4.74897
[10/31/2023-17:57:18] [V] [TRT] Tactic: 0x000000000000001f Time: 6.03707
[10/31/2023-17:57:18] [V] [TRT] Fastest Tactic: 0x000000000000001b Time: 4.74897
[10/31/2023-17:57:18] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 112) [ElementWise], PWN(PWN((Unnamed Layer* 109) [Constant] + (Unnamed Layer* 110) [ElementWise], (Unnamed Layer* 111) [Unary]), (Unnamed Layer* 113) [ElementWise])) (PointWise)
[10/31/2023-17:57:18] [V] [TRT] PointWise has no valid tactics for this config, skipping
[10/31/2023-17:57:18] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001b
[10/31/2023-17:57:18] [V] [TRT] *************** Autotuning format combination: Float(1:4,133584,484,1), Float(1:4,1,1,1), Float(1:4,1,1,1) -> Float(1:4,133584,484,1) ***************
[10/31/2023-17:57:18] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 112) [ElementWise], PWN(PWN((Unnamed Layer* 109) [Constant] + (Unnamed Layer* 110) [ElementWise], (Unnamed Layer* 111) [Unary]), (Unnamed Layer* 113) [ElementWise])) (PointWiseV2)
[10/31/2023-17:57:18] [V] [TRT] Tactic: 0x000000000000001c Time: 26.8473
[10/31/2023-17:57:19] [V] [TRT] Tactic: 0x000000000000001d Time: 38.5238
[10/31/2023-17:57:20] [V] [TRT] Tactic: 0x000000000000001e Time: 60.376
[10/31/2023-17:57:20] [V] [TRT] Fastest Tactic: 0x000000000000001c Time: 26.8473
[10/31/2023-17:57:20] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001c
[10/31/2023-17:57:20] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:57:20] [V] [TRT] *************** Autotuning format combination: Float(17098752,133584,484,1) -> Float(17098752,133584,484,1) ***************
[10/31/2023-17:57:20] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.downsample.1.weight + (Unnamed Layer* 105) [Shuffle] + (Unnamed Layer* 114) [ElementWise] + backbone.layer2.0.bn2.bias + Identity_2 + (Unnamed Layer* 106) [Shuffle] + /backbone/layer2/layer2.0/downsample/downsample.1/BatchNormalization (Scale)
[10/31/2023-17:57:21] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.35279
[10/31/2023-17:57:21] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 3.35279
[10/31/2023-17:57:21] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0x0000000000000000
[10/31/2023-17:57:21] [V] [TRT] *************** Autotuning format combination: Float(17098752,1,61952,128) -> Float(17098752,1,61952,128) ***************
[10/31/2023-17:57:21] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.downsample.1.weight + (Unnamed Layer* 105) [Shuffle] + (Unnamed Layer* 114) [ElementWise] + backbone.layer2.0.bn2.bias + Identity_2 + (Unnamed Layer* 106) [Shuffle] + /backbone/layer2/layer2.0/downsample/downsample.1/BatchNormalization (Scale)
[10/31/2023-17:57:21] [V] [TRT] Scale has no valid tactics for this config, skipping
[10/31/2023-17:57:21] [V] [TRT] *************** Autotuning format combination: Float(4274688,1:4,15488,32) -> Float(4274688,1:4,15488,32) ***************
[10/31/2023-17:57:21] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.downsample.1.weight + (Unnamed Layer* 105) [Shuffle] + (Unnamed Layer* 114) [ElementWise] + backbone.layer2.0.bn2.bias + Identity_2 + (Unnamed Layer* 106) [Shuffle] + /backbone/layer2/layer2.0/downsample/downsample.1/BatchNormalization (Scale)
[10/31/2023-17:57:21] [V] [TRT] Scale has no valid tactics for this config, skipping
[10/31/2023-17:57:21] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:57:21] [V] [TRT] *************** Autotuning format combination: Int8(4274688,133584:4,484,1), Float(17098752,133584,484,1) -> Float(17098752,133584,484,1) ***************
[10/31/2023-17:57:21] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu (CaskConvolution)
[10/31/2023-17:57:21] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 0xb29abdd00304c881
[10/31/2023-17:57:21] [V] [TRT] Tactic: 0xb29abdd00304c881 Time: 34.2571
[10/31/2023-17:57:21] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: 0x127de12a1f1a4809
[10/31/2023-17:57:21] [V] [TRT] Tactic: 0x127de12a1f1a4809 Time: 39.2
[10/31/2023-17:57:21] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99
[10/31/2023-17:57:21] [V] [TRT] Tactic: 0xfe80445f7bb61a99 Time: 35.7741
[10/31/2023-17:57:22] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 0x14ef32afe5695907
[10/31/2023-17:57:22] [V] [TRT] Tactic: 0x14ef32afe5695907 Time: 33.1957
[10/31/2023-17:57:22] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 0xec391424db39a74f
[10/31/2023-17:57:22] [V] [TRT] Tactic: 0xec391424db39a74f Time: 34.7724
[10/31/2023-17:57:22] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2
[10/31/2023-17:57:22] [V] [TRT] Tactic: 0x69c4e2ca38eadce2 Time: 34.1622
[10/31/2023-17:57:22] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 0x4d9d0d03129bceb1
[10/31/2023-17:57:23] [V] [TRT] Tactic: 0x4d9d0d03129bceb1 Time: 33.8527
[10/31/2023-17:57:23] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 0xe01ccc14da28fa6f
[10/31/2023-17:57:23] [V] [TRT] Tactic: 0xe01ccc14da28fa6f Time: 36.0974
[10/31/2023-17:57:23] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 0x1b0534177b414e71
[10/31/2023-17:57:23] [V] [TRT] Tactic: 0x1b0534177b414e71 Time: 33.5318
[10/31/2023-17:57:23] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 0x2cf4271504d30e29
[10/31/2023-17:57:24] [V] [TRT] Tactic: 0x2cf4271504d30e29 Time: 33.2874
[10/31/2023-17:57:24] [V] [TRT] Fastest Tactic: 0x14ef32afe5695907 Time: 33.1957
[10/31/2023-17:57:24] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu (CaskFlattenConvolution)
[10/31/2023-17:57:24] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:24] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x14ef32afe5695907
[10/31/2023-17:57:24] [V] [TRT] *************** Autotuning format combination: Int8(534336,133584:32,484,1), Float(17098752,133584,484,1) -> Float(17098752,133584,484,1) ***************
[10/31/2023-17:57:24] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu (CaskConvolution)
[10/31/2023-17:57:24] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84
[10/31/2023-17:57:24] [V] [TRT] Tactic: 0x85c1a5f7f239cf84 Time: 14.2205
[10/31/2023-17:57:24] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10
[10/31/2023-17:57:24] [V] [TRT] Tactic: 0x2bcbba39f608bf10 Time: 11.7884
[10/31/2023-17:57:24] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808
[10/31/2023-17:57:24] [V] [TRT] Tactic: 0xac914b235d066808 Time: 10.1005
[10/31/2023-17:57:24] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e
[10/31/2023-17:57:24] [V] [TRT] Tactic: 0x23b890da05937b9e Time: 8.86016
[10/31/2023-17:57:24] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463
[10/31/2023-17:57:24] [V] [TRT] Tactic: 0xa8b56a226b057463 Time: 11.3746
[10/31/2023-17:57:24] [V] [TRT] Fastest Tactic: 0x23b890da05937b9e Time: 8.86016
[10/31/2023-17:57:24] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu (CaskFlattenConvolution)
[10/31/2023-17:57:24] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:24] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x23b890da05937b9e
[10/31/2023-17:57:24] [V] [TRT] *************** Autotuning format combination: Int8(534336,133584:32,484,1), Float(534336,133584:32,484,1) -> Float(534336,133584:32,484,1) ***************
[10/31/2023-17:57:24] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu (CaskConvolution)
[10/31/2023-17:57:24] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c
[10/31/2023-17:57:24] [V] [TRT] Tactic: 0xb936321f82fd390c Time: 11.2289
[10/31/2023-17:57:24] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa
[10/31/2023-17:57:24] [V] [TRT] Tactic: 0x55edef142e02adaa Time: 6.20411
[10/31/2023-17:57:24] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1
[10/31/2023-17:57:24] [V] [TRT] Tactic: 0xe742f4598442d2f1 Time: 8.05671
[10/31/2023-17:57:24] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e
[10/31/2023-17:57:25] [V] [TRT] Tactic: 0x3f2b14dec582741e Time: 11.3919
[10/31/2023-17:57:25] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4
[10/31/2023-17:57:25] [V] [TRT] Tactic: 0x0edd5d0285e564d4 Time: 7.8871
[10/31/2023-17:57:25] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79
[10/31/2023-17:57:25] [V] [TRT] Tactic: 0xb86b920539eb9c79 Time: 6.31617
[10/31/2023-17:57:25] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87
[10/31/2023-17:57:25] [V] [TRT] Tactic: 0xdc1f355deb032b87 Time: 13.8831
[10/31/2023-17:57:25] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb
[10/31/2023-17:57:25] [V] [TRT] Tactic: 0x45f7566cdb2b10fb Time: 12.2384
[10/31/2023-17:57:25] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1
[10/31/2023-17:57:25] [V] [TRT] Tactic: 0xea2b7420057a01c1 Time: 11.8225
[10/31/2023-17:57:25] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e
[10/31/2023-17:57:25] [V] [TRT] Tactic: 0x6986b0c6a136276e Time: 9.80313
[10/31/2023-17:57:25] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922
[10/31/2023-17:57:25] [V] [TRT] Tactic: 0xad886d4d69834922 Time: 7.87357
[10/31/2023-17:57:25] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a
[10/31/2023-17:57:25] [V] [TRT] Tactic: 0x2468d082d0ff7c9a Time: 12.8546
[10/31/2023-17:57:25] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61
[10/31/2023-17:57:26] [V] [TRT] Tactic: 0x8141573686849b61 Time: 8.01275
[10/31/2023-17:57:26] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377
[10/31/2023-17:57:26] [V] [TRT] Tactic: 0xb5f710b331ba8377 Time: 7.76385
[10/31/2023-17:57:26] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4
[10/31/2023-17:57:26] [V] [TRT] Tactic: 0x748f926574b7bdc4 Time: 12.0701
[10/31/2023-17:57:26] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f
[10/31/2023-17:57:26] [V] [TRT] Tactic: 0x0e07dc8353bf7e9f Time: 6.848
[10/31/2023-17:57:26] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800
[10/31/2023-17:57:26] [V] [TRT] Tactic: 0x47b1629a4bff4800 Time: 7.66431
[10/31/2023-17:57:26] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e
[10/31/2023-17:57:26] [V] [TRT] Tactic: 0x5f1a472d416ff35e Time: 9.76127
[10/31/2023-17:57:26] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33
[10/31/2023-17:57:26] [V] [TRT] Tactic: 0x991db9fd58152c33 Time: 6.86503
[10/31/2023-17:57:26] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e
[10/31/2023-17:57:26] [V] [TRT] Tactic: 0xd14bd6d95fefd45e Time: 14.4719
[10/31/2023-17:57:26] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9
[10/31/2023-17:57:26] [V] [TRT] Tactic: 0x2d8ab2aa0639fda9 Time: 9.38119
[10/31/2023-17:57:26] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda
[10/31/2023-17:57:26] [V] [TRT] Tactic: 0x445983715412fbda Time: 14.9215
[10/31/2023-17:57:26] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec
[10/31/2023-17:57:27] [V] [TRT] Tactic: 0xa2f15b0a75b7dcec Time: 12.0116
[10/31/2023-17:57:27] [V] [TRT] Fastest Tactic: 0x55edef142e02adaa Time: 6.20411
[10/31/2023-17:57:27] [V] [TRT] --------------- Timing Runner: backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu (CaskFlattenConvolution)
[10/31/2023-17:57:27] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:27] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x55edef142e02adaa
[10/31/2023-17:57:27] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:57:27] [V] [TRT] *************** Autotuning format combination: Int8(17098752,133584,484,1) -> Int8(17098752,133584,484,1) ***************
[10/31/2023-17:57:27] [V] [TRT] --------------- Timing Runner: backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv (CaskConvolution)
[10/31/2023-17:57:27] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:27] [V] [TRT] --------------- Timing Runner: backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:57:27] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:27] [V] [TRT] *************** Autotuning format combination: Int8(4274688,133584:4,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:57:27] [V] [TRT] --------------- Timing Runner: backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv (CudaDepthwiseConvolution)
[10/31/2023-17:57:27] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:27] [V] [TRT] --------------- Timing Runner: backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv (FusedConvActConvolution)
[10/31/2023-17:57:27] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:27] [V] [TRT] --------------- Timing Runner: backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv (CaskConvolution)
[10/31/2023-17:57:27] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0x8846c5916f34f7e9
[10/31/2023-17:57:27] [V] [TRT] Tactic: 0x8846c5916f34f7e9 Time: 35.21
[10/31/2023-17:57:27] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 0xf6833cac33ebf690
[10/31/2023-17:57:27] [V] [TRT] Tactic: 0xf6833cac33ebf690 Time: 32.749
[10/31/2023-17:57:27] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 0xa49b04dbd5d9448a
[10/31/2023-17:57:27] [V] [TRT] Tactic: 0xa49b04dbd5d9448a Time: 32.7548
[10/31/2023-17:57:27] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: 0xe0658ca00f36ea3e
[10/31/2023-17:57:28] [V] [TRT] Tactic: 0xe0658ca00f36ea3e Time: 36.1954
[10/31/2023-17:57:28] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 0x0270c2170caa55f8
[10/31/2023-17:57:28] [V] [TRT] Tactic: 0x0270c2170caa55f8 Time: 35.4099
[10/31/2023-17:57:28] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 0xcc3c5501ab7b5867
[10/31/2023-17:57:28] [V] [TRT] Tactic: 0xcc3c5501ab7b5867 Time: 33.0958
[10/31/2023-17:57:28] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 0x50f6b3d3d5866716
[10/31/2023-17:57:29] [V] [TRT] Tactic: 0x50f6b3d3d5866716 Time: 33.7937
[10/31/2023-17:57:29] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 0x1e2179aecb150578
[10/31/2023-17:57:29] [V] [TRT] Tactic: 0x1e2179aecb150578 Time: 34.1225
[10/31/2023-17:57:29] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x1f0263042c683192
[10/31/2023-17:57:29] [V] [TRT] Tactic: 0x1f0263042c683192 Time: 33.675
[10/31/2023-17:57:29] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 0x9aa46c15c2a1d8a7
[10/31/2023-17:57:29] [V] [TRT] Tactic: 0x9aa46c15c2a1d8a7 Time: 33.3351
[10/31/2023-17:57:29] [V] [TRT] Fastest Tactic: 0xf6833cac33ebf690 Time: 32.749
[10/31/2023-17:57:29] [V] [TRT] --------------- Timing Runner: backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:57:29] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xf6833cac33ebf690
[10/31/2023-17:57:29] [V] [TRT] *************** Autotuning format combination: Int8(4274688,133584:4,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:57:29] [V] [TRT] --------------- Timing Runner: backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv (CaskConvolution)
[10/31/2023-17:57:29] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4
[10/31/2023-17:57:30] [V] [TRT] Tactic: 0xeb494e2e67f079d4 Time: 35.3935
[10/31/2023-17:57:30] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18
[10/31/2023-17:57:30] [V] [TRT] Tactic: 0x0f47434ace2a7d18 Time: 35.2336
[10/31/2023-17:57:30] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 0x0da8f7df8cfd2e37
[10/31/2023-17:57:30] [V] [TRT] Tactic: 0x0da8f7df8cfd2e37 Time: 33.0812
[10/31/2023-17:57:30] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: 0x2cf0cc5b8a069ae7
[10/31/2023-17:57:31] [V] [TRT] Tactic: 0x2cf0cc5b8a069ae7 Time: 36.2387
[10/31/2023-17:57:31] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: 0x320c30eda409da30
[10/31/2023-17:57:31] [V] [TRT] Tactic: 0x320c30eda409da30 Time: 33.7336
[10/31/2023-17:57:31] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: 0xf3f27c40da3380fe
[10/31/2023-17:57:31] [V] [TRT] Tactic: 0xf3f27c40da3380fe Time: 33.6925
[10/31/2023-17:57:31] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: 0xc4f09503ebafdf51
[10/31/2023-17:57:31] [V] [TRT] Tactic: 0xc4f09503ebafdf51 Time: 32.8902
[10/31/2023-17:57:31] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 0xd4d010e83172b290
[10/31/2023-17:57:32] [V] [TRT] Tactic: 0xd4d010e83172b290 Time: 34.0848
[10/31/2023-17:57:32] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: 0x3c0834dbcd849973
[10/31/2023-17:57:32] [V] [TRT] Tactic: 0x3c0834dbcd849973 Time: 32.8265
[10/31/2023-17:57:32] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: 0xc911b61f7a5c0315
[10/31/2023-17:57:32] [V] [TRT] Tactic: 0xc911b61f7a5c0315 Time: 33.3274
[10/31/2023-17:57:32] [V] [TRT] Fastest Tactic: 0x3c0834dbcd849973 Time: 32.8265
[10/31/2023-17:57:32] [V] [TRT] --------------- Timing Runner: backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:57:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3c0834dbcd849973
[10/31/2023-17:57:32] [V] [TRT] *************** Autotuning format combination: Int8(534336,133584:32,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:57:32] [V] [TRT] --------------- Timing Runner: backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv (CudaGroupConvolution)
[10/31/2023-17:57:32] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:32] [V] [TRT] --------------- Timing Runner: backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv (CudaDepthwiseConvolution)
[10/31/2023-17:57:32] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:32] [V] [TRT] --------------- Timing Runner: backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv (FusedConvActConvolution)
[10/31/2023-17:57:32] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:32] [V] [TRT] --------------- Timing Runner: backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv (CaskConvolution)
[10/31/2023-17:57:32] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18
[10/31/2023-17:57:32] [V] [TRT] Tactic: 0x4cf18ad3e295ea18 Time: 10.837
[10/31/2023-17:57:32] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5
[10/31/2023-17:57:32] [V] [TRT] Tactic: 0x89c1d75fe77808d5 Time: 8.00937
[10/31/2023-17:57:32] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363
[10/31/2023-17:57:33] [V] [TRT] Tactic: 0xf56c0ac895d82363 Time: 8.43872
[10/31/2023-17:57:33] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673
[10/31/2023-17:57:33] [V] [TRT] Tactic: 0xf33711e7c9ed4673 Time: 5.23697
[10/31/2023-17:57:33] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780
[10/31/2023-17:57:33] [V] [TRT] Tactic: 0xeb33cf0799237780 Time: 8.32824
[10/31/2023-17:57:33] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50
[10/31/2023-17:57:33] [V] [TRT] Tactic: 0x3818ca0093333b50 Time: 6.3442
[10/31/2023-17:57:33] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7
[10/31/2023-17:57:33] [V] [TRT] Tactic: 0xee2fce9480a52be7 Time: 13.4123
[10/31/2023-17:57:33] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x048dc59a1807b5bb
[10/31/2023-17:57:33] [V] [TRT] Tactic: 0x048dc59a1807b5bb Time: 19.5146
[10/31/2023-17:57:33] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af
[10/31/2023-17:57:33] [V] [TRT] Tactic: 0x26a29d5b8b3f62af Time: 5.58555
[10/31/2023-17:57:33] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee
[10/31/2023-17:57:33] [V] [TRT] Tactic: 0xd277f13d771603ee Time: 8.01151
[10/31/2023-17:57:33] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52
[10/31/2023-17:57:33] [V] [TRT] Tactic: 0x11b75d98d540ee52 Time: 11.1091
[10/31/2023-17:57:33] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071
[10/31/2023-17:57:33] [V] [TRT] Tactic: 0x2f34f689bfca5071 Time: 9.93194
[10/31/2023-17:57:33] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4
[10/31/2023-17:57:33] [V] [TRT] Tactic: 0x9f6920f4a40549f4 Time: 7.99518
[10/31/2023-17:57:33] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e
[10/31/2023-17:57:33] [V] [TRT] Tactic: 0x2ff8bc85c8ced89e Time: 8.26295
[10/31/2023-17:57:33] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0
[10/31/2023-17:57:34] [V] [TRT] Tactic: 0x5e1b696f91f572c0 Time: 6.82404
[10/31/2023-17:57:34] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26
[10/31/2023-17:57:34] [V] [TRT] Tactic: 0x623894f465c90f26 Time: 5.60457
[10/31/2023-17:57:34] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423
[10/31/2023-17:57:34] [V] [TRT] Tactic: 0x955d593b1135a423 Time: 10.2603
[10/31/2023-17:57:34] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d
[10/31/2023-17:57:34] [V] [TRT] Tactic: 0x2728bbf79375f71d Time: 5.39537
[10/31/2023-17:57:34] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df
[10/31/2023-17:57:34] [V] [TRT] Tactic: 0x87620679fb5f37df Time: 5.33023
[10/31/2023-17:57:34] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b
[10/31/2023-17:57:34] [V] [TRT] Tactic: 0x705baf38e41eee0b Time: 6.57158
[10/31/2023-17:57:34] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd
[10/31/2023-17:57:34] [V] [TRT] Tactic: 0xd30e9f770878c3fd Time: 6.29827
[10/31/2023-17:57:34] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0
[10/31/2023-17:57:34] [V] [TRT] Tactic: 0x41b4640eb250ffd0 Time: 10.7816
[10/31/2023-17:57:34] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6
[10/31/2023-17:57:34] [V] [TRT] Tactic: 0xd3d41ef6de22d9b6 Time: 5.38205
[10/31/2023-17:57:34] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4805f65acfc5cf25
[10/31/2023-17:57:34] [V] [TRT] Tactic: 0x4805f65acfc5cf25 Time: 18.4343
[10/31/2023-17:57:34] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d
[10/31/2023-17:57:34] [V] [TRT] Tactic: 0x8d50646eff0cde6d Time: 5.12814
[10/31/2023-17:57:34] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c
[10/31/2023-17:57:34] [V] [TRT] Tactic: 0xf8d4389f60adfa3c Time: 5.33932
[10/31/2023-17:57:34] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163
[10/31/2023-17:57:34] [V] [TRT] Tactic: 0x80cce6d8f75dc163 Time: 5.52093
[10/31/2023-17:57:34] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96
[10/31/2023-17:57:34] [V] [TRT] Tactic: 0xea88b51105501f96 Time: 4.99211
[10/31/2023-17:57:34] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468
[10/31/2023-17:57:34] [V] [TRT] Tactic: 0x53554c607d072468 Time: 8.04651
[10/31/2023-17:57:34] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab
[10/31/2023-17:57:35] [V] [TRT] Tactic: 0x70ccdad7e8ced9ab Time: 6.24167
[10/31/2023-17:57:35] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9
[10/31/2023-17:57:35] [V] [TRT] Tactic: 0x99901a83d7176ba9 Time: 6.37878
[10/31/2023-17:57:35] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7
[10/31/2023-17:57:35] [V] [TRT] Tactic: 0x8e1e9d670448aca7 Time: 11.2215
[10/31/2023-17:57:35] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02
[10/31/2023-17:57:35] [V] [TRT] Tactic: 0x204424cb4da28c02 Time: 5.50527
[10/31/2023-17:57:35] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49
[10/31/2023-17:57:35] [V] [TRT] Tactic: 0x0fbfd5fa9864ab49 Time: 10.5113
[10/31/2023-17:57:35] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb
[10/31/2023-17:57:35] [V] [TRT] Tactic: 0x9d0f90e0cec890bb Time: 10.3222
[10/31/2023-17:57:35] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4
[10/31/2023-17:57:35] [V] [TRT] Tactic: 0xa92dc4358ef627d4 Time: 11.2824
[10/31/2023-17:57:35] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530
[10/31/2023-17:57:35] [V] [TRT] Tactic: 0x7dcb6d55062fd530 Time: 5.73182
[10/31/2023-17:57:35] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792
[10/31/2023-17:57:35] [V] [TRT] Tactic: 0x8b2f6e180c235792 Time: 8.09846
[10/31/2023-17:57:35] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986
[10/31/2023-17:57:35] [V] [TRT] Tactic: 0xd3f592fae61c7986 Time: 10.9194
[10/31/2023-17:57:35] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8
[10/31/2023-17:57:35] [V] [TRT] Tactic: 0x60edc1e1753cd5b8 Time: 11.042
[10/31/2023-17:57:35] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393
[10/31/2023-17:57:35] [V] [TRT] Tactic: 0x0f6ba1e5b0320393 Time: 8.36204
[10/31/2023-17:57:35] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662
[10/31/2023-17:57:36] [V] [TRT] Tactic: 0x4b860619b6031662 Time: 11.0809
[10/31/2023-17:57:36] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da
[10/31/2023-17:57:36] [V] [TRT] Tactic: 0x19b79d0c4ff629da Time: 13.2138
[10/31/2023-17:57:36] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49
[10/31/2023-17:57:36] [V] [TRT] Tactic: 0x136c072084f41e49 Time: 13.0054
[10/31/2023-17:57:36] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23
[10/31/2023-17:57:36] [V] [TRT] Tactic: 0x4749124f62d8bd23 Time: 12.9746
[10/31/2023-17:57:36] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b
[10/31/2023-17:57:36] [V] [TRT] Tactic: 0x444f830f80ff098b Time: 8.2088
[10/31/2023-17:57:36] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655
[10/31/2023-17:57:36] [V] [TRT] Tactic: 0x671c943720ba8655 Time: 6.21765
[10/31/2023-17:57:36] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23
[10/31/2023-17:57:36] [V] [TRT] Tactic: 0xeb9516439d87ac23 Time: 8.09059
[10/31/2023-17:57:36] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50
[10/31/2023-17:57:36] [V] [TRT] Tactic: 0xef01fb6e433afa50 Time: 5.48465
[10/31/2023-17:57:36] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4
[10/31/2023-17:57:36] [V] [TRT] Tactic: 0xbb88763c3b0e94d4 Time: 8.08387
[10/31/2023-17:57:36] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0
[10/31/2023-17:57:36] [V] [TRT] Tactic: 0x4b476758492c15b0 Time: 8.34908
[10/31/2023-17:57:36] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd
[10/31/2023-17:57:36] [V] [TRT] Tactic: 0x554e2e252e28b3fd Time: 10.5341
[10/31/2023-17:57:36] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d
[10/31/2023-17:57:36] [V] [TRT] Tactic: 0x9262f8f95beb428d Time: 8.10555
[10/31/2023-17:57:36] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0xd9f3bbc3e16b16ac Time: 5.51003
[10/31/2023-17:57:37] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0x4f6730716f78f4a3 Time: 5.28501
[10/31/2023-17:57:37] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0x028e842ce51fbd9d Time: 10.7263
[10/31/2023-17:57:37] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0xe37d64a29ca3101c Time: 6.38235
[10/31/2023-17:57:37] [V] [TRT] Fastest Tactic: 0xea88b51105501f96 Time: 4.99211
[10/31/2023-17:57:37] [V] [TRT] --------------- Timing Runner: backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:57:37] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xea88b51105501f96
[10/31/2023-17:57:37] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:57:37] [V] [TRT] *************** Autotuning format combination: Int8(4274688,133584:4,484,1), Float(17098752,133584,484,1) -> Float(17098752,133584,484,1) ***************
[10/31/2023-17:57:37] [V] [TRT] *************** Autotuning format combination: Int8(534336,133584:32,484,1), Float(17098752,133584,484,1) -> Float(17098752,133584,484,1) ***************
[10/31/2023-17:57:37] [V] [TRT] *************** Autotuning format combination: Int8(534336,133584:32,484,1), Float(534336,133584:32,484,1) -> Float(534336,133584:32,484,1) ***************
[10/31/2023-17:57:37] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:57:37] [V] [TRT] *************** Autotuning format combination: Int8(4274688,133584:4,484,1) -> Float(8549376,33396,242,1) ***************
[10/31/2023-17:57:37] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv (CudaDepthwiseConvolution)
[10/31/2023-17:57:37] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:37] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv (CaskConvolution)
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 0xb29abdd00304c881
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0xb29abdd00304c881 Time: 2.52639
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: 0xc25454de2efcccdc
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0xc25454de2efcccdc Time: 2.86497
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0xfe80445f7bb61a99 Time: 2.88506
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 0x14ef32afe5695907
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0x14ef32afe5695907 Time: 2.57834
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0xff6944b17d5b2e32 Time: 2.34489
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 0xec391424db39a74f
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0xec391424db39a74f Time: 3.13235
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0x69c4e2ca38eadce2 Time: 2.48555
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 0x4d9d0d03129bceb1
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0x4d9d0d03129bceb1 Time: 2.94671
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 0xe01ccc14da28fa6f
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0xe01ccc14da28fa6f Time: 2.95277
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0xa4ae2d82115c3e83 Time: 2.7214
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: 0x1db6e8cf1382fbe0
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0x1db6e8cf1382fbe0 Time: 2.55524
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 0x2cf4271504d30e29
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0x2cf4271504d30e29 Time: 2.61232
[10/31/2023-17:57:37] [V] [TRT] Fastest Tactic: 0xff6944b17d5b2e32 Time: 2.34489
[10/31/2023-17:57:37] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv (CaskFlattenConvolution)
[10/31/2023-17:57:37] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[10/31/2023-17:57:37] [V] [TRT] *************** Autotuning format combination: Int8(534336,133584:32,484,1) -> Float(8549376,33396,242,1) ***************
[10/31/2023-17:57:37] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv (CaskConvolution)
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0xf04572b287451f42 Time: 2.46373
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0x733ba2a91a48d431 Time: 2.02048
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0x5e4f6d7c83746fd6 Time: 2.58944
[10/31/2023-17:57:37] [V] [TRT] Fastest Tactic: 0x733ba2a91a48d431 Time: 2.02048
[10/31/2023-17:57:37] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv (CaskFlattenConvolution)
[10/31/2023-17:57:37] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:37] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431
[10/31/2023-17:57:37] [V] [TRT] *************** Autotuning format combination: Int8(534336,133584:32,484,1) -> Float(267168,33396:32,242,1) ***************
[10/31/2023-17:57:37] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv (CaskConvolution)
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0x33a5c6dd086942c1 Time: 1.90736
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0xe742f4598442d2f1 Time: 1.9234
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0xdf7e1bd6a496d667 Time: 1.59668
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0x3f2b14dec582741e Time: 2.14097
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0xcf64a2ae51bf6b36 Time: 1.88536
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0xb86b920539eb9c79 Time: 1.63868
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0xa71946688cad8664 Time: 1.74639
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0xdc1f355deb032b87 Time: 2.11445
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0x6986b0c6a136276e Time: 2.39445
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef
[10/31/2023-17:57:37] [V] [TRT] Tactic: 0x271b998fe31732ef Time: 1.77865
[10/31/2023-17:57:37] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a
[10/31/2023-17:57:38] [V] [TRT] Tactic: 0x2468d082d0ff7c9a Time: 2.02736
[10/31/2023-17:57:38] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd
[10/31/2023-17:57:38] [V] [TRT] Tactic: 0xcd229658c16b33cd Time: 2.06821
[10/31/2023-17:57:38] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61
[10/31/2023-17:57:38] [V] [TRT] Tactic: 0x8141573686849b61 Time: 1.81106
[10/31/2023-17:57:38] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c
[10/31/2023-17:57:38] [V] [TRT] Tactic: 0x60b880e28fee7a0c Time: 2.01099
[10/31/2023-17:57:38] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377
[10/31/2023-17:57:38] [V] [TRT] Tactic: 0xb5f710b331ba8377 Time: 2.11792
[10/31/2023-17:57:38] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4
[10/31/2023-17:57:38] [V] [TRT] Tactic: 0x748f926574b7bdc4 Time: 2.04358
[10/31/2023-17:57:38] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e
[10/31/2023-17:57:38] [V] [TRT] Tactic: 0x5f1a472d416ff35e Time: 1.94217
[10/31/2023-17:57:38] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93
[10/31/2023-17:57:38] [V] [TRT] Tactic: 0x5bd8221bd57baf93 Time: 2.14012
[10/31/2023-17:57:38] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33
[10/31/2023-17:57:38] [V] [TRT] Tactic: 0x991db9fd58152c33 Time: 1.92192
[10/31/2023-17:57:38] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552
[10/31/2023-17:57:38] [V] [TRT] Tactic: 0x84942841d92b0552 Time: 1.73829
[10/31/2023-17:57:38] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda
[10/31/2023-17:57:38] [V] [TRT] Tactic: 0x445983715412fbda Time: 2.47853
[10/31/2023-17:57:38] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19
[10/31/2023-17:57:38] [V] [TRT] Tactic: 0x844ea9c00f711f19 Time: 1.69682
[10/31/2023-17:57:38] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec
[10/31/2023-17:57:38] [V] [TRT] Tactic: 0xa2f15b0a75b7dcec Time: 2.48813
[10/31/2023-17:57:38] [V] [TRT] Fastest Tactic: 0xdf7e1bd6a496d667 Time: 1.59668
[10/31/2023-17:57:38] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv (CaskFlattenConvolution)
[10/31/2023-17:57:38] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:38] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xdf7e1bd6a496d667
[10/31/2023-17:57:38] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:57:38] [V] [TRT] *************** Autotuning format combination: Int8(17098752,133584,484,1) -> Int8(8549376,33396,242,1) ***************
[10/31/2023-17:57:38] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv (CaskConvolution)
[10/31/2023-17:57:38] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:38] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:57:38] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:38] [V] [TRT] *************** Autotuning format combination: Int8(4274688,133584:4,484,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:57:38] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv (CudaDepthwiseConvolution)
[10/31/2023-17:57:38] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:38] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv (FusedConvActConvolution)
[10/31/2023-17:57:38] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:38] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv (CaskConvolution)
[10/31/2023-17:57:38] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0x8846c5916f34f7e9
[10/31/2023-17:57:38] [V] [TRT] Tactic: 0x8846c5916f34f7e9 Time: 17.7324
[10/31/2023-17:57:38] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 0xf6833cac33ebf690
[10/31/2023-17:57:38] [V] [TRT] Tactic: 0xf6833cac33ebf690 Time: 16.4714
[10/31/2023-17:57:38] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 0xa49b04dbd5d9448a
[10/31/2023-17:57:38] [V] [TRT] Tactic: 0xa49b04dbd5d9448a Time: 16.4575
[10/31/2023-17:57:38] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: 0xe0658ca00f36ea3e
[10/31/2023-17:57:38] [V] [TRT] Tactic: 0xe0658ca00f36ea3e Time: 18.1392
[10/31/2023-17:57:38] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 0x0270c2170caa55f8
[10/31/2023-17:57:39] [V] [TRT] Tactic: 0x0270c2170caa55f8 Time: 17.8027
[10/31/2023-17:57:39] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 0xcc3c5501ab7b5867
[10/31/2023-17:57:39] [V] [TRT] Tactic: 0xcc3c5501ab7b5867 Time: 16.5531
[10/31/2023-17:57:39] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 0x50f6b3d3d5866716
[10/31/2023-17:57:39] [V] [TRT] Tactic: 0x50f6b3d3d5866716 Time: 16.9036
[10/31/2023-17:57:39] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 0x1e2179aecb150578
[10/31/2023-17:57:39] [V] [TRT] Tactic: 0x1e2179aecb150578 Time: 17.0814
[10/31/2023-17:57:39] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x1f0263042c683192
[10/31/2023-17:57:39] [V] [TRT] Tactic: 0x1f0263042c683192 Time: 16.8352
[10/31/2023-17:57:39] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 0x9aa46c15c2a1d8a7
[10/31/2023-17:57:39] [V] [TRT] Tactic: 0x9aa46c15c2a1d8a7 Time: 16.6745
[10/31/2023-17:57:39] [V] [TRT] Fastest Tactic: 0xa49b04dbd5d9448a Time: 16.4575
[10/31/2023-17:57:39] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:57:39] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa49b04dbd5d9448a
[10/31/2023-17:57:39] [V] [TRT] *************** Autotuning format combination: Int8(4274688,133584:4,484,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:57:39] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv (CaskConvolution)
[10/31/2023-17:57:39] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4
[10/31/2023-17:57:39] [V] [TRT] Tactic: 0xeb494e2e67f079d4 Time: 17.7944
[10/31/2023-17:57:39] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18
[10/31/2023-17:57:40] [V] [TRT] Tactic: 0x0f47434ace2a7d18 Time: 17.7171
[10/31/2023-17:57:40] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 0x0da8f7df8cfd2e37
[10/31/2023-17:57:40] [V] [TRT] Tactic: 0x0da8f7df8cfd2e37 Time: 16.5282
[10/31/2023-17:57:40] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: 0x2cf0cc5b8a069ae7
[10/31/2023-17:57:40] [V] [TRT] Tactic: 0x2cf0cc5b8a069ae7 Time: 18.1689
[10/31/2023-17:57:40] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: 0x320c30eda409da30
[10/31/2023-17:57:40] [V] [TRT] Tactic: 0x320c30eda409da30 Time: 16.8771
[10/31/2023-17:57:40] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: 0xf3f27c40da3380fe
[10/31/2023-17:57:40] [V] [TRT] Tactic: 0xf3f27c40da3380fe Time: 16.8212
[10/31/2023-17:57:40] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: 0xc4f09503ebafdf51
[10/31/2023-17:57:40] [V] [TRT] Tactic: 0xc4f09503ebafdf51 Time: 16.5155
[10/31/2023-17:57:40] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 0xd4d010e83172b290
[10/31/2023-17:57:40] [V] [TRT] Tactic: 0xd4d010e83172b290 Time: 17.0566
[10/31/2023-17:57:40] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: 0x3c0834dbcd849973
[10/31/2023-17:57:41] [V] [TRT] Tactic: 0x3c0834dbcd849973 Time: 16.4561
[10/31/2023-17:57:41] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: 0xc911b61f7a5c0315
[10/31/2023-17:57:41] [V] [TRT] Tactic: 0xc911b61f7a5c0315 Time: 16.6765
[10/31/2023-17:57:41] [V] [TRT] Fastest Tactic: 0x3c0834dbcd849973 Time: 16.4561
[10/31/2023-17:57:41] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:57:41] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:41] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3c0834dbcd849973
[10/31/2023-17:57:41] [V] [TRT] *************** Autotuning format combination: Int8(534336,133584:32,484,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:57:41] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv (CudaGroupConvolution)
[10/31/2023-17:57:41] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:41] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv (CudaDepthwiseConvolution)
[10/31/2023-17:57:41] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:41] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv (FusedConvActConvolution)
[10/31/2023-17:57:41] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:41] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv (CaskConvolution)
[10/31/2023-17:57:41] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18
[10/31/2023-17:57:41] [V] [TRT] Tactic: 0x4cf18ad3e295ea18 Time: 2.87029
[10/31/2023-17:57:41] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5
[10/31/2023-17:57:41] [V] [TRT] Tactic: 0x89c1d75fe77808d5 Time: 4.13643
[10/31/2023-17:57:41] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363
[10/31/2023-17:57:41] [V] [TRT] Tactic: 0xf56c0ac895d82363 Time: 4.57047
[10/31/2023-17:57:41] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673
[10/31/2023-17:57:41] [V] [TRT] Tactic: 0xf33711e7c9ed4673 Time: 2.74048
[10/31/2023-17:57:41] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780
[10/31/2023-17:57:41] [V] [TRT] Tactic: 0xeb33cf0799237780 Time: 5.62633
[10/31/2023-17:57:41] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50
[10/31/2023-17:57:41] [V] [TRT] Tactic: 0x3818ca0093333b50 Time: 3.36769
[10/31/2023-17:57:41] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7
[10/31/2023-17:57:41] [V] [TRT] Tactic: 0xee2fce9480a52be7 Time: 7.79903
[10/31/2023-17:57:41] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x048dc59a1807b5bb
[10/31/2023-17:57:41] [V] [TRT] Tactic: 0x048dc59a1807b5bb Time: 7.87045
[10/31/2023-17:57:41] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af
[10/31/2023-17:57:41] [V] [TRT] Tactic: 0x26a29d5b8b3f62af Time: 3.13362
[10/31/2023-17:57:41] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee
[10/31/2023-17:57:41] [V] [TRT] Tactic: 0xd277f13d771603ee Time: 3.29805
[10/31/2023-17:57:41] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52
[10/31/2023-17:57:41] [V] [TRT] Tactic: 0x11b75d98d540ee52 Time: 4.39963
[10/31/2023-17:57:41] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071
[10/31/2023-17:57:41] [V] [TRT] Tactic: 0x2f34f689bfca5071 Time: 6.11558
[10/31/2023-17:57:41] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4
[10/31/2023-17:57:41] [V] [TRT] Tactic: 0x9f6920f4a40549f4 Time: 4.20785
[10/31/2023-17:57:41] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e
[10/31/2023-17:57:41] [V] [TRT] Tactic: 0x2ff8bc85c8ced89e Time: 3.36652
[10/31/2023-17:57:41] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0
[10/31/2023-17:57:41] [V] [TRT] Tactic: 0x5e1b696f91f572c0 Time: 3.86826
[10/31/2023-17:57:41] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0x623894f465c90f26 Time: 3.00079
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0x955d593b1135a423 Time: 6.37427
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0x2728bbf79375f71d Time: 2.81384
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0x87620679fb5f37df Time: 2.78848
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0x705baf38e41eee0b Time: 3.77988
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0xd30e9f770878c3fd Time: 3.33707
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0x41b4640eb250ffd0 Time: 2.7767
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0xd3d41ef6de22d9b6 Time: 3.16069
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4805f65acfc5cf25
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0x4805f65acfc5cf25 Time: 7.38352
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0x8d50646eff0cde6d Time: 2.68176
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0xf8d4389f60adfa3c Time: 2.79019
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0x80cce6d8f75dc163 Time: 2.87467
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0xea88b51105501f96 Time: 2.61901
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0x53554c607d072468 Time: 5.0674
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0x70ccdad7e8ced9ab Time: 3.50999
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0x99901a83d7176ba9 Time: 3.52839
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0x8e1e9d670448aca7 Time: 5.99887
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0x204424cb4da28c02 Time: 3.18953
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0x0fbfd5fa9864ab49 Time: 4.10489
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0x9d0f90e0cec890bb Time: 4.04526
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0xa92dc4358ef627d4 Time: 4.48915
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0x7dcb6d55062fd530 Time: 3.02917
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0x8b2f6e180c235792 Time: 4.33109
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0xd3f592fae61c7986 Time: 5.87735
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0x60edc1e1753cd5b8 Time: 4.40381
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393
[10/31/2023-17:57:42] [V] [TRT] Tactic: 0x0f6ba1e5b0320393 Time: 4.48613
[10/31/2023-17:57:42] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0x4b860619b6031662 Time: 2.94315
[10/31/2023-17:57:43] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0x19b79d0c4ff629da Time: 5.10625
[10/31/2023-17:57:43] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0x136c072084f41e49 Time: 5.13034
[10/31/2023-17:57:43] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0x4749124f62d8bd23 Time: 7.59173
[10/31/2023-17:57:43] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0x444f830f80ff098b Time: 4.31348
[10/31/2023-17:57:43] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0x671c943720ba8655 Time: 3.35839
[10/31/2023-17:57:43] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0xeb9516439d87ac23 Time: 4.13573
[10/31/2023-17:57:43] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0xef01fb6e433afa50 Time: 3.0902
[10/31/2023-17:57:43] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0xbb88763c3b0e94d4 Time: 4.41291
[10/31/2023-17:57:43] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0x4b476758492c15b0 Time: 5.05519
[10/31/2023-17:57:43] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0x554e2e252e28b3fd Time: 2.6961
[10/31/2023-17:57:43] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0x9262f8f95beb428d Time: 5.02899
[10/31/2023-17:57:43] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0xd9f3bbc3e16b16ac Time: 2.92428
[10/31/2023-17:57:43] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0x4f6730716f78f4a3 Time: 2.76395
[10/31/2023-17:57:43] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0x028e842ce51fbd9d Time: 4.26936
[10/31/2023-17:57:43] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0xe37d64a29ca3101c Time: 3.41132
[10/31/2023-17:57:43] [V] [TRT] Fastest Tactic: 0xea88b51105501f96 Time: 2.61901
[10/31/2023-17:57:43] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:57:43] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:43] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xea88b51105501f96
[10/31/2023-17:57:43] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:57:43] [V] [TRT] *************** Autotuning format combination: Float(8549376,33396,242,1), Float(256,1,1,1), Float(256,1,1,1) -> Float(8549376,33396,242,1) ***************
[10/31/2023-17:57:43] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 186) [ElementWise], PWN(PWN((Unnamed Layer* 183) [Constant] + (Unnamed Layer* 184) [ElementWise], (Unnamed Layer* 185) [Unary]), (Unnamed Layer* 187) [ElementWise])) (PointWiseV2)
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.06836
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0x0000000000000001 Time: 4.57148
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0x0000000000000002 Time: 4.32352
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0x0000000000000003 Time: 3.55469
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0x0000000000000004 Time: 3.42231
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0x0000000000000005 Time: 3.57779
[10/31/2023-17:57:43] [V] [TRT] Tactic: 0x0000000000000006 Time: 3.13545
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x0000000000000007 Time: 2.76112
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x0000000000000008 Time: 3.19268
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x0000000000000009 Time: 3.26792
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x000000000000001c Time: 5.53737
[10/31/2023-17:57:44] [V] [TRT] Fastest Tactic: 0x0000000000000007 Time: 2.76112
[10/31/2023-17:57:44] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 186) [ElementWise], PWN(PWN((Unnamed Layer* 183) [Constant] + (Unnamed Layer* 184) [ElementWise], (Unnamed Layer* 185) [Unary]), (Unnamed Layer* 187) [ElementWise])) (PointWise)
[10/31/2023-17:57:44] [V] [TRT] PointWise has no valid tactics for this config, skipping
[10/31/2023-17:57:44] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000007
[10/31/2023-17:57:44] [V] [TRT] *************** Autotuning format combination: Float(8549376,1,61952,256), Float(256,1,256,256), Float(256,1,256,256) -> Float(8549376,1,61952,256) ***************
[10/31/2023-17:57:44] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 186) [ElementWise], PWN(PWN((Unnamed Layer* 183) [Constant] + (Unnamed Layer* 184) [ElementWise], (Unnamed Layer* 185) [Unary]), (Unnamed Layer* 187) [ElementWise])) (PointWiseV2)
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 6.75161
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x0000000000000001 Time: 4.75213
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x0000000000000002 Time: 4.58116
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x0000000000000003 Time: 3.54497
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x0000000000000004 Time: 3.49218
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x0000000000000005 Time: 3.67766
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x0000000000000006 Time: 3.02852
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x0000000000000007 Time: 2.73146
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x0000000000000008 Time: 3.16665
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x0000000000000009 Time: 3.22219
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x000000000000001c Time: 6.5604
[10/31/2023-17:57:44] [V] [TRT] Fastest Tactic: 0x0000000000000007 Time: 2.73146
[10/31/2023-17:57:44] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 186) [ElementWise], PWN(PWN((Unnamed Layer* 183) [Constant] + (Unnamed Layer* 184) [ElementWise], (Unnamed Layer* 185) [Unary]), (Unnamed Layer* 187) [ElementWise])) (PointWise)
[10/31/2023-17:57:44] [V] [TRT] PointWise has no valid tactics for this config, skipping
[10/31/2023-17:57:44] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000007
[10/31/2023-17:57:44] [V] [TRT] *************** Autotuning format combination: Float(2137344,1:4,15488,64), Float(64,1:4,64,64), Float(64,1:4,64,64) -> Float(2137344,1:4,15488,64) ***************
[10/31/2023-17:57:44] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 186) [ElementWise], PWN(PWN((Unnamed Layer* 183) [Constant] + (Unnamed Layer* 184) [ElementWise], (Unnamed Layer* 185) [Unary]), (Unnamed Layer* 187) [ElementWise])) (PointWiseV2)
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.23872
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x0000000000000001 Time: 2.74455
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x0000000000000002 Time: 2.76493
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x0000000000000003 Time: 2.92889
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x0000000000000004 Time: 3.24507
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x0000000000000005 Time: 3.26937
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x0000000000000006 Time: 3.29455
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x0000000000000007 Time: 3.644
[10/31/2023-17:57:44] [V] [TRT] Tactic: 0x0000000000000008 Time: 5.07036
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x0000000000000009 Time: 5.78024
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x000000000000000a Time: 4.67823
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x000000000000000b Time: 3.47113
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x000000000000000c Time: 3.51112
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x000000000000000d Time: 2.70714
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x000000000000000e Time: 3.21189
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x000000000000000f Time: 3.19392
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x0000000000000010 Time: 3.00685
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x0000000000000011 Time: 3.51742
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x0000000000000012 Time: 4.07275
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x0000000000000013 Time: 4.14698
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x0000000000000014 Time: 7.29585
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x0000000000000015 Time: 5.11053
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x0000000000000016 Time: 3.73278
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x0000000000000017 Time: 3.1371
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x000000000000001c Time: 3.13905
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x000000000000001d Time: 4.67616
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x000000000000001e Time: 7.18032
[10/31/2023-17:57:45] [V] [TRT] Fastest Tactic: 0x000000000000000d Time: 2.70714
[10/31/2023-17:57:45] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 186) [ElementWise], PWN(PWN((Unnamed Layer* 183) [Constant] + (Unnamed Layer* 184) [ElementWise], (Unnamed Layer* 185) [Unary]), (Unnamed Layer* 187) [ElementWise])) (PointWise)
[10/31/2023-17:57:45] [V] [TRT] PointWise has no valid tactics for this config, skipping
[10/31/2023-17:57:45] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000000d
[10/31/2023-17:57:45] [V] [TRT] *************** Autotuning format combination: Float(267168,33396:32,242,1), Float(8,1:32,1,1), Float(8,1:32,1,1) -> Float(267168,33396:32,242,1) ***************
[10/31/2023-17:57:45] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 186) [ElementWise], PWN(PWN((Unnamed Layer* 183) [Constant] + (Unnamed Layer* 184) [ElementWise], (Unnamed Layer* 185) [Unary]), (Unnamed Layer* 187) [ElementWise])) (PointWiseV2)
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x0000000000000018 Time: 3.11595
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x0000000000000019 Time: 2.77163
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x000000000000001a Time: 2.50335
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x000000000000001b Time: 2.38902
[10/31/2023-17:57:45] [V] [TRT] Tactic: 0x000000000000001f Time: 3.02261
[10/31/2023-17:57:45] [V] [TRT] Fastest Tactic: 0x000000000000001b Time: 2.38902
[10/31/2023-17:57:45] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 186) [ElementWise], PWN(PWN((Unnamed Layer* 183) [Constant] + (Unnamed Layer* 184) [ElementWise], (Unnamed Layer* 185) [Unary]), (Unnamed Layer* 187) [ElementWise])) (PointWise)
[10/31/2023-17:57:45] [V] [TRT] PointWise has no valid tactics for this config, skipping
[10/31/2023-17:57:45] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001b
[10/31/2023-17:57:45] [V] [TRT] *************** Autotuning format combination: Float(1:4,33396,242,1), Float(1:4,1,1,1), Float(1:4,1,1,1) -> Float(1:4,33396,242,1) ***************
[10/31/2023-17:57:45] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 186) [ElementWise], PWN(PWN((Unnamed Layer* 183) [Constant] + (Unnamed Layer* 184) [ElementWise], (Unnamed Layer* 185) [Unary]), (Unnamed Layer* 187) [ElementWise])) (PointWiseV2)
[10/31/2023-17:57:46] [V] [TRT] Tactic: 0x000000000000001c Time: 13.3112
[10/31/2023-17:57:46] [V] [TRT] Tactic: 0x000000000000001d Time: 19.2201
[10/31/2023-17:57:46] [V] [TRT] Tactic: 0x000000000000001e Time: 30.1373
[10/31/2023-17:57:46] [V] [TRT] Fastest Tactic: 0x000000000000001c Time: 13.3112
[10/31/2023-17:57:46] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001c
[10/31/2023-17:57:46] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:57:46] [V] [TRT] *************** Autotuning format combination: Float(8549376,33396,242,1) -> Float(8549376,33396,242,1) ***************
[10/31/2023-17:57:46] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.downsample.1.weight + (Unnamed Layer* 179) [Shuffle] + (Unnamed Layer* 188) [ElementWise] + backbone.layer3.0.bn2.bias + Identity_1 + (Unnamed Layer* 180) [Shuffle] + /backbone/layer3/layer3.0/downsample/downsample.1/BatchNormalization (Scale)
[10/31/2023-17:57:46] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.67567
[10/31/2023-17:57:46] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 1.67567
[10/31/2023-17:57:46] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0x0000000000000000
[10/31/2023-17:57:46] [V] [TRT] *************** Autotuning format combination: Float(8549376,1,61952,256) -> Float(8549376,1,61952,256) ***************
[10/31/2023-17:57:46] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.downsample.1.weight + (Unnamed Layer* 179) [Shuffle] + (Unnamed Layer* 188) [ElementWise] + backbone.layer3.0.bn2.bias + Identity_1 + (Unnamed Layer* 180) [Shuffle] + /backbone/layer3/layer3.0/downsample/downsample.1/BatchNormalization (Scale)
[10/31/2023-17:57:46] [V] [TRT] Scale has no valid tactics for this config, skipping
[10/31/2023-17:57:46] [V] [TRT] *************** Autotuning format combination: Float(2137344,1:4,15488,64) -> Float(2137344,1:4,15488,64) ***************
[10/31/2023-17:57:46] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.downsample.1.weight + (Unnamed Layer* 179) [Shuffle] + (Unnamed Layer* 188) [ElementWise] + backbone.layer3.0.bn2.bias + Identity_1 + (Unnamed Layer* 180) [Shuffle] + /backbone/layer3/layer3.0/downsample/downsample.1/BatchNormalization (Scale)
[10/31/2023-17:57:46] [V] [TRT] Scale has no valid tactics for this config, skipping
[10/31/2023-17:57:46] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:57:46] [V] [TRT] *************** Autotuning format combination: Int8(2137344,33396:4,242,1), Float(8549376,33396,242,1) -> Float(8549376,33396,242,1) ***************
[10/31/2023-17:57:46] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu (CaskConvolution)
[10/31/2023-17:57:46] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 0xb29abdd00304c881
[10/31/2023-17:57:46] [V] [TRT] Tactic: 0xb29abdd00304c881 Time: 33.7426
[10/31/2023-17:57:46] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: 0x127de12a1f1a4809
[10/31/2023-17:57:47] [V] [TRT] Tactic: 0x127de12a1f1a4809 Time: 36.3781
[10/31/2023-17:57:47] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99
[10/31/2023-17:57:47] [V] [TRT] Tactic: 0xfe80445f7bb61a99 Time: 35.2275
[10/31/2023-17:57:47] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 0x14ef32afe5695907
[10/31/2023-17:57:47] [V] [TRT] Tactic: 0x14ef32afe5695907 Time: 32.8467
[10/31/2023-17:57:47] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 0xec391424db39a74f
[10/31/2023-17:57:48] [V] [TRT] Tactic: 0xec391424db39a74f Time: 34.0764
[10/31/2023-17:57:48] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2
[10/31/2023-17:57:48] [V] [TRT] Tactic: 0x69c4e2ca38eadce2 Time: 33.6712
[10/31/2023-17:57:48] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 0x4d9d0d03129bceb1
[10/31/2023-17:57:48] [V] [TRT] Tactic: 0x4d9d0d03129bceb1 Time: 33.2911
[10/31/2023-17:57:48] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 0xe01ccc14da28fa6f
[10/31/2023-17:57:49] [V] [TRT] Tactic: 0xe01ccc14da28fa6f Time: 35.4316
[10/31/2023-17:57:49] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 0x1b0534177b414e71
[10/31/2023-17:57:49] [V] [TRT] Tactic: 0x1b0534177b414e71 Time: 33.0885
[10/31/2023-17:57:49] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 0x2cf4271504d30e29
[10/31/2023-17:57:49] [V] [TRT] Tactic: 0x2cf4271504d30e29 Time: 32.8864
[10/31/2023-17:57:49] [V] [TRT] Fastest Tactic: 0x14ef32afe5695907 Time: 32.8467
[10/31/2023-17:57:49] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu (CaskFlattenConvolution)
[10/31/2023-17:57:49] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:49] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x14ef32afe5695907
[10/31/2023-17:57:49] [V] [TRT] *************** Autotuning format combination: Int8(267168,33396:32,242,1), Float(8549376,33396,242,1) -> Float(8549376,33396,242,1) ***************
[10/31/2023-17:57:49] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu (CaskConvolution)
[10/31/2023-17:57:49] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84
[10/31/2023-17:57:49] [V] [TRT] Tactic: 0x85c1a5f7f239cf84 Time: 10.3632
[10/31/2023-17:57:49] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10
[10/31/2023-17:57:49] [V] [TRT] Tactic: 0x2bcbba39f608bf10 Time: 8.48921
[10/31/2023-17:57:49] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808
[10/31/2023-17:57:49] [V] [TRT] Tactic: 0xac914b235d066808 Time: 7.92699
[10/31/2023-17:57:49] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e
[10/31/2023-17:57:49] [V] [TRT] Tactic: 0x23b890da05937b9e Time: 6.45839
[10/31/2023-17:57:49] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463
[10/31/2023-17:57:50] [V] [TRT] Tactic: 0xa8b56a226b057463 Time: 9.00041
[10/31/2023-17:57:50] [V] [TRT] Fastest Tactic: 0x23b890da05937b9e Time: 6.45839
[10/31/2023-17:57:50] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu (CaskFlattenConvolution)
[10/31/2023-17:57:50] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:50] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x23b890da05937b9e
[10/31/2023-17:57:50] [V] [TRT] *************** Autotuning format combination: Int8(267168,33396:32,242,1), Float(267168,33396:32,242,1) -> Float(267168,33396:32,242,1) ***************
[10/31/2023-17:57:50] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu (CaskConvolution)
[10/31/2023-17:57:50] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c
[10/31/2023-17:57:50] [V] [TRT] Tactic: 0xb936321f82fd390c Time: 9.48789
[10/31/2023-17:57:50] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa
[10/31/2023-17:57:50] [V] [TRT] Tactic: 0x55edef142e02adaa Time: 5.136
[10/31/2023-17:57:50] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1
[10/31/2023-17:57:50] [V] [TRT] Tactic: 0xe742f4598442d2f1 Time: 6.47892
[10/31/2023-17:57:50] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e
[10/31/2023-17:57:50] [V] [TRT] Tactic: 0x3f2b14dec582741e Time: 9.61103
[10/31/2023-17:57:50] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4
[10/31/2023-17:57:50] [V] [TRT] Tactic: 0x0edd5d0285e564d4 Time: 6.20297
[10/31/2023-17:57:50] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79
[10/31/2023-17:57:50] [V] [TRT] Tactic: 0xb86b920539eb9c79 Time: 5.20027
[10/31/2023-17:57:50] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87
[10/31/2023-17:57:50] [V] [TRT] Tactic: 0xdc1f355deb032b87 Time: 11.7083
[10/31/2023-17:57:50] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb
[10/31/2023-17:57:50] [V] [TRT] Tactic: 0x45f7566cdb2b10fb Time: 10.5591
[10/31/2023-17:57:50] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1
[10/31/2023-17:57:50] [V] [TRT] Tactic: 0xea2b7420057a01c1 Time: 5.80045
[10/31/2023-17:57:50] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e
[10/31/2023-17:57:50] [V] [TRT] Tactic: 0x6986b0c6a136276e Time: 7.7034
[10/31/2023-17:57:50] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922
[10/31/2023-17:57:50] [V] [TRT] Tactic: 0xad886d4d69834922 Time: 6.38691
[10/31/2023-17:57:50] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a
[10/31/2023-17:57:51] [V] [TRT] Tactic: 0x2468d082d0ff7c9a Time: 10.7848
[10/31/2023-17:57:51] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61
[10/31/2023-17:57:51] [V] [TRT] Tactic: 0x8141573686849b61 Time: 6.26878
[10/31/2023-17:57:51] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377
[10/31/2023-17:57:51] [V] [TRT] Tactic: 0xb5f710b331ba8377 Time: 5.92178
[10/31/2023-17:57:51] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4
[10/31/2023-17:57:51] [V] [TRT] Tactic: 0x748f926574b7bdc4 Time: 5.84248
[10/31/2023-17:57:51] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f
[10/31/2023-17:57:51] [V] [TRT] Tactic: 0x0e07dc8353bf7e9f Time: 5.55908
[10/31/2023-17:57:51] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800
[10/31/2023-17:57:51] [V] [TRT] Tactic: 0x47b1629a4bff4800 Time: 5.86633
[10/31/2023-17:57:51] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e
[10/31/2023-17:57:51] [V] [TRT] Tactic: 0x5f1a472d416ff35e Time: 7.99932
[10/31/2023-17:57:51] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33
[10/31/2023-17:57:51] [V] [TRT] Tactic: 0x991db9fd58152c33 Time: 5.58157
[10/31/2023-17:57:51] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e
[10/31/2023-17:57:51] [V] [TRT] Tactic: 0xd14bd6d95fefd45e Time: 12.5719
[10/31/2023-17:57:51] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9
[10/31/2023-17:57:51] [V] [TRT] Tactic: 0x2d8ab2aa0639fda9 Time: 7.83722
[10/31/2023-17:57:51] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda
[10/31/2023-17:57:51] [V] [TRT] Tactic: 0x445983715412fbda Time: 12.8244
[10/31/2023-17:57:51] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec
[10/31/2023-17:57:51] [V] [TRT] Tactic: 0xa2f15b0a75b7dcec Time: 10.1641
[10/31/2023-17:57:51] [V] [TRT] Fastest Tactic: 0x55edef142e02adaa Time: 5.136
[10/31/2023-17:57:51] [V] [TRT] --------------- Timing Runner: backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu (CaskFlattenConvolution)
[10/31/2023-17:57:51] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:51] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x55edef142e02adaa
[10/31/2023-17:57:51] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:57:51] [V] [TRT] *************** Autotuning format combination: Int8(8549376,33396,242,1) -> Int8(8549376,33396,242,1) ***************
[10/31/2023-17:57:51] [V] [TRT] --------------- Timing Runner: backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv (CaskConvolution)
[10/31/2023-17:57:51] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:51] [V] [TRT] --------------- Timing Runner: backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:57:51] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:51] [V] [TRT] *************** Autotuning format combination: Int8(2137344,33396:4,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:57:51] [V] [TRT] --------------- Timing Runner: backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv (CudaDepthwiseConvolution)
[10/31/2023-17:57:51] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:51] [V] [TRT] --------------- Timing Runner: backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv (FusedConvActConvolution)
[10/31/2023-17:57:51] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:51] [V] [TRT] --------------- Timing Runner: backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv (CaskConvolution)
[10/31/2023-17:57:51] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0x8846c5916f34f7e9
[10/31/2023-17:57:52] [V] [TRT] Tactic: 0x8846c5916f34f7e9 Time: 34.9052
[10/31/2023-17:57:52] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 0xf6833cac33ebf690
[10/31/2023-17:57:52] [V] [TRT] Tactic: 0xf6833cac33ebf690 Time: 32.5698
[10/31/2023-17:57:52] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 0xa49b04dbd5d9448a
[10/31/2023-17:57:52] [V] [TRT] Tactic: 0xa49b04dbd5d9448a Time: 32.5953
[10/31/2023-17:57:52] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: 0xe0658ca00f36ea3e
[10/31/2023-17:57:53] [V] [TRT] Tactic: 0xe0658ca00f36ea3e Time: 34.7307
[10/31/2023-17:57:53] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 0x0270c2170caa55f8
[10/31/2023-17:57:53] [V] [TRT] Tactic: 0x0270c2170caa55f8 Time: 35.0443
[10/31/2023-17:57:53] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 0xcc3c5501ab7b5867
[10/31/2023-17:57:53] [V] [TRT] Tactic: 0xcc3c5501ab7b5867 Time: 32.7896
[10/31/2023-17:57:53] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 0x50f6b3d3d5866716
[10/31/2023-17:57:53] [V] [TRT] Tactic: 0x50f6b3d3d5866716 Time: 33.4436
[10/31/2023-17:57:53] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 0x1e2179aecb150578
[10/31/2023-17:57:54] [V] [TRT] Tactic: 0x1e2179aecb150578 Time: 33.6523
[10/31/2023-17:57:54] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x1f0263042c683192
[10/31/2023-17:57:54] [V] [TRT] Tactic: 0x1f0263042c683192 Time: 33.3836
[10/31/2023-17:57:54] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 0x9aa46c15c2a1d8a7
[10/31/2023-17:57:54] [V] [TRT] Tactic: 0x9aa46c15c2a1d8a7 Time: 32.9972
[10/31/2023-17:57:54] [V] [TRT] Fastest Tactic: 0xf6833cac33ebf690 Time: 32.5698
[10/31/2023-17:57:54] [V] [TRT] --------------- Timing Runner: backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:57:54] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xf6833cac33ebf690
[10/31/2023-17:57:54] [V] [TRT] *************** Autotuning format combination: Int8(2137344,33396:4,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:57:54] [V] [TRT] --------------- Timing Runner: backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv (CaskConvolution)
[10/31/2023-17:57:54] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4
[10/31/2023-17:57:55] [V] [TRT] Tactic: 0xeb494e2e67f079d4 Time: 34.9901
[10/31/2023-17:57:55] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18
[10/31/2023-17:57:55] [V] [TRT] Tactic: 0x0f47434ace2a7d18 Time: 34.8853
[10/31/2023-17:57:55] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 0x0da8f7df8cfd2e37
[10/31/2023-17:57:55] [V] [TRT] Tactic: 0x0da8f7df8cfd2e37 Time: 32.7489
[10/31/2023-17:57:55] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: 0x2cf0cc5b8a069ae7
[10/31/2023-17:57:55] [V] [TRT] Tactic: 0x2cf0cc5b8a069ae7 Time: 34.803
[10/31/2023-17:57:55] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: 0x320c30eda409da30
[10/31/2023-17:57:56] [V] [TRT] Tactic: 0x320c30eda409da30 Time: 33.4606
[10/31/2023-17:57:56] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: 0xf3f27c40da3380fe
[10/31/2023-17:57:56] [V] [TRT] Tactic: 0xf3f27c40da3380fe Time: 33.4002
[10/31/2023-17:57:56] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: 0xc4f09503ebafdf51
[10/31/2023-17:57:56] [V] [TRT] Tactic: 0xc4f09503ebafdf51 Time: 32.6238
[10/31/2023-17:57:56] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 0xd4d010e83172b290
[10/31/2023-17:57:56] [V] [TRT] Tactic: 0xd4d010e83172b290 Time: 33.6759
[10/31/2023-17:57:56] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: 0x3c0834dbcd849973
[10/31/2023-17:57:57] [V] [TRT] Tactic: 0x3c0834dbcd849973 Time: 32.6368
[10/31/2023-17:57:57] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: 0xc911b61f7a5c0315
[10/31/2023-17:57:57] [V] [TRT] Tactic: 0xc911b61f7a5c0315 Time: 32.9955
[10/31/2023-17:57:57] [V] [TRT] Fastest Tactic: 0xc4f09503ebafdf51 Time: 32.6238
[10/31/2023-17:57:57] [V] [TRT] --------------- Timing Runner: backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:57:57] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc4f09503ebafdf51
[10/31/2023-17:57:57] [V] [TRT] *************** Autotuning format combination: Int8(267168,33396:32,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:57:57] [V] [TRT] --------------- Timing Runner: backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv (CudaGroupConvolution)
[10/31/2023-17:57:57] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:57] [V] [TRT] --------------- Timing Runner: backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv (CudaDepthwiseConvolution)
[10/31/2023-17:57:57] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:57] [V] [TRT] --------------- Timing Runner: backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv (FusedConvActConvolution)
[10/31/2023-17:57:57] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:57:57] [V] [TRT] --------------- Timing Runner: backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv (CaskConvolution)
[10/31/2023-17:57:57] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18
[10/31/2023-17:57:57] [V] [TRT] Tactic: 0x4cf18ad3e295ea18 Time: 4.77509
[10/31/2023-17:57:57] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5
[10/31/2023-17:57:57] [V] [TRT] Tactic: 0x89c1d75fe77808d5 Time: 7.14053
[10/31/2023-17:57:57] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363
[10/31/2023-17:57:57] [V] [TRT] Tactic: 0xf56c0ac895d82363 Time: 7.33893
[10/31/2023-17:57:57] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673
[10/31/2023-17:57:57] [V] [TRT] Tactic: 0xf33711e7c9ed4673 Time: 4.7266
[10/31/2023-17:57:57] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780
[10/31/2023-17:57:57] [V] [TRT] Tactic: 0xeb33cf0799237780 Time: 7.53127
[10/31/2023-17:57:57] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50
[10/31/2023-17:57:57] [V] [TRT] Tactic: 0x3818ca0093333b50 Time: 5.44892
[10/31/2023-17:57:57] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7
[10/31/2023-17:57:58] [V] [TRT] Tactic: 0xee2fce9480a52be7 Time: 11.9498
[10/31/2023-17:57:58] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x048dc59a1807b5bb
[10/31/2023-17:57:58] [V] [TRT] Tactic: 0x048dc59a1807b5bb Time: 13.1933
[10/31/2023-17:57:58] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af
[10/31/2023-17:57:58] [V] [TRT] Tactic: 0x26a29d5b8b3f62af Time: 4.95883
[10/31/2023-17:57:58] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee
[10/31/2023-17:57:58] [V] [TRT] Tactic: 0xd277f13d771603ee Time: 5.45821
[10/31/2023-17:57:58] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52
[10/31/2023-17:57:58] [V] [TRT] Tactic: 0x11b75d98d540ee52 Time: 7.35642
[10/31/2023-17:57:58] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071
[10/31/2023-17:57:58] [V] [TRT] Tactic: 0x2f34f689bfca5071 Time: 8.78388
[10/31/2023-17:57:58] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4
[10/31/2023-17:57:58] [V] [TRT] Tactic: 0x9f6920f4a40549f4 Time: 7.24282
[10/31/2023-17:57:58] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e
[10/31/2023-17:57:58] [V] [TRT] Tactic: 0x2ff8bc85c8ced89e Time: 5.50162
[10/31/2023-17:57:58] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0
[10/31/2023-17:57:58] [V] [TRT] Tactic: 0x5e1b696f91f572c0 Time: 5.84687
[10/31/2023-17:57:58] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26
[10/31/2023-17:57:58] [V] [TRT] Tactic: 0x623894f465c90f26 Time: 4.83211
[10/31/2023-17:57:58] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423
[10/31/2023-17:57:58] [V] [TRT] Tactic: 0x955d593b1135a423 Time: 8.84581
[10/31/2023-17:57:58] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d
[10/31/2023-17:57:58] [V] [TRT] Tactic: 0x2728bbf79375f71d Time: 4.8203
[10/31/2023-17:57:58] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df
[10/31/2023-17:57:58] [V] [TRT] Tactic: 0x87620679fb5f37df Time: 4.71561
[10/31/2023-17:57:58] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b
[10/31/2023-17:57:58] [V] [TRT] Tactic: 0x705baf38e41eee0b Time: 5.72652
[10/31/2023-17:57:58] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd
[10/31/2023-17:57:58] [V] [TRT] Tactic: 0xd30e9f770878c3fd Time: 5.5326
[10/31/2023-17:57:58] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0
[10/31/2023-17:57:58] [V] [TRT] Tactic: 0x41b4640eb250ffd0 Time: 4.76025
[10/31/2023-17:57:58] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6
[10/31/2023-17:57:58] [V] [TRT] Tactic: 0xd3d41ef6de22d9b6 Time: 4.84262
[10/31/2023-17:57:58] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4805f65acfc5cf25
[10/31/2023-17:57:59] [V] [TRT] Tactic: 0x4805f65acfc5cf25 Time: 12.5906
[10/31/2023-17:57:59] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d
[10/31/2023-17:57:59] [V] [TRT] Tactic: 0x8d50646eff0cde6d Time: 4.63408
[10/31/2023-17:57:59] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c
[10/31/2023-17:57:59] [V] [TRT] Tactic: 0xf8d4389f60adfa3c Time: 4.75176
[10/31/2023-17:57:59] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163
[10/31/2023-17:57:59] [V] [TRT] Tactic: 0x80cce6d8f75dc163 Time: 4.85272
[10/31/2023-17:57:59] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96
[10/31/2023-17:57:59] [V] [TRT] Tactic: 0xea88b51105501f96 Time: 4.58048
[10/31/2023-17:57:59] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468
[10/31/2023-17:57:59] [V] [TRT] Tactic: 0x53554c607d072468 Time: 7.29525
[10/31/2023-17:57:59] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab
[10/31/2023-17:57:59] [V] [TRT] Tactic: 0x70ccdad7e8ced9ab Time: 5.74048
[10/31/2023-17:57:59] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9
[10/31/2023-17:57:59] [V] [TRT] Tactic: 0x99901a83d7176ba9 Time: 5.81716
[10/31/2023-17:57:59] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7
[10/31/2023-17:57:59] [V] [TRT] Tactic: 0x8e1e9d670448aca7 Time: 9.93125
[10/31/2023-17:57:59] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02
[10/31/2023-17:57:59] [V] [TRT] Tactic: 0x204424cb4da28c02 Time: 4.88096
[10/31/2023-17:57:59] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49
[10/31/2023-17:57:59] [V] [TRT] Tactic: 0x0fbfd5fa9864ab49 Time: 7.0232
[10/31/2023-17:57:59] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb
[10/31/2023-17:57:59] [V] [TRT] Tactic: 0x9d0f90e0cec890bb Time: 7.01009
[10/31/2023-17:57:59] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4
[10/31/2023-17:57:59] [V] [TRT] Tactic: 0xa92dc4358ef627d4 Time: 7.31817
[10/31/2023-17:57:59] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530
[10/31/2023-17:57:59] [V] [TRT] Tactic: 0x7dcb6d55062fd530 Time: 5.14856
[10/31/2023-17:57:59] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792
[10/31/2023-17:57:59] [V] [TRT] Tactic: 0x8b2f6e180c235792 Time: 7.20904
[10/31/2023-17:57:59] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986
[10/31/2023-17:57:59] [V] [TRT] Tactic: 0xd3f592fae61c7986 Time: 9.80745
[10/31/2023-17:57:59] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8
[10/31/2023-17:57:59] [V] [TRT] Tactic: 0x60edc1e1753cd5b8 Time: 7.27091
[10/31/2023-17:57:59] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393
[10/31/2023-17:58:00] [V] [TRT] Tactic: 0x0f6ba1e5b0320393 Time: 7.35978
[10/31/2023-17:58:00] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662
[10/31/2023-17:58:00] [V] [TRT] Tactic: 0x4b860619b6031662 Time: 4.83977
[10/31/2023-17:58:00] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da
[10/31/2023-17:58:00] [V] [TRT] Tactic: 0x19b79d0c4ff629da Time: 8.67192
[10/31/2023-17:58:00] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49
[10/31/2023-17:58:00] [V] [TRT] Tactic: 0x136c072084f41e49 Time: 8.70084
[10/31/2023-17:58:00] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23
[10/31/2023-17:58:00] [V] [TRT] Tactic: 0x4749124f62d8bd23 Time: 11.7865
[10/31/2023-17:58:00] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b
[10/31/2023-17:58:00] [V] [TRT] Tactic: 0x444f830f80ff098b Time: 7.34301
[10/31/2023-17:58:00] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655
[10/31/2023-17:58:00] [V] [TRT] Tactic: 0x671c943720ba8655 Time: 5.67037
[10/31/2023-17:58:00] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23
[10/31/2023-17:58:00] [V] [TRT] Tactic: 0xeb9516439d87ac23 Time: 7.2607
[10/31/2023-17:58:00] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50
[10/31/2023-17:58:00] [V] [TRT] Tactic: 0xef01fb6e433afa50 Time: 4.90072
[10/31/2023-17:58:00] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4
[10/31/2023-17:58:00] [V] [TRT] Tactic: 0xbb88763c3b0e94d4 Time: 7.16
[10/31/2023-17:58:00] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0
[10/31/2023-17:58:00] [V] [TRT] Tactic: 0x4b476758492c15b0 Time: 7.4388
[10/31/2023-17:58:00] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd
[10/31/2023-17:58:00] [V] [TRT] Tactic: 0x554e2e252e28b3fd Time: 4.70826
[10/31/2023-17:58:00] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d
[10/31/2023-17:58:00] [V] [TRT] Tactic: 0x9262f8f95beb428d Time: 7.39119
[10/31/2023-17:58:00] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac
[10/31/2023-17:58:00] [V] [TRT] Tactic: 0xd9f3bbc3e16b16ac Time: 4.82539
[10/31/2023-17:58:00] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3
[10/31/2023-17:58:00] [V] [TRT] Tactic: 0x4f6730716f78f4a3 Time: 4.76938
[10/31/2023-17:58:00] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d
[10/31/2023-17:58:00] [V] [TRT] Tactic: 0x028e842ce51fbd9d Time: 7.18827
[10/31/2023-17:58:00] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0xe37d64a29ca3101c Time: 5.76418
[10/31/2023-17:58:01] [V] [TRT] Fastest Tactic: 0xea88b51105501f96 Time: 4.58048
[10/31/2023-17:58:01] [V] [TRT] --------------- Timing Runner: backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:58:01] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:01] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xea88b51105501f96
[10/31/2023-17:58:01] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:01] [V] [TRT] *************** Autotuning format combination: Int8(2137344,33396:4,242,1), Float(8549376,33396,242,1) -> Float(8549376,33396,242,1) ***************
[10/31/2023-17:58:01] [V] [TRT] *************** Autotuning format combination: Int8(267168,33396:32,242,1), Float(8549376,33396,242,1) -> Float(8549376,33396,242,1) ***************
[10/31/2023-17:58:01] [V] [TRT] *************** Autotuning format combination: Int8(267168,33396:32,242,1), Float(267168,33396:32,242,1) -> Float(267168,33396:32,242,1) ***************
[10/31/2023-17:58:01] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:01] [V] [TRT] *************** Autotuning format combination: Int8(2137344,33396:4,242,1) -> Float(4274688,8349,121,1) ***************
[10/31/2023-17:58:01] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv (CudaDepthwiseConvolution)
[10/31/2023-17:58:01] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:01] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv (CaskConvolution)
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 0xb29abdd00304c881
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0xb29abdd00304c881 Time: 2.11672
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: 0xc25454de2efcccdc
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0xc25454de2efcccdc Time: 2.15877
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0xfe80445f7bb61a99 Time: 2.31567
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 0x14ef32afe5695907
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0x14ef32afe5695907 Time: 2.03717
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0xff6944b17d5b2e32 Time: 1.97306
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 0xec391424db39a74f
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0xec391424db39a74f Time: 2.364
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0x69c4e2ca38eadce2 Time: 2.09192
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 0x4d9d0d03129bceb1
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0x4d9d0d03129bceb1 Time: 2.22268
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 0xe01ccc14da28fa6f
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0xe01ccc14da28fa6f Time: 2.37205
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0xa4ae2d82115c3e83 Time: 2.14843
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: 0x1db6e8cf1382fbe0
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0x1db6e8cf1382fbe0 Time: 2.0181
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 0x2cf4271504d30e29
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0x2cf4271504d30e29 Time: 2.04711
[10/31/2023-17:58:01] [V] [TRT] Fastest Tactic: 0xff6944b17d5b2e32 Time: 1.97306
[10/31/2023-17:58:01] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv (CaskFlattenConvolution)
[10/31/2023-17:58:01] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:01] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[10/31/2023-17:58:01] [V] [TRT] *************** Autotuning format combination: Int8(267168,33396:32,242,1) -> Float(4274688,8349,121,1) ***************
[10/31/2023-17:58:01] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv (CaskConvolution)
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0xf04572b287451f42 Time: 1.64906
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0x733ba2a91a48d431 Time: 1.44847
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0x5e4f6d7c83746fd6 Time: 1.77289
[10/31/2023-17:58:01] [V] [TRT] Fastest Tactic: 0x733ba2a91a48d431 Time: 1.44847
[10/31/2023-17:58:01] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv (CaskFlattenConvolution)
[10/31/2023-17:58:01] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:01] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431
[10/31/2023-17:58:01] [V] [TRT] *************** Autotuning format combination: Int8(267168,33396:32,242,1) -> Float(133584,8349:32,121,1) ***************
[10/31/2023-17:58:01] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv (CaskConvolution)
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0x33a5c6dd086942c1 Time: 1.34955
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0xe742f4598442d2f1 Time: 1.20689
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0xdf7e1bd6a496d667 Time: 0.918094
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0x3f2b14dec582741e Time: 1.48679
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0xcf64a2ae51bf6b36 Time: 1.26885
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0xb86b920539eb9c79 Time: 0.945573
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0xa71946688cad8664 Time: 1.12508
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0xdc1f355deb032b87 Time: 1.51814
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0x6986b0c6a136276e Time: 1.40246
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0x271b998fe31732ef Time: 1.09188
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0x2468d082d0ff7c9a Time: 1.32913
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0xcd229658c16b33cd Time: 1.15535
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0x8141573686849b61 Time: 1.1194
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0x60b880e28fee7a0c Time: 1.11826
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0xb5f710b331ba8377 Time: 1.19707
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0x748f926574b7bdc4 Time: 1.15825
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0x5f1a472d416ff35e Time: 1.22903
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0x5bd8221bd57baf93 Time: 1.55254
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0x991db9fd58152c33 Time: 1.12619
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0x84942841d92b0552 Time: 1.05158
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0x445983715412fbda Time: 1.64115
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0x844ea9c00f711f19 Time: 1.04095
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec
[10/31/2023-17:58:01] [V] [TRT] Tactic: 0xa2f15b0a75b7dcec Time: 1.66571
[10/31/2023-17:58:01] [V] [TRT] Fastest Tactic: 0xdf7e1bd6a496d667 Time: 0.918094
[10/31/2023-17:58:01] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv (CaskFlattenConvolution)
[10/31/2023-17:58:01] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:01] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xdf7e1bd6a496d667
[10/31/2023-17:58:01] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:01] [V] [TRT] *************** Autotuning format combination: Int8(8549376,33396,242,1) -> Int8(4274688,8349,121,1) ***************
[10/31/2023-17:58:01] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv (CaskConvolution)
[10/31/2023-17:58:01] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:01] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:58:01] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:01] [V] [TRT] *************** Autotuning format combination: Int8(2137344,33396:4,242,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:58:01] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv (CudaDepthwiseConvolution)
[10/31/2023-17:58:01] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:01] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv (FusedConvActConvolution)
[10/31/2023-17:58:01] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:01] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv (CaskConvolution)
[10/31/2023-17:58:01] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0x8846c5916f34f7e9
[10/31/2023-17:58:02] [V] [TRT] Tactic: 0x8846c5916f34f7e9 Time: 17.7559
[10/31/2023-17:58:02] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 0xf6833cac33ebf690
[10/31/2023-17:58:02] [V] [TRT] Tactic: 0xf6833cac33ebf690 Time: 16.4294
[10/31/2023-17:58:02] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 0xa49b04dbd5d9448a
[10/31/2023-17:58:02] [V] [TRT] Tactic: 0xa49b04dbd5d9448a Time: 16.4252
[10/31/2023-17:58:02] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: 0xe0658ca00f36ea3e
[10/31/2023-17:58:02] [V] [TRT] Tactic: 0xe0658ca00f36ea3e Time: 17.5447
[10/31/2023-17:58:02] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 0x0270c2170caa55f8
[10/31/2023-17:58:02] [V] [TRT] Tactic: 0x0270c2170caa55f8 Time: 17.8605
[10/31/2023-17:58:02] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 0xcc3c5501ab7b5867
[10/31/2023-17:58:02] [V] [TRT] Tactic: 0xcc3c5501ab7b5867 Time: 16.627
[10/31/2023-17:58:02] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 0x50f6b3d3d5866716
[10/31/2023-17:58:02] [V] [TRT] Tactic: 0x50f6b3d3d5866716 Time: 16.9651
[10/31/2023-17:58:02] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 0x1e2179aecb150578
[10/31/2023-17:58:03] [V] [TRT] Tactic: 0x1e2179aecb150578 Time: 17.1033
[10/31/2023-17:58:03] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x1f0263042c683192
[10/31/2023-17:58:03] [V] [TRT] Tactic: 0x1f0263042c683192 Time: 16.9288
[10/31/2023-17:58:03] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 0x9aa46c15c2a1d8a7
[10/31/2023-17:58:03] [V] [TRT] Tactic: 0x9aa46c15c2a1d8a7 Time: 16.7186
[10/31/2023-17:58:03] [V] [TRT] Fastest Tactic: 0xa49b04dbd5d9448a Time: 16.4252
[10/31/2023-17:58:03] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:58:03] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:03] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa49b04dbd5d9448a
[10/31/2023-17:58:03] [V] [TRT] *************** Autotuning format combination: Int8(2137344,33396:4,242,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:58:03] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv (CaskConvolution)
[10/31/2023-17:58:03] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4
[10/31/2023-17:58:03] [V] [TRT] Tactic: 0xeb494e2e67f079d4 Time: 17.8204
[10/31/2023-17:58:03] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18
[10/31/2023-17:58:03] [V] [TRT] Tactic: 0x0f47434ace2a7d18 Time: 17.7673
[10/31/2023-17:58:03] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 0x0da8f7df8cfd2e37
[10/31/2023-17:58:03] [V] [TRT] Tactic: 0x0da8f7df8cfd2e37 Time: 16.5991
[10/31/2023-17:58:03] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: 0x2cf0cc5b8a069ae7
[10/31/2023-17:58:04] [V] [TRT] Tactic: 0x2cf0cc5b8a069ae7 Time: 17.5899
[10/31/2023-17:58:04] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: 0x320c30eda409da30
[10/31/2023-17:58:04] [V] [TRT] Tactic: 0x320c30eda409da30 Time: 16.943
[10/31/2023-17:58:04] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: 0xf3f27c40da3380fe
[10/31/2023-17:58:04] [V] [TRT] Tactic: 0xf3f27c40da3380fe Time: 16.9212
[10/31/2023-17:58:04] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: 0xc4f09503ebafdf51
[10/31/2023-17:58:04] [V] [TRT] Tactic: 0xc4f09503ebafdf51 Time: 16.4585
[10/31/2023-17:58:04] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 0xd4d010e83172b290
[10/31/2023-17:58:04] [V] [TRT] Tactic: 0xd4d010e83172b290 Time: 17.0628
[10/31/2023-17:58:04] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: 0x3c0834dbcd849973
[10/31/2023-17:58:04] [V] [TRT] Tactic: 0x3c0834dbcd849973 Time: 16.4499
[10/31/2023-17:58:04] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: 0xc911b61f7a5c0315
[10/31/2023-17:58:04] [V] [TRT] Tactic: 0xc911b61f7a5c0315 Time: 16.7042
[10/31/2023-17:58:04] [V] [TRT] Fastest Tactic: 0x3c0834dbcd849973 Time: 16.4499
[10/31/2023-17:58:04] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:58:04] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:04] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3c0834dbcd849973
[10/31/2023-17:58:04] [V] [TRT] *************** Autotuning format combination: Int8(267168,33396:32,242,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:58:04] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv (CudaGroupConvolution)
[10/31/2023-17:58:04] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:04] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv (CudaDepthwiseConvolution)
[10/31/2023-17:58:04] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:04] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv (FusedConvActConvolution)
[10/31/2023-17:58:04] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:04] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv (CaskConvolution)
[10/31/2023-17:58:04] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0x4cf18ad3e295ea18 Time: 2.55015
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0x89c1d75fe77808d5 Time: 2.81104
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0xf56c0ac895d82363 Time: 4.08859
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0xf33711e7c9ed4673 Time: 2.49416
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0xeb33cf0799237780 Time: 5.02507
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0x3818ca0093333b50 Time: 2.93077
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0xee2fce9480a52be7 Time: 7.09543
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x048dc59a1807b5bb
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0x048dc59a1807b5bb Time: 8.18427
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0x26a29d5b8b3f62af Time: 2.85335
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0xd277f13d771603ee Time: 3.05277
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0x11b75d98d540ee52 Time: 3.99168
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0x2f34f689bfca5071 Time: 5.49062
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0x9f6920f4a40549f4 Time: 2.88327
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0x2ff8bc85c8ced89e Time: 3.08161
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0x5e1b696f91f572c0 Time: 3.4497
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0x623894f465c90f26 Time: 2.62189
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0x955d593b1135a423 Time: 5.5685
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0x2728bbf79375f71d Time: 2.54741
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0x87620679fb5f37df Time: 2.48472
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0x705baf38e41eee0b Time: 3.40955
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0xd30e9f770878c3fd Time: 2.96325
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0x41b4640eb250ffd0 Time: 2.42317
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0xd3d41ef6de22d9b6 Time: 2.8953
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4805f65acfc5cf25
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0x4805f65acfc5cf25 Time: 7.75635
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0x8d50646eff0cde6d Time: 2.41797
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0xf8d4389f60adfa3c Time: 2.44128
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0x80cce6d8f75dc163 Time: 2.47661
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96
[10/31/2023-17:58:05] [V] [TRT] Tactic: 0xea88b51105501f96 Time: 2.38309
[10/31/2023-17:58:05] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0x53554c607d072468 Time: 4.94014
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0x70ccdad7e8ced9ab Time: 3.33004
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0x99901a83d7176ba9 Time: 3.34095
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0x8e1e9d670448aca7 Time: 5.41885
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0x204424cb4da28c02 Time: 2.90848
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0x0fbfd5fa9864ab49 Time: 3.66062
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0x9d0f90e0cec890bb Time: 3.68605
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0xa92dc4358ef627d4 Time: 3.88371
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0x7dcb6d55062fd530 Time: 2.76087
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0x8b2f6e180c235792 Time: 2.96483
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0xd3f592fae61c7986 Time: 5.37896
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0x60edc1e1753cd5b8 Time: 3.86483
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0x0f6ba1e5b0320393 Time: 3.01122
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0x4b860619b6031662 Time: 2.60239
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0x19b79d0c4ff629da Time: 4.68588
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0x136c072084f41e49 Time: 4.69643
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0x4749124f62d8bd23 Time: 6.99814
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0x444f830f80ff098b Time: 2.9059
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0x671c943720ba8655 Time: 3.13527
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0xeb9516439d87ac23 Time: 2.818
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0xef01fb6e433afa50 Time: 2.84675
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0xbb88763c3b0e94d4 Time: 4.04162
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0x4b476758492c15b0 Time: 4.75484
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0x554e2e252e28b3fd Time: 2.39543
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0x9262f8f95beb428d Time: 4.73642
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0xd9f3bbc3e16b16ac Time: 2.58925
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3
[10/31/2023-17:58:06] [V] [TRT] Tactic: 0x4f6730716f78f4a3 Time: 2.53858
[10/31/2023-17:58:06] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x028e842ce51fbd9d Time: 3.92703
[10/31/2023-17:58:07] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0xe37d64a29ca3101c Time: 3.15759
[10/31/2023-17:58:07] [V] [TRT] Fastest Tactic: 0xea88b51105501f96 Time: 2.38309
[10/31/2023-17:58:07] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:58:07] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:07] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xea88b51105501f96
[10/31/2023-17:58:07] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:07] [V] [TRT] *************** Autotuning format combination: Float(4274688,8349,121,1), Float(512,1,1,1), Float(512,1,1,1) -> Float(4274688,8349,121,1) ***************
[10/31/2023-17:58:07] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 260) [ElementWise], PWN(PWN((Unnamed Layer* 257) [Constant] + (Unnamed Layer* 258) [ElementWise], (Unnamed Layer* 259) [Unary]), (Unnamed Layer* 261) [ElementWise])) (PointWiseV2)
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.03765
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000001 Time: 2.29255
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000002 Time: 2.16768
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000003 Time: 1.78504
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000004 Time: 1.71803
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000005 Time: 1.79661
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000006 Time: 2.08599
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000007 Time: 1.99942
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000008 Time: 2.23123
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000009 Time: 2.27834
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x000000000000001c Time: 2.7736
[10/31/2023-17:58:07] [V] [TRT] Fastest Tactic: 0x0000000000000004 Time: 1.71803
[10/31/2023-17:58:07] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 260) [ElementWise], PWN(PWN((Unnamed Layer* 257) [Constant] + (Unnamed Layer* 258) [ElementWise], (Unnamed Layer* 259) [Unary]), (Unnamed Layer* 261) [ElementWise])) (PointWise)
[10/31/2023-17:58:07] [V] [TRT] PointWise has no valid tactics for this config, skipping
[10/31/2023-17:58:07] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000004
[10/31/2023-17:58:07] [V] [TRT] *************** Autotuning format combination: Float(4274688,1,61952,512), Float(512,1,512,512), Float(512,1,512,512) -> Float(4274688,1,61952,512) ***************
[10/31/2023-17:58:07] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 260) [ElementWise], PWN(PWN((Unnamed Layer* 257) [Constant] + (Unnamed Layer* 258) [ElementWise], (Unnamed Layer* 259) [Unary]), (Unnamed Layer* 261) [ElementWise])) (PointWiseV2)
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000000 Time: 3.38071
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000001 Time: 2.37695
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000002 Time: 2.29669
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000003 Time: 1.77826
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000004 Time: 1.75461
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000005 Time: 1.84723
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000006 Time: 1.52128
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000007 Time: 1.37008
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000008 Time: 1.58578
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000009 Time: 1.61906
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x000000000000001c Time: 3.28597
[10/31/2023-17:58:07] [V] [TRT] Fastest Tactic: 0x0000000000000007 Time: 1.37008
[10/31/2023-17:58:07] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 260) [ElementWise], PWN(PWN((Unnamed Layer* 257) [Constant] + (Unnamed Layer* 258) [ElementWise], (Unnamed Layer* 259) [Unary]), (Unnamed Layer* 261) [ElementWise])) (PointWise)
[10/31/2023-17:58:07] [V] [TRT] PointWise has no valid tactics for this config, skipping
[10/31/2023-17:58:07] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000007
[10/31/2023-17:58:07] [V] [TRT] *************** Autotuning format combination: Float(1068672,1:4,15488,128), Float(128,1:4,128,128), Float(128,1:4,128,128) -> Float(1068672,1:4,15488,128) ***************
[10/31/2023-17:58:07] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 260) [ElementWise], PWN(PWN((Unnamed Layer* 257) [Constant] + (Unnamed Layer* 258) [ElementWise], (Unnamed Layer* 259) [Unary]), (Unnamed Layer* 261) [ElementWise])) (PointWiseV2)
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000000 Time: 1.61975
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000001 Time: 1.37585
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000002 Time: 1.39234
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000003 Time: 1.19708
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000004 Time: 1.37659
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000005 Time: 1.62382
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000006 Time: 1.28438
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000007 Time: 1.47008
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000008 Time: 2.28618
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000009 Time: 2.63645
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x000000000000000a Time: 2.33576
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x000000000000000b Time: 1.73309
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x000000000000000c Time: 1.76246
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x000000000000000d Time: 1.36101
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x000000000000000e Time: 1.61428
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x000000000000000f Time: 1.60885
[10/31/2023-17:58:07] [V] [TRT] Tactic: 0x0000000000000010 Time: 1.22779
[10/31/2023-17:58:08] [V] [TRT] Tactic: 0x0000000000000011 Time: 1.43108
[10/31/2023-17:58:08] [V] [TRT] Tactic: 0x0000000000000012 Time: 1.72942
[10/31/2023-17:58:08] [V] [TRT] Tactic: 0x0000000000000013 Time: 1.96576
[10/31/2023-17:58:08] [V] [TRT] Tactic: 0x0000000000000014 Time: 3.65659
[10/31/2023-17:58:08] [V] [TRT] Tactic: 0x0000000000000015 Time: 2.5589
[10/31/2023-17:58:08] [V] [TRT] Tactic: 0x0000000000000016 Time: 1.87199
[10/31/2023-17:58:08] [V] [TRT] Tactic: 0x0000000000000017 Time: 1.5779
[10/31/2023-17:58:08] [V] [TRT] Tactic: 0x000000000000001c Time: 1.5773
[10/31/2023-17:58:08] [V] [TRT] Tactic: 0x000000000000001d Time: 2.34233
[10/31/2023-17:58:08] [V] [TRT] Tactic: 0x000000000000001e Time: 3.59535
[10/31/2023-17:58:08] [V] [TRT] Fastest Tactic: 0x0000000000000003 Time: 1.19708
[10/31/2023-17:58:08] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 260) [ElementWise], PWN(PWN((Unnamed Layer* 257) [Constant] + (Unnamed Layer* 258) [ElementWise], (Unnamed Layer* 259) [Unary]), (Unnamed Layer* 261) [ElementWise])) (PointWise)
[10/31/2023-17:58:08] [V] [TRT] PointWise has no valid tactics for this config, skipping
[10/31/2023-17:58:08] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000003
[10/31/2023-17:58:08] [V] [TRT] *************** Autotuning format combination: Float(133584,8349:32,121,1), Float(16,1:32,1,1), Float(16,1:32,1,1) -> Float(133584,8349:32,121,1) ***************
[10/31/2023-17:58:08] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 260) [ElementWise], PWN(PWN((Unnamed Layer* 257) [Constant] + (Unnamed Layer* 258) [ElementWise], (Unnamed Layer* 259) [Unary]), (Unnamed Layer* 261) [ElementWise])) (PointWiseV2)
[10/31/2023-17:58:08] [V] [TRT] Tactic: 0x0000000000000018 Time: 1.59208
[10/31/2023-17:58:08] [V] [TRT] Tactic: 0x0000000000000019 Time: 1.42701
[10/31/2023-17:58:08] [V] [TRT] Tactic: 0x000000000000001a Time: 1.26474
[10/31/2023-17:58:08] [V] [TRT] Tactic: 0x000000000000001b Time: 1.21419
[10/31/2023-17:58:08] [V] [TRT] Tactic: 0x000000000000001f Time: 1.51753
[10/31/2023-17:58:08] [V] [TRT] Fastest Tactic: 0x000000000000001b Time: 1.21419
[10/31/2023-17:58:08] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 260) [ElementWise], PWN(PWN((Unnamed Layer* 257) [Constant] + (Unnamed Layer* 258) [ElementWise], (Unnamed Layer* 259) [Unary]), (Unnamed Layer* 261) [ElementWise])) (PointWise)
[10/31/2023-17:58:08] [V] [TRT] PointWise has no valid tactics for this config, skipping
[10/31/2023-17:58:08] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001b
[10/31/2023-17:58:08] [V] [TRT] *************** Autotuning format combination: Float(1:4,8349,121,1), Float(1:4,1,1,1), Float(1:4,1,1,1) -> Float(1:4,8349,121,1) ***************
[10/31/2023-17:58:08] [V] [TRT] --------------- Timing Runner: PWN((Unnamed Layer* 260) [ElementWise], PWN(PWN((Unnamed Layer* 257) [Constant] + (Unnamed Layer* 258) [ElementWise], (Unnamed Layer* 259) [Unary]), (Unnamed Layer* 261) [ElementWise])) (PointWiseV2)
[10/31/2023-17:58:08] [V] [TRT] Tactic: 0x000000000000001c Time: 6.67582
[10/31/2023-17:58:08] [V] [TRT] Tactic: 0x000000000000001d Time: 9.59955
[10/31/2023-17:58:08] [V] [TRT] Tactic: 0x000000000000001e Time: 15.0476
[10/31/2023-17:58:08] [V] [TRT] Fastest Tactic: 0x000000000000001c Time: 6.67582
[10/31/2023-17:58:08] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001c
[10/31/2023-17:58:08] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:08] [V] [TRT] *************** Autotuning format combination: Float(4274688,8349,121,1) -> Float(4274688,8349,121,1) ***************
[10/31/2023-17:58:08] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.downsample.1.weight + (Unnamed Layer* 253) [Shuffle] + (Unnamed Layer* 262) [ElementWise] + backbone.layer4.0.bn2.bias + Identity_0 + (Unnamed Layer* 254) [Shuffle] + /backbone/layer4/layer4.0/downsample/downsample.1/BatchNormalization (Scale)
[10/31/2023-17:58:08] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.845783
[10/31/2023-17:58:08] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.845783
[10/31/2023-17:58:08] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0x0000000000000000
[10/31/2023-17:58:08] [V] [TRT] *************** Autotuning format combination: Float(4274688,1,61952,512) -> Float(4274688,1,61952,512) ***************
[10/31/2023-17:58:08] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.downsample.1.weight + (Unnamed Layer* 253) [Shuffle] + (Unnamed Layer* 262) [ElementWise] + backbone.layer4.0.bn2.bias + Identity_0 + (Unnamed Layer* 254) [Shuffle] + /backbone/layer4/layer4.0/downsample/downsample.1/BatchNormalization (Scale)
[10/31/2023-17:58:08] [V] [TRT] Scale has no valid tactics for this config, skipping
[10/31/2023-17:58:08] [V] [TRT] *************** Autotuning format combination: Float(1068672,1:4,15488,128) -> Float(1068672,1:4,15488,128) ***************
[10/31/2023-17:58:08] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.downsample.1.weight + (Unnamed Layer* 253) [Shuffle] + (Unnamed Layer* 262) [ElementWise] + backbone.layer4.0.bn2.bias + Identity_0 + (Unnamed Layer* 254) [Shuffle] + /backbone/layer4/layer4.0/downsample/downsample.1/BatchNormalization (Scale)
[10/31/2023-17:58:08] [V] [TRT] Scale has no valid tactics for this config, skipping
[10/31/2023-17:58:08] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:08] [V] [TRT] *************** Autotuning format combination: Int8(1068672,8349:4,121,1), Float(4274688,8349,121,1) -> Float(4274688,8349,121,1) ***************
[10/31/2023-17:58:08] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu (CaskConvolution)
[10/31/2023-17:58:08] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 0xb29abdd00304c881
[10/31/2023-17:58:09] [V] [TRT] Tactic: 0xb29abdd00304c881 Time: 33.952
[10/31/2023-17:58:09] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: 0x127de12a1f1a4809
[10/31/2023-17:58:09] [V] [TRT] Tactic: 0x127de12a1f1a4809 Time: 35.2662
[10/31/2023-17:58:09] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99
[10/31/2023-17:58:09] [V] [TRT] Tactic: 0xfe80445f7bb61a99 Time: 35.4717
[10/31/2023-17:58:09] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 0x14ef32afe5695907
[10/31/2023-17:58:09] [V] [TRT] Tactic: 0x14ef32afe5695907 Time: 32.8545
[10/31/2023-17:58:09] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 0xec391424db39a74f
[10/31/2023-17:58:10] [V] [TRT] Tactic: 0xec391424db39a74f Time: 34.1733
[10/31/2023-17:58:10] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2
[10/31/2023-17:58:10] [V] [TRT] Tactic: 0x69c4e2ca38eadce2 Time: 33.8696
[10/31/2023-17:58:10] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 0x4d9d0d03129bceb1
[10/31/2023-17:58:10] [V] [TRT] Tactic: 0x4d9d0d03129bceb1 Time: 33.4693
[10/31/2023-17:58:10] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 0xe01ccc14da28fa6f
[10/31/2023-17:58:11] [V] [TRT] Tactic: 0xe01ccc14da28fa6f Time: 35.5663
[10/31/2023-17:58:11] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 0x1b0534177b414e71
[10/31/2023-17:58:11] [V] [TRT] Tactic: 0x1b0534177b414e71 Time: 33.2348
[10/31/2023-17:58:11] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 0x2cf4271504d30e29
[10/31/2023-17:58:11] [V] [TRT] Tactic: 0x2cf4271504d30e29 Time: 32.9554
[10/31/2023-17:58:11] [V] [TRT] Fastest Tactic: 0x14ef32afe5695907 Time: 32.8545
[10/31/2023-17:58:11] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu (CaskFlattenConvolution)
[10/31/2023-17:58:11] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:11] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x14ef32afe5695907
[10/31/2023-17:58:11] [V] [TRT] *************** Autotuning format combination: Int8(133584,8349:32,121,1), Float(4274688,8349,121,1) -> Float(4274688,8349,121,1) ***************
[10/31/2023-17:58:11] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu (CaskConvolution)
[10/31/2023-17:58:11] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84
[10/31/2023-17:58:11] [V] [TRT] Tactic: 0x85c1a5f7f239cf84 Time: 8.99963
[10/31/2023-17:58:11] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10
[10/31/2023-17:58:11] [V] [TRT] Tactic: 0x2bcbba39f608bf10 Time: 7.44448
[10/31/2023-17:58:11] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808
[10/31/2023-17:58:11] [V] [TRT] Tactic: 0xac914b235d066808 Time: 5.14396
[10/31/2023-17:58:11] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e
[10/31/2023-17:58:11] [V] [TRT] Tactic: 0x23b890da05937b9e Time: 5.8249
[10/31/2023-17:58:11] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463
[10/31/2023-17:58:12] [V] [TRT] Tactic: 0xa8b56a226b057463 Time: 8.46223
[10/31/2023-17:58:12] [V] [TRT] Fastest Tactic: 0xac914b235d066808 Time: 5.14396
[10/31/2023-17:58:12] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu (CaskFlattenConvolution)
[10/31/2023-17:58:12] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:12] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xac914b235d066808
[10/31/2023-17:58:12] [V] [TRT] *************** Autotuning format combination: Int8(133584,8349:32,121,1), Float(133584,8349:32,121,1) -> Float(133584,8349:32,121,1) ***************
[10/31/2023-17:58:12] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu (CaskConvolution)
[10/31/2023-17:58:12] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c
[10/31/2023-17:58:12] [V] [TRT] Tactic: 0xb936321f82fd390c Time: 8.65145
[10/31/2023-17:58:12] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa
[10/31/2023-17:58:12] [V] [TRT] Tactic: 0x55edef142e02adaa Time: 4.69658
[10/31/2023-17:58:12] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1
[10/31/2023-17:58:12] [V] [TRT] Tactic: 0xe742f4598442d2f1 Time: 5.72876
[10/31/2023-17:58:12] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e
[10/31/2023-17:58:12] [V] [TRT] Tactic: 0x3f2b14dec582741e Time: 8.7184
[10/31/2023-17:58:12] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4
[10/31/2023-17:58:12] [V] [TRT] Tactic: 0x0edd5d0285e564d4 Time: 5.66209
[10/31/2023-17:58:12] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79
[10/31/2023-17:58:12] [V] [TRT] Tactic: 0xb86b920539eb9c79 Time: 4.73455
[10/31/2023-17:58:12] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87
[10/31/2023-17:58:12] [V] [TRT] Tactic: 0xdc1f355deb032b87 Time: 10.4774
[10/31/2023-17:58:12] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb
[10/31/2023-17:58:12] [V] [TRT] Tactic: 0x45f7566cdb2b10fb Time: 9.76342
[10/31/2023-17:58:12] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1
[10/31/2023-17:58:12] [V] [TRT] Tactic: 0xea2b7420057a01c1 Time: 4.98251
[10/31/2023-17:58:12] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e
[10/31/2023-17:58:12] [V] [TRT] Tactic: 0x6986b0c6a136276e Time: 6.29368
[10/31/2023-17:58:12] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922
[10/31/2023-17:58:12] [V] [TRT] Tactic: 0xad886d4d69834922 Time: 5.66904
[10/31/2023-17:58:12] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a
[10/31/2023-17:58:12] [V] [TRT] Tactic: 0x2468d082d0ff7c9a Time: 9.67829
[10/31/2023-17:58:12] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61
[10/31/2023-17:58:13] [V] [TRT] Tactic: 0x8141573686849b61 Time: 5.784
[10/31/2023-17:58:13] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377
[10/31/2023-17:58:13] [V] [TRT] Tactic: 0xb5f710b331ba8377 Time: 5.06492
[10/31/2023-17:58:13] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4
[10/31/2023-17:58:13] [V] [TRT] Tactic: 0x748f926574b7bdc4 Time: 4.99216
[10/31/2023-17:58:13] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f
[10/31/2023-17:58:13] [V] [TRT] Tactic: 0x0e07dc8353bf7e9f Time: 4.88286
[10/31/2023-17:58:13] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800
[10/31/2023-17:58:13] [V] [TRT] Tactic: 0x47b1629a4bff4800 Time: 5.06621
[10/31/2023-17:58:13] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e
[10/31/2023-17:58:13] [V] [TRT] Tactic: 0x5f1a472d416ff35e Time: 7.10251
[10/31/2023-17:58:13] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33
[10/31/2023-17:58:13] [V] [TRT] Tactic: 0x991db9fd58152c33 Time: 4.89966
[10/31/2023-17:58:13] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e
[10/31/2023-17:58:13] [V] [TRT] Tactic: 0xd14bd6d95fefd45e Time: 11.439
[10/31/2023-17:58:13] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9
[10/31/2023-17:58:13] [V] [TRT] Tactic: 0x2d8ab2aa0639fda9 Time: 7.01588
[10/31/2023-17:58:13] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda
[10/31/2023-17:58:13] [V] [TRT] Tactic: 0x445983715412fbda Time: 11.6561
[10/31/2023-17:58:13] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec
[10/31/2023-17:58:13] [V] [TRT] Tactic: 0xa2f15b0a75b7dcec Time: 9.04981
[10/31/2023-17:58:13] [V] [TRT] Fastest Tactic: 0x55edef142e02adaa Time: 4.69658
[10/31/2023-17:58:13] [V] [TRT] --------------- Timing Runner: backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu (CaskFlattenConvolution)
[10/31/2023-17:58:13] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:13] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x55edef142e02adaa
[10/31/2023-17:58:13] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:13] [V] [TRT] *************** Autotuning format combination: Int8(4274688,8349,121,1) -> Int8(4274688,8349,121,1) ***************
[10/31/2023-17:58:13] [V] [TRT] --------------- Timing Runner: backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv (CaskConvolution)
[10/31/2023-17:58:13] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:13] [V] [TRT] --------------- Timing Runner: backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:58:13] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:13] [V] [TRT] *************** Autotuning format combination: Int8(1068672,8349:4,121,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:58:13] [V] [TRT] --------------- Timing Runner: backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv (CudaDepthwiseConvolution)
[10/31/2023-17:58:13] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:13] [V] [TRT] --------------- Timing Runner: backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv (FusedConvActConvolution)
[10/31/2023-17:58:13] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:13] [V] [TRT] --------------- Timing Runner: backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv (CaskConvolution)
[10/31/2023-17:58:13] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0x8846c5916f34f7e9
[10/31/2023-17:58:14] [V] [TRT] Tactic: 0x8846c5916f34f7e9 Time: 35.2298
[10/31/2023-17:58:14] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 0xf6833cac33ebf690
[10/31/2023-17:58:14] [V] [TRT] Tactic: 0xf6833cac33ebf690 Time: 32.7139
[10/31/2023-17:58:14] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 0xa49b04dbd5d9448a
[10/31/2023-17:58:14] [V] [TRT] Tactic: 0xa49b04dbd5d9448a Time: 32.7699
[10/31/2023-17:58:14] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: 0xe0658ca00f36ea3e
[10/31/2023-17:58:14] [V] [TRT] Tactic: 0xe0658ca00f36ea3e Time: 34.4186
[10/31/2023-17:58:15] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 0x0270c2170caa55f8
[10/31/2023-17:58:15] [V] [TRT] Tactic: 0x0270c2170caa55f8 Time: 35.3099
[10/31/2023-17:58:15] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 0xcc3c5501ab7b5867
[10/31/2023-17:58:15] [V] [TRT] Tactic: 0xcc3c5501ab7b5867 Time: 33.0416
[10/31/2023-17:58:15] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 0x50f6b3d3d5866716
[10/31/2023-17:58:15] [V] [TRT] Tactic: 0x50f6b3d3d5866716 Time: 33.7316
[10/31/2023-17:58:15] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 0x1e2179aecb150578
[10/31/2023-17:58:16] [V] [TRT] Tactic: 0x1e2179aecb150578 Time: 33.9449
[10/31/2023-17:58:16] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x1f0263042c683192
[10/31/2023-17:58:16] [V] [TRT] Tactic: 0x1f0263042c683192 Time: 33.6632
[10/31/2023-17:58:16] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 0x9aa46c15c2a1d8a7
[10/31/2023-17:58:16] [V] [TRT] Tactic: 0x9aa46c15c2a1d8a7 Time: 33.2606
[10/31/2023-17:58:16] [V] [TRT] Fastest Tactic: 0xf6833cac33ebf690 Time: 32.7139
[10/31/2023-17:58:16] [V] [TRT] --------------- Timing Runner: backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:58:16] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:16] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xf6833cac33ebf690
[10/31/2023-17:58:16] [V] [TRT] *************** Autotuning format combination: Int8(1068672,8349:4,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:58:16] [V] [TRT] --------------- Timing Runner: backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv (CaskConvolution)
[10/31/2023-17:58:16] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4
[10/31/2023-17:58:17] [V] [TRT] Tactic: 0xeb494e2e67f079d4 Time: 35.3632
[10/31/2023-17:58:17] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18
[10/31/2023-17:58:17] [V] [TRT] Tactic: 0x0f47434ace2a7d18 Time: 35.2218
[10/31/2023-17:58:17] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 0x0da8f7df8cfd2e37
[10/31/2023-17:58:17] [V] [TRT] Tactic: 0x0da8f7df8cfd2e37 Time: 33.0839
[10/31/2023-17:58:17] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: 0x2cf0cc5b8a069ae7
[10/31/2023-17:58:17] [V] [TRT] Tactic: 0x2cf0cc5b8a069ae7 Time: 34.4305
[10/31/2023-17:58:17] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: 0x320c30eda409da30
[10/31/2023-17:58:18] [V] [TRT] Tactic: 0x320c30eda409da30 Time: 33.7952
[10/31/2023-17:58:18] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: 0xf3f27c40da3380fe
[10/31/2023-17:58:18] [V] [TRT] Tactic: 0xf3f27c40da3380fe Time: 33.6745
[10/31/2023-17:58:18] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: 0xc4f09503ebafdf51
[10/31/2023-17:58:18] [V] [TRT] Tactic: 0xc4f09503ebafdf51 Time: 32.8399
[10/31/2023-17:58:18] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 0xd4d010e83172b290
[10/31/2023-17:58:19] [V] [TRT] Tactic: 0xd4d010e83172b290 Time: 33.9697
[10/31/2023-17:58:19] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: 0x3c0834dbcd849973
[10/31/2023-17:58:19] [V] [TRT] Tactic: 0x3c0834dbcd849973 Time: 32.7003
[10/31/2023-17:58:19] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: 0xc911b61f7a5c0315
[10/31/2023-17:58:19] [V] [TRT] Tactic: 0xc911b61f7a5c0315 Time: 33.2259
[10/31/2023-17:58:19] [V] [TRT] Fastest Tactic: 0x3c0834dbcd849973 Time: 32.7003
[10/31/2023-17:58:19] [V] [TRT] --------------- Timing Runner: backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:58:19] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:19] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3c0834dbcd849973
[10/31/2023-17:58:19] [V] [TRT] *************** Autotuning format combination: Int8(133584,8349:32,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:58:19] [V] [TRT] --------------- Timing Runner: backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv (CudaGroupConvolution)
[10/31/2023-17:58:19] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:19] [V] [TRT] --------------- Timing Runner: backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv (CudaDepthwiseConvolution)
[10/31/2023-17:58:19] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:19] [V] [TRT] --------------- Timing Runner: backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv (FusedConvActConvolution)
[10/31/2023-17:58:19] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:19] [V] [TRT] --------------- Timing Runner: backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv (CaskConvolution)
[10/31/2023-17:58:19] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18
[10/31/2023-17:58:19] [V] [TRT] Tactic: 0x4cf18ad3e295ea18 Time: 4.5915
[10/31/2023-17:58:19] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5
[10/31/2023-17:58:19] [V] [TRT] Tactic: 0x89c1d75fe77808d5 Time: 5.14877
[10/31/2023-17:58:19] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363
[10/31/2023-17:58:19] [V] [TRT] Tactic: 0xf56c0ac895d82363 Time: 6.75806
[10/31/2023-17:58:19] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673
[10/31/2023-17:58:19] [V] [TRT] Tactic: 0xf33711e7c9ed4673 Time: 4.51438
[10/31/2023-17:58:19] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780
[10/31/2023-17:58:19] [V] [TRT] Tactic: 0xeb33cf0799237780 Time: 6.77599
[10/31/2023-17:58:19] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50
[10/31/2023-17:58:20] [V] [TRT] Tactic: 0x3818ca0093333b50 Time: 4.99743
[10/31/2023-17:58:20] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7
[10/31/2023-17:58:20] [V] [TRT] Tactic: 0xee2fce9480a52be7 Time: 11.1867
[10/31/2023-17:58:20] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x048dc59a1807b5bb
[10/31/2023-17:58:20] [V] [TRT] Tactic: 0x048dc59a1807b5bb Time: 15.3297
[10/31/2023-17:58:20] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af
[10/31/2023-17:58:20] [V] [TRT] Tactic: 0x26a29d5b8b3f62af Time: 4.66133
[10/31/2023-17:58:20] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee
[10/31/2023-17:58:20] [V] [TRT] Tactic: 0xd277f13d771603ee Time: 5.27145
[10/31/2023-17:58:20] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52
[10/31/2023-17:58:20] [V] [TRT] Tactic: 0x11b75d98d540ee52 Time: 6.9592
[10/31/2023-17:58:20] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071
[10/31/2023-17:58:20] [V] [TRT] Tactic: 0x2f34f689bfca5071 Time: 8.24416
[10/31/2023-17:58:20] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4
[10/31/2023-17:58:20] [V] [TRT] Tactic: 0x9f6920f4a40549f4 Time: 5.24461
[10/31/2023-17:58:20] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e
[10/31/2023-17:58:20] [V] [TRT] Tactic: 0x2ff8bc85c8ced89e Time: 5.30858
[10/31/2023-17:58:20] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0
[10/31/2023-17:58:20] [V] [TRT] Tactic: 0x5e1b696f91f572c0 Time: 5.37982
[10/31/2023-17:58:20] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26
[10/31/2023-17:58:20] [V] [TRT] Tactic: 0x623894f465c90f26 Time: 4.57938
[10/31/2023-17:58:20] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423
[10/31/2023-17:58:20] [V] [TRT] Tactic: 0x955d593b1135a423 Time: 8.3206
[10/31/2023-17:58:20] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d
[10/31/2023-17:58:20] [V] [TRT] Tactic: 0x2728bbf79375f71d Time: 4.60721
[10/31/2023-17:58:20] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df
[10/31/2023-17:58:20] [V] [TRT] Tactic: 0x87620679fb5f37df Time: 4.45902
[10/31/2023-17:58:20] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b
[10/31/2023-17:58:20] [V] [TRT] Tactic: 0x705baf38e41eee0b Time: 5.28725
[10/31/2023-17:58:20] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd
[10/31/2023-17:58:20] [V] [TRT] Tactic: 0xd30e9f770878c3fd Time: 5.15639
[10/31/2023-17:58:20] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0
[10/31/2023-17:58:21] [V] [TRT] Tactic: 0x41b4640eb250ffd0 Time: 4.4451
[10/31/2023-17:58:21] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6
[10/31/2023-17:58:21] [V] [TRT] Tactic: 0xd3d41ef6de22d9b6 Time: 4.51055
[10/31/2023-17:58:21] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4805f65acfc5cf25
[10/31/2023-17:58:21] [V] [TRT] Tactic: 0x4805f65acfc5cf25 Time: 15.0451
[10/31/2023-17:58:21] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d
[10/31/2023-17:58:21] [V] [TRT] Tactic: 0x8d50646eff0cde6d Time: 4.40795
[10/31/2023-17:58:21] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c
[10/31/2023-17:58:21] [V] [TRT] Tactic: 0xf8d4389f60adfa3c Time: 4.43234
[10/31/2023-17:58:21] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163
[10/31/2023-17:58:21] [V] [TRT] Tactic: 0x80cce6d8f75dc163 Time: 4.48849
[10/31/2023-17:58:21] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96
[10/31/2023-17:58:21] [V] [TRT] Tactic: 0xea88b51105501f96 Time: 4.35451
[10/31/2023-17:58:21] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468
[10/31/2023-17:58:21] [V] [TRT] Tactic: 0x53554c607d072468 Time: 6.66065
[10/31/2023-17:58:21] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab
[10/31/2023-17:58:21] [V] [TRT] Tactic: 0x70ccdad7e8ced9ab Time: 5.44923
[10/31/2023-17:58:21] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9
[10/31/2023-17:58:21] [V] [TRT] Tactic: 0x99901a83d7176ba9 Time: 5.49784
[10/31/2023-17:58:21] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7
[10/31/2023-17:58:21] [V] [TRT] Tactic: 0x8e1e9d670448aca7 Time: 9.27209
[10/31/2023-17:58:21] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02
[10/31/2023-17:58:21] [V] [TRT] Tactic: 0x204424cb4da28c02 Time: 4.53099
[10/31/2023-17:58:21] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49
[10/31/2023-17:58:21] [V] [TRT] Tactic: 0x0fbfd5fa9864ab49 Time: 6.65521
[10/31/2023-17:58:21] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb
[10/31/2023-17:58:21] [V] [TRT] Tactic: 0x9d0f90e0cec890bb Time: 6.71949
[10/31/2023-17:58:21] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4
[10/31/2023-17:58:21] [V] [TRT] Tactic: 0xa92dc4358ef627d4 Time: 6.79433
[10/31/2023-17:58:21] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530
[10/31/2023-17:58:21] [V] [TRT] Tactic: 0x7dcb6d55062fd530 Time: 4.94796
[10/31/2023-17:58:21] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792
[10/31/2023-17:58:21] [V] [TRT] Tactic: 0x8b2f6e180c235792 Time: 5.22119
[10/31/2023-17:58:21] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986
[10/31/2023-17:58:22] [V] [TRT] Tactic: 0xd3f592fae61c7986 Time: 9.41113
[10/31/2023-17:58:22] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8
[10/31/2023-17:58:22] [V] [TRT] Tactic: 0x60edc1e1753cd5b8 Time: 6.81688
[10/31/2023-17:58:22] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393
[10/31/2023-17:58:22] [V] [TRT] Tactic: 0x0f6ba1e5b0320393 Time: 5.30661
[10/31/2023-17:58:22] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662
[10/31/2023-17:58:22] [V] [TRT] Tactic: 0x4b860619b6031662 Time: 4.66416
[10/31/2023-17:58:22] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da
[10/31/2023-17:58:22] [V] [TRT] Tactic: 0x19b79d0c4ff629da Time: 8.19499
[10/31/2023-17:58:22] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49
[10/31/2023-17:58:22] [V] [TRT] Tactic: 0x136c072084f41e49 Time: 8.20275
[10/31/2023-17:58:22] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23
[10/31/2023-17:58:22] [V] [TRT] Tactic: 0x4749124f62d8bd23 Time: 11.0643
[10/31/2023-17:58:22] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b
[10/31/2023-17:58:22] [V] [TRT] Tactic: 0x444f830f80ff098b Time: 5.22619
[10/31/2023-17:58:22] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655
[10/31/2023-17:58:22] [V] [TRT] Tactic: 0x671c943720ba8655 Time: 5.45421
[10/31/2023-17:58:22] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23
[10/31/2023-17:58:22] [V] [TRT] Tactic: 0xeb9516439d87ac23 Time: 5.24176
[10/31/2023-17:58:22] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50
[10/31/2023-17:58:22] [V] [TRT] Tactic: 0xef01fb6e433afa50 Time: 4.61853
[10/31/2023-17:58:22] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4
[10/31/2023-17:58:22] [V] [TRT] Tactic: 0xbb88763c3b0e94d4 Time: 6.66549
[10/31/2023-17:58:22] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0
[10/31/2023-17:58:22] [V] [TRT] Tactic: 0x4b476758492c15b0 Time: 7.02068
[10/31/2023-17:58:22] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd
[10/31/2023-17:58:22] [V] [TRT] Tactic: 0x554e2e252e28b3fd Time: 4.43093
[10/31/2023-17:58:22] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d
[10/31/2023-17:58:22] [V] [TRT] Tactic: 0x9262f8f95beb428d Time: 7.05437
[10/31/2023-17:58:22] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac
[10/31/2023-17:58:22] [V] [TRT] Tactic: 0xd9f3bbc3e16b16ac Time: 4.59336
[10/31/2023-17:58:22] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3
[10/31/2023-17:58:22] [V] [TRT] Tactic: 0x4f6730716f78f4a3 Time: 4.55136
[10/31/2023-17:58:22] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0x028e842ce51fbd9d Time: 6.8595
[10/31/2023-17:58:23] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0xe37d64a29ca3101c Time: 5.48922
[10/31/2023-17:58:23] [V] [TRT] Fastest Tactic: 0xea88b51105501f96 Time: 4.35451
[10/31/2023-17:58:23] [V] [TRT] --------------- Timing Runner: backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv (CaskFlattenConvolution)
[10/31/2023-17:58:23] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:23] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xea88b51105501f96
[10/31/2023-17:58:23] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:23] [V] [TRT] *************** Autotuning format combination: Int8(1068672,8349:4,121,1), Float(4274688,8349,121,1) -> Float(4274688,8349,121,1) ***************
[10/31/2023-17:58:23] [V] [TRT] *************** Autotuning format combination: Int8(133584,8349:32,121,1), Float(4274688,8349,121,1) -> Float(4274688,8349,121,1) ***************
[10/31/2023-17:58:23] [V] [TRT] *************** Autotuning format combination: Int8(133584,8349:32,121,1), Float(133584,8349:32,121,1) -> Float(133584,8349:32,121,1) ***************
[10/31/2023-17:58:23] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:23] [V] [TRT] *************** Autotuning format combination: Int8(4274688,8349,121,1) -> Int8(2137344,8349,121,1) ***************
[10/31/2023-17:58:23] [V] [TRT] --------------- Timing Runner: neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:23] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:23] [V] [TRT] --------------- Timing Runner: neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:23] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:23] [V] [TRT] *************** Autotuning format combination: Int8(1068672,8349:4,121,1) -> Int8(534336,8349:4,121,1) ***************
[10/31/2023-17:58:23] [V] [TRT] --------------- Timing Runner: neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:23] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:23] [V] [TRT] --------------- Timing Runner: neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:23] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:23] [V] [TRT] --------------- Timing Runner: neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:23] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:23] [V] [TRT] --------------- Timing Runner: neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:23] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:23] [V] [TRT] *************** Autotuning format combination: Int8(1068672,8349:4,121,1) -> Int8(66792,8349:32,121,1) ***************
[10/31/2023-17:58:23] [V] [TRT] --------------- Timing Runner: neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:23] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:23] [V] [TRT] --------------- Timing Runner: neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:23] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:23] [V] [TRT] *************** Autotuning format combination: Int8(133584,8349:32,121,1) -> Int8(66792,8349:32,121,1) ***************
[10/31/2023-17:58:23] [V] [TRT] --------------- Timing Runner: neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) (CudaGroupConvolution)
[10/31/2023-17:58:23] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:23] [V] [TRT] --------------- Timing Runner: neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:23] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:23] [V] [TRT] --------------- Timing Runner: neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:23] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:23] [V] [TRT] --------------- Timing Runner: neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0xeb1602207b0c3d49
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0xeb1602207b0c3d49 Time: 0.80027
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_linkable Tactic: 0x1f54ac6f2468a25d
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0x1f54ac6f2468a25d Time: 0.542176
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x25bf7139bbd93bc8
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0x25bf7139bbd93bc8 Time: 0.443479
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x6d066fa755f280f9
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0x6d066fa755f280f9 Time: 0.456498
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x3f084feb7c0e1257
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0x3f084feb7c0e1257 Time: 0.994715
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x4df7e35b8de312d7
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0x4df7e35b8de312d7 Time: 0.546139
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xb960a0bfa8850b75
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0xb960a0bfa8850b75 Time: 0.633138
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x5e3d76022ffbdce2
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0x5e3d76022ffbdce2 Time: 1.15264
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x3321b9c38b0ba08d
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0x3321b9c38b0ba08d Time: 0.521614
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0x9b90c142f7ba1756
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0x9b90c142f7ba1756 Time: 0.740366
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0xa40d21cb7ebb07bf
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0xa40d21cb7ebb07bf Time: 0.621257
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x0779fa4ac8f9f2ee
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0x0779fa4ac8f9f2ee Time: 0.519936
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x14db55f2724dda9f
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0x14db55f2724dda9f Time: 0.511474
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0xd7c8759ddafac114
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0xd7c8759ddafac114 Time: 0.706423
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x4bb804b3b091b301
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0x4bb804b3b091b301 Time: 1.06019
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x2a8d3d5ae3647db4
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0x2a8d3d5ae3647db4 Time: 0.887397
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0xc69ab4295ea0103e
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0xc69ab4295ea0103e Time: 0.777079
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x892689d63e1f5cb0
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0x892689d63e1f5cb0 Time: 0.590089
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x9ca4b256db721f9d
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0x9ca4b256db721f9d Time: 0.517344
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x4862d9a1cd4ec818
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0x4862d9a1cd4ec818 Time: 0.714949
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_linkable Tactic: 0x58613478e5ec50a9
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0x58613478e5ec50a9 Time: 0.550615
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_linkable Tactic: 0x826da74f523d0cce
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0x826da74f523d0cce Time: 0.545499
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x459509ec55322f60
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0x459509ec55322f60 Time: 0.568923
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x10c7e671ac664e67
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0x10c7e671ac664e67 Time: 0.512288
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x54c7c858d168fe4a
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0x54c7c858d168fe4a Time: 0.672347
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xace5d20e37ef6496
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0xace5d20e37ef6496 Time: 0.739863
[10/31/2023-17:58:23] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xcdd0ebe7641aaa23
[10/31/2023-17:58:23] [V] [TRT] Tactic: 0xcdd0ebe7641aaa23 Time: 0.982176
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0xa767fa987290f29d
[10/31/2023-17:58:24] [V] [TRT] Tactic: 0xa767fa987290f29d Time: 0.528329
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0xd851a0a9f1b84453
[10/31/2023-17:58:24] [V] [TRT] Tactic: 0xd851a0a9f1b84453 Time: 0.514057
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0x8ac200f673e0c67c
[10/31/2023-17:58:24] [V] [TRT] Tactic: 0x8ac200f673e0c67c Time: 0.552274
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x4a79db7911ff56ec
[10/31/2023-17:58:24] [V] [TRT] Tactic: 0x4a79db7911ff56ec Time: 0.453431
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0xf6d111860199beba
[10/31/2023-17:58:24] [V] [TRT] Tactic: 0xf6d111860199beba Time: 0.476183
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_linkable Tactic: 0x7a6e06a0138186b0
[10/31/2023-17:58:24] [V] [TRT] Tactic: 0x7a6e06a0138186b0 Time: 0.535703
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xe8c4479eeddac419
[10/31/2023-17:58:24] [V] [TRT] Tactic: 0xe8c4479eeddac419 Time: 0.462537
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x15f0bde1f6cbbf61
[10/31/2023-17:58:24] [V] [TRT] Tactic: 0x15f0bde1f6cbbf61 Time: 0.421157
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x4ce54017654ce216
[10/31/2023-17:58:24] [V] [TRT] Tactic: 0x4ce54017654ce216 Time: 0.437701
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x16bf68703cf6ee6c
[10/31/2023-17:58:24] [V] [TRT] Tactic: 0x16bf68703cf6ee6c Time: 0.480425
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x98744862ba458d9a
[10/31/2023-17:58:24] [V] [TRT] Tactic: 0x98744862ba458d9a Time: 0.531442
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x5e22a4cd8f7a7f29
[10/31/2023-17:58:24] [V] [TRT] Tactic: 0x5e22a4cd8f7a7f29 Time: 0.569289
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x8aed0c9812ff1e34
[10/31/2023-17:58:24] [V] [TRT] Tactic: 0x8aed0c9812ff1e34 Time: 0.44283
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xd8559956fb70c5c0
[10/31/2023-17:58:24] [V] [TRT] Tactic: 0xd8559956fb70c5c0 Time: 0.858601
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x6eb8c2607f51ccff
[10/31/2023-17:58:24] [V] [TRT] Tactic: 0x6eb8c2607f51ccff Time: 0.736334
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0xb3e028693e71e990
[10/31/2023-17:58:24] [V] [TRT] Tactic: 0xb3e028693e71e990 Time: 0.663803
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x90af0952ccc337b8
[10/31/2023-17:58:24] [V] [TRT] Tactic: 0x90af0952ccc337b8 Time: 0.821499
[10/31/2023-17:58:24] [V] [TRT] Fastest Tactic: 0x15f0bde1f6cbbf61 Time: 0.421157
[10/31/2023-17:58:24] [V] [TRT] --------------- Timing Runner: neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:24] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:24] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x15f0bde1f6cbbf61
[10/31/2023-17:58:24] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:24] [V] [TRT] *************** Autotuning format combination: Int8(2137344,8349,121,1) -> Int8(4274688,8349,121,1) ***************
[10/31/2023-17:58:24] [V] [TRT] --------------- Timing Runner: neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:24] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:24] [V] [TRT] --------------- Timing Runner: neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:24] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:24] [V] [TRT] *************** Autotuning format combination: Int8(534336,8349:4,121,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:58:24] [V] [TRT] --------------- Timing Runner: neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:24] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:24] [V] [TRT] --------------- Timing Runner: neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:24] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:24] [V] [TRT] --------------- Timing Runner: neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:24] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:24] [V] [TRT] --------------- Timing Runner: neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:24] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:24] [V] [TRT] *************** Autotuning format combination: Int8(534336,8349:4,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:58:24] [V] [TRT] --------------- Timing Runner: neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:24] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:24] [V] [TRT] --------------- Timing Runner: neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:24] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:24] [V] [TRT] *************** Autotuning format combination: Int8(66792,8349:32,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:58:24] [V] [TRT] --------------- Timing Runner: neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) (CudaGroupConvolution)
[10/31/2023-17:58:24] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:24] [V] [TRT] --------------- Timing Runner: neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:24] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:24] [V] [TRT] --------------- Timing Runner: neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:24] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:24] [V] [TRT] --------------- Timing Runner: neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0x7ddd3b3f55023bab
[10/31/2023-17:58:24] [V] [TRT] Tactic: 0x7ddd3b3f55023bab Time: 5.87122
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_linkable Tactic: 0x1f54ac6f2468a25d
[10/31/2023-17:58:24] [V] [TRT] Tactic: 0x1f54ac6f2468a25d Time: 2.64656
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0x4a15c8f5eb51fe8e
[10/31/2023-17:58:24] [V] [TRT] Tactic: 0x4a15c8f5eb51fe8e Time: 2.6232
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x3f084feb7c0e1257
[10/31/2023-17:58:24] [V] [TRT] Tactic: 0x3f084feb7c0e1257 Time: 5.66329
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xb960a0bfa8850b75
[10/31/2023-17:58:24] [V] [TRT] Tactic: 0xb960a0bfa8850b75 Time: 3.59502
[10/31/2023-17:58:24] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xa70be914883e5aff
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0xa70be914883e5aff Time: 2.81947
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x5e3d76022ffbdce2
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0x5e3d76022ffbdce2 Time: 6.74965
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xa197727afc8dee05
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0xa197727afc8dee05 Time: 2.42746
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0x9b90c142f7ba1756
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0x9b90c142f7ba1756 Time: 3.63904
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0xd7c8759ddafac114
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0xd7c8759ddafac114 Time: 3.60517
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0x71d6803b42b3138e
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0x71d6803b42b3138e Time: 2.60187
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0x83caf00de690c0ec
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0x83caf00de690c0ec Time: 4.90807
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x4bb804b3b091b301
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0x4bb804b3b091b301 Time: 5.92124
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x2a8d3d5ae3647db4
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0x2a8d3d5ae3647db4 Time: 4.45204
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0xc69ab4295ea0103e
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0xc69ab4295ea0103e Time: 4.51614
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x892689d63e1f5cb0
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0x892689d63e1f5cb0 Time: 3.29674
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_linkable Tactic: 0x58613478e5ec50a9
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0x58613478e5ec50a9 Time: 2.83325
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_linkable Tactic: 0x826da74f523d0cce
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0x826da74f523d0cce Time: 2.77588
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x459509ec55322f60
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0x459509ec55322f60 Time: 2.96322
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x10c7e671ac664e67
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0x10c7e671ac664e67 Time: 2.86896
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x54c7c858d168fe4a
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0x54c7c858d168fe4a Time: 3.65147
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xace5d20e37ef6496
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0xace5d20e37ef6496 Time: 4.16703
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xcdd0ebe7641aaa23
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0xcdd0ebe7641aaa23 Time: 5.61042
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0x8ac200f673e0c67c
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0x8ac200f673e0c67c Time: 2.57775
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0x1ba323eb9858b2a9
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0x1ba323eb9858b2a9 Time: 2.55721
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xb35096a016bb733a
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0xb35096a016bb733a Time: 3.59297
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_linkable Tactic: 0x7a6e06a0138186b0
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0x7a6e06a0138186b0 Time: 2.6678
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xe8c4479eeddac419
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0xe8c4479eeddac419 Time: 2.44438
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xfbcd5a1da537e27f
[10/31/2023-17:58:25] [V] [TRT] Tactic: 0xfbcd5a1da537e27f Time: 2.91099
[10/31/2023-17:58:25] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0x5e921a04a7b0e583
[10/31/2023-17:58:26] [V] [TRT] Tactic: 0x5e921a04a7b0e583 Time: 4.40289
[10/31/2023-17:58:26] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x98744862ba458d9a
[10/31/2023-17:58:26] [V] [TRT] Tactic: 0x98744862ba458d9a Time: 2.75068
[10/31/2023-17:58:26] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xa085d13614221ec4
[10/31/2023-17:58:26] [V] [TRT] Tactic: 0xa085d13614221ec4 Time: 3.5565
[10/31/2023-17:58:26] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xd8559956fb70c5c0
[10/31/2023-17:58:26] [V] [TRT] Tactic: 0xd8559956fb70c5c0 Time: 4.98554
[10/31/2023-17:58:26] [V] [TRT] Fastest Tactic: 0xa197727afc8dee05 Time: 2.42746
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa197727afc8dee05
[10/31/2023-17:58:26] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:26] [V] [TRT] *************** Autotuning format combination: Int8(4274688,8349,121,1) -> Int8(2137344,8349,121,1) ***************
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv3.conv.weight + /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv3/conv/Conv + PWN(/neck/detect1/conv3/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv3.conv.weight + /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv3/conv/Conv + PWN(/neck/detect1/conv3/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] *************** Autotuning format combination: Int8(1068672,8349:4,121,1) -> Int8(534336,8349:4,121,1) ***************
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv3.conv.weight + /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv3/conv/Conv + PWN(/neck/detect1/conv3/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:26] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv3.conv.weight + /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv3/conv/Conv + PWN(/neck/detect1/conv3/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:26] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv3.conv.weight + /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv3/conv/Conv + PWN(/neck/detect1/conv3/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv3.conv.weight + /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv3/conv/Conv + PWN(/neck/detect1/conv3/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] *************** Autotuning format combination: Int8(1068672,8349:4,121,1) -> Int8(66792,8349:32,121,1) ***************
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv3.conv.weight + /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv3/conv/Conv + PWN(/neck/detect1/conv3/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv3.conv.weight + /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv3/conv/Conv + PWN(/neck/detect1/conv3/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] *************** Autotuning format combination: Int8(133584,8349:32,121,1) -> Int8(66792,8349:32,121,1) ***************
[10/31/2023-17:58:26] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:26] [V] [TRT] *************** Autotuning format combination: Int8(2137344,8349,121,1) -> Int8(4274688,8349,121,1) ***************
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv4.conv.weight + /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv4/conv/Conv + PWN(/neck/detect1/conv4/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv4.conv.weight + /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv4/conv/Conv + PWN(/neck/detect1/conv4/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] *************** Autotuning format combination: Int8(534336,8349:4,121,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv4.conv.weight + /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv4/conv/Conv + PWN(/neck/detect1/conv4/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:26] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv4.conv.weight + /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv4/conv/Conv + PWN(/neck/detect1/conv4/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:26] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv4.conv.weight + /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv4/conv/Conv + PWN(/neck/detect1/conv4/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv4.conv.weight + /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv4/conv/Conv + PWN(/neck/detect1/conv4/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] *************** Autotuning format combination: Int8(534336,8349:4,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv4.conv.weight + /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv4/conv/Conv + PWN(/neck/detect1/conv4/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv4.conv.weight + /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv4/conv/Conv + PWN(/neck/detect1/conv4/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] *************** Autotuning format combination: Int8(66792,8349:32,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:58:26] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:26] [V] [TRT] *************** Autotuning format combination: Int8(4274688,8349,121,1) -> Int8(2137344,8349,121,1) ***************
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv5.conv.weight + /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv5/conv/Conv + PWN(/neck/detect1/conv5/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv5.conv.weight + /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv5/conv/Conv + PWN(/neck/detect1/conv5/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] *************** Autotuning format combination: Int8(1068672,8349:4,121,1) -> Int8(534336,8349:4,121,1) ***************
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv5.conv.weight + /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv5/conv/Conv + PWN(/neck/detect1/conv5/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:26] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv5.conv.weight + /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv5/conv/Conv + PWN(/neck/detect1/conv5/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:26] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv5.conv.weight + /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv5/conv/Conv + PWN(/neck/detect1/conv5/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv5.conv.weight + /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv5/conv/Conv + PWN(/neck/detect1/conv5/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] *************** Autotuning format combination: Int8(1068672,8349:4,121,1) -> Int8(66792,8349:32,121,1) ***************
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv5.conv.weight + /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv5/conv/Conv + PWN(/neck/detect1/conv5/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.detect1.conv5.conv.weight + /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv5/conv/Conv + PWN(/neck/detect1/conv5/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] *************** Autotuning format combination: Int8(133584,8349:32,121,1) -> Int8(66792,8349:32,121,1) ***************
[10/31/2023-17:58:26] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:26] [V] [TRT] *************** Autotuning format combination: Int8(2137344,8349,121,1) -> Int8(1068672,8349,121,1) ***************
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] *************** Autotuning format combination: Int8(534336,8349:4,121,1) -> Int8(267168,8349:4,121,1) ***************
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:26] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:26] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] *************** Autotuning format combination: Int8(534336,8349:4,121,1) -> Int8(33396,8349:32,121,1) ***************
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:26] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] *************** Autotuning format combination: Int8(66792,8349:32,121,1) -> Int8(33396,8349:32,121,1) ***************
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) (CudaGroupConvolution)
[10/31/2023-17:58:26] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:26] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:26] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:26] [V] [TRT] --------------- Timing Runner: neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:26] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0xeb1602207b0c3d49
[10/31/2023-17:58:26] [V] [TRT] Tactic: 0xeb1602207b0c3d49 Time: 0.296306
[10/31/2023-17:58:26] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_linkable Tactic: 0x1f54ac6f2468a25d
[10/31/2023-17:58:26] [V] [TRT] Tactic: 0x1f54ac6f2468a25d Time: 0.231323
[10/31/2023-17:58:26] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x25bf7139bbd93bc8
[10/31/2023-17:58:26] [V] [TRT] Tactic: 0x25bf7139bbd93bc8 Time: 0.186779
[10/31/2023-17:58:26] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x6d066fa755f280f9
[10/31/2023-17:58:26] [V] [TRT] Tactic: 0x6d066fa755f280f9 Time: 0.200215
[10/31/2023-17:58:26] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x3f084feb7c0e1257
[10/31/2023-17:58:26] [V] [TRT] Tactic: 0x3f084feb7c0e1257 Time: 0.372704
[10/31/2023-17:58:26] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x4df7e35b8de312d7
[10/31/2023-17:58:26] [V] [TRT] Tactic: 0x4df7e35b8de312d7 Time: 0.212795
[10/31/2023-17:58:26] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xb960a0bfa8850b75
[10/31/2023-17:58:26] [V] [TRT] Tactic: 0xb960a0bfa8850b75 Time: 0.246729
[10/31/2023-17:58:26] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x5e3d76022ffbdce2
[10/31/2023-17:58:26] [V] [TRT] Tactic: 0x5e3d76022ffbdce2 Time: 0.424379
[10/31/2023-17:58:26] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x3321b9c38b0ba08d
[10/31/2023-17:58:26] [V] [TRT] Tactic: 0x3321b9c38b0ba08d Time: 0.205207
[10/31/2023-17:58:26] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0x9b90c142f7ba1756
[10/31/2023-17:58:26] [V] [TRT] Tactic: 0x9b90c142f7ba1756 Time: 0.309787
[10/31/2023-17:58:26] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0xa40d21cb7ebb07bf
[10/31/2023-17:58:26] [V] [TRT] Tactic: 0xa40d21cb7ebb07bf Time: 0.251328
[10/31/2023-17:58:26] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x0779fa4ac8f9f2ee
[10/31/2023-17:58:26] [V] [TRT] Tactic: 0x0779fa4ac8f9f2ee Time: 0.201888
[10/31/2023-17:58:26] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x14db55f2724dda9f
[10/31/2023-17:58:26] [V] [TRT] Tactic: 0x14db55f2724dda9f Time: 0.380069
[10/31/2023-17:58:26] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0xd7c8759ddafac114
[10/31/2023-17:58:26] [V] [TRT] Tactic: 0xd7c8759ddafac114 Time: 0.280585
[10/31/2023-17:58:26] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x4bb804b3b091b301
[10/31/2023-17:58:26] [V] [TRT] Tactic: 0x4bb804b3b091b301 Time: 0.413669
[10/31/2023-17:58:26] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x2a8d3d5ae3647db4
[10/31/2023-17:58:26] [V] [TRT] Tactic: 0x2a8d3d5ae3647db4 Time: 0.345481
[10/31/2023-17:58:26] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0xc69ab4295ea0103e
[10/31/2023-17:58:26] [V] [TRT] Tactic: 0xc69ab4295ea0103e Time: 0.295794
[10/31/2023-17:58:26] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x892689d63e1f5cb0
[10/31/2023-17:58:26] [V] [TRT] Tactic: 0x892689d63e1f5cb0 Time: 0.228119
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x9ca4b256db721f9d
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x9ca4b256db721f9d Time: 0.387936
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x4862d9a1cd4ec818
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x4862d9a1cd4ec818 Time: 0.263401
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_linkable Tactic: 0x58613478e5ec50a9
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x58613478e5ec50a9 Time: 0.235886
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_linkable Tactic: 0x826da74f523d0cce
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x826da74f523d0cce Time: 0.411648
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x459509ec55322f60
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x459509ec55322f60 Time: 0.229362
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x10c7e671ac664e67
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x10c7e671ac664e67 Time: 0.218336
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x54c7c858d168fe4a
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x54c7c858d168fe4a Time: 0.255566
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xace5d20e37ef6496
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xace5d20e37ef6496 Time: 0.277198
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xcdd0ebe7641aaa23
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xcdd0ebe7641aaa23 Time: 0.367186
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0xa767fa987290f29d
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xa767fa987290f29d Time: 0.225563
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0xd851a0a9f1b84453
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xd851a0a9f1b84453 Time: 0.214441
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0x8ac200f673e0c67c
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x8ac200f673e0c67c Time: 0.232192
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x4a79db7911ff56ec
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x4a79db7911ff56ec Time: 0.18885
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0xf6d111860199beba
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xf6d111860199beba Time: 0.207781
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_linkable Tactic: 0x7a6e06a0138186b0
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x7a6e06a0138186b0 Time: 0.404521
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xe8c4479eeddac419
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xe8c4479eeddac419 Time: 0.203858
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x15f0bde1f6cbbf61
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x15f0bde1f6cbbf61 Time: 0.182743
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x4ce54017654ce216
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x4ce54017654ce216 Time: 0.189426
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x16bf68703cf6ee6c
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x16bf68703cf6ee6c Time: 0.202528
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x98744862ba458d9a
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x98744862ba458d9a Time: 0.213303
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x5e22a4cd8f7a7f29
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x5e22a4cd8f7a7f29 Time: 0.226574
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x8aed0c9812ff1e34
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x8aed0c9812ff1e34 Time: 0.186907
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xd8559956fb70c5c0
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xd8559956fb70c5c0 Time: 0.321033
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x6eb8c2607f51ccff
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x6eb8c2607f51ccff Time: 0.273033
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0xb3e028693e71e990
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xb3e028693e71e990 Time: 0.27232
[10/31/2023-17:58:27] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x90af0952ccc337b8
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x90af0952ccc337b8 Time: 0.30917
[10/31/2023-17:58:27] [V] [TRT] Fastest Tactic: 0x15f0bde1f6cbbf61 Time: 0.182743
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:27] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x15f0bde1f6cbbf61
[10/31/2023-17:58:27] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:27] [V] [TRT] *************** Autotuning format combination: Int8(2137344,8349,121,1) -> Int8(4274688,8349,121,1) ***************
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect1.conv6.conv.weight + /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv6/conv/Conv + PWN(/neck/detect1/conv6/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:27] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect1.conv6.conv.weight + /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv6/conv/Conv + PWN(/neck/detect1/conv6/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:27] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] *************** Autotuning format combination: Int8(534336,8349:4,121,1) -> Int8(1068672,8349:4,121,1) ***************
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect1.conv6.conv.weight + /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv6/conv/Conv + PWN(/neck/detect1/conv6/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:27] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect1.conv6.conv.weight + /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv6/conv/Conv + PWN(/neck/detect1/conv6/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:27] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect1.conv6.conv.weight + /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv6/conv/Conv + PWN(/neck/detect1/conv6/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:27] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect1.conv6.conv.weight + /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv6/conv/Conv + PWN(/neck/detect1/conv6/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:27] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] *************** Autotuning format combination: Int8(534336,8349:4,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect1.conv6.conv.weight + /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv6/conv/Conv + PWN(/neck/detect1/conv6/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:27] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect1.conv6.conv.weight + /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv6/conv/Conv + PWN(/neck/detect1/conv6/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:27] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] *************** Autotuning format combination: Int8(66792,8349:32,121,1) -> Int8(133584,8349:32,121,1) ***************
[10/31/2023-17:58:27] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:27] [V] [TRT] *************** Autotuning format combination: Int8(1068672,8349,121,1) -> Int8(4274688,33396,242,1) ***************
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: /neck/Resize (Resize)
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x0000000000000000 Time: 2.45742
[10/31/2023-17:58:27] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 2.45742
[10/31/2023-17:58:27] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000000
[10/31/2023-17:58:27] [V] [TRT] *************** Autotuning format combination: Int8(33396,8349:32,121,1) -> Int8(133584,33396:32,242,1) ***************
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: /neck/Resize (Resize)
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x0000000000000003 Time: 0.565559
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.294427
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x0000000000000006 Time: 0.265097
[10/31/2023-17:58:27] [V] [TRT] Fastest Tactic: 0x0000000000000006 Time: 0.265097
[10/31/2023-17:58:27] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000006
[10/31/2023-17:58:27] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:27] [V] [TRT] *************** Autotuning format combination: Int8(1068672,8349:4,121,1) -> Float(525987,8349,121,1) ***************
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv (CudaDepthwiseConvolution)
[10/31/2023-17:58:27] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv (CaskConvolution)
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 0xb29abdd00304c881
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xb29abdd00304c881 Time: 0.537838
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: 0xc25454de2efcccdc
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xc25454de2efcccdc Time: 0.527666
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xfe80445f7bb61a99 Time: 0.581559
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 0x14ef32afe5695907
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x14ef32afe5695907 Time: 1.00762
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xff6944b17d5b2e32 Time: 0.508503
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 0xec391424db39a74f
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xec391424db39a74f Time: 0.563022
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x69c4e2ca38eadce2 Time: 0.532663
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 0x4d9d0d03129bceb1
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x4d9d0d03129bceb1 Time: 0.535141
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 0xe01ccc14da28fa6f
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xe01ccc14da28fa6f Time: 0.590921
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xa4ae2d82115c3e83 Time: 0.540361
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: 0x1db6e8cf1382fbe0
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x1db6e8cf1382fbe0 Time: 0.99835
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 0x2cf4271504d30e29
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x2cf4271504d30e29 Time: 1.0062
[10/31/2023-17:58:27] [V] [TRT] Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.508503
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv (CaskFlattenConvolution)
[10/31/2023-17:58:27] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[10/31/2023-17:58:27] [V] [TRT] *************** Autotuning format combination: Int8(133584,8349:32,121,1) -> Float(525987,8349,121,1) ***************
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv (CaskConvolution)
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xf04572b287451f42 Time: 0.261065
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x733ba2a91a48d431 Time: 0.235008
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x5e4f6d7c83746fd6 Time: 0.297742
[10/31/2023-17:58:27] [V] [TRT] Fastest Tactic: 0x733ba2a91a48d431 Time: 0.235008
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv (CaskFlattenConvolution)
[10/31/2023-17:58:27] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431
[10/31/2023-17:58:27] [V] [TRT] *************** Autotuning format combination: Int8(133584,8349:32,121,1) -> Float(16698,8349:32,121,1) ***************
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv (CaskConvolution)
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x33a5c6dd086942c1 Time: 0.216055
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xe742f4598442d2f1 Time: 0.210697
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xdf7e1bd6a496d667 Time: 0.226208
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xbeb5d91e1874a437 Time: 0.218971
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x3f2b14dec582741e Time: 0.265147
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xcf64a2ae51bf6b36 Time: 0.239602
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x65fbe45b4cb1d8a5 Time: 0.235415
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xb86b920539eb9c79 Time: 0.232352
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xa71946688cad8664 Time: 0.196274
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xdc1f355deb032b87 Time: 0.292754
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x6986b0c6a136276e Time: 0.226757
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x91930a570b557437 Time: 0.398491
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x7720f198395e7d3d Time: 0.17493
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x271b998fe31732ef Time: 0.184082
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x2468d082d0ff7c9a Time: 0.255419
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xcd229658c16b33cd Time: 0.280663
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x1e55f8b415964e81 Time: 0.179483
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x8141573686849b61 Time: 0.273531
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x60b880e28fee7a0c Time: 0.411497
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x6e9c17a33c93d9b0 Time: 0.244677
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xb5f710b331ba8377 Time: 0.289495
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x960e9baa2a6cad5b Time: 0.190423
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x748f926574b7bdc4 Time: 0.444091
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x5f1a472d416ff35e Time: 0.232229
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x5bd8221bd57baf93 Time: 0.260777
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x9003f5f7ff9b1aec Time: 0.272448
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x991db9fd58152c33 Time: 0.200503
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x84942841d92b0552 Time: 0.184503
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x6d377e4222886190 Time: 0.257381
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x445983715412fbda Time: 0.320256
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x9ec201b34455146e Time: 0.207982
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0x844ea9c00f711f19 Time: 0.253248
[10/31/2023-17:58:27] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec
[10/31/2023-17:58:27] [V] [TRT] Tactic: 0xa2f15b0a75b7dcec Time: 0.289632
[10/31/2023-17:58:27] [V] [TRT] Fastest Tactic: 0x7720f198395e7d3d Time: 0.17493
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv (CaskFlattenConvolution)
[10/31/2023-17:58:27] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7720f198395e7d3d
[10/31/2023-17:58:27] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:27] [V] [TRT] *************** Autotuning format combination: Int8(12824064,33396,242,1) -> Int8(4274688,33396,242,1) ***************
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:27] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:27] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] *************** Autotuning format combination: Int8(3206016,33396:4,242,1) -> Int8(1068672,33396:4,242,1) ***************
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:27] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:27] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:27] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:27] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] *************** Autotuning format combination: Int8(3206016,33396:4,242,1) -> Int8(133584,33396:32,242,1) ***************
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:27] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:27] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] *************** Autotuning format combination: Int8(400752,33396:32,242,1) -> Int8(133584,33396:32,242,1) ***************
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) (CudaGroupConvolution)
[10/31/2023-17:58:27] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:27] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:27] [V] [TRT] --------------- Timing Runner: neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:27] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:28] [V] [TRT] --------------- Timing Runner: neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0xeb1602207b0c3d49
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0xeb1602207b0c3d49 Time: 1.35856
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_linkable Tactic: 0x1f54ac6f2468a25d
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x1f54ac6f2468a25d Time: 0.915195
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x25bf7139bbd93bc8
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x25bf7139bbd93bc8 Time: 0.759552
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x6d066fa755f280f9
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x6d066fa755f280f9 Time: 0.780128
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x3f084feb7c0e1257
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x3f084feb7c0e1257 Time: 1.69381
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x4df7e35b8de312d7
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x4df7e35b8de312d7 Time: 0.935077
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xb960a0bfa8850b75
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0xb960a0bfa8850b75 Time: 1.07946
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x5e3d76022ffbdce2
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x5e3d76022ffbdce2 Time: 1.98006
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x3321b9c38b0ba08d
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x3321b9c38b0ba08d Time: 0.880288
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0x9b90c142f7ba1756
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x9b90c142f7ba1756 Time: 1.26917
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0xa40d21cb7ebb07bf
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0xa40d21cb7ebb07bf Time: 1.08597
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x0779fa4ac8f9f2ee
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x0779fa4ac8f9f2ee Time: 0.880786
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x14db55f2724dda9f
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x14db55f2724dda9f Time: 1.65255
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0xd7c8759ddafac114
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0xd7c8759ddafac114 Time: 1.14864
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x4bb804b3b091b301
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x4bb804b3b091b301 Time: 1.88585
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x2a8d3d5ae3647db4
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x2a8d3d5ae3647db4 Time: 1.53912
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0xc69ab4295ea0103e
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0xc69ab4295ea0103e Time: 1.25734
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x892689d63e1f5cb0
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x892689d63e1f5cb0 Time: 1.00343
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x9ca4b256db721f9d
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x9ca4b256db721f9d Time: 1.67369
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x4862d9a1cd4ec818
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x4862d9a1cd4ec818 Time: 1.22529
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_linkable Tactic: 0x58613478e5ec50a9
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x58613478e5ec50a9 Time: 0.938272
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_linkable Tactic: 0x826da74f523d0cce
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x826da74f523d0cce Time: 1.78121
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x459509ec55322f60
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x459509ec55322f60 Time: 0.975063
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x10c7e671ac664e67
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x10c7e671ac664e67 Time: 0.884594
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x54c7c858d168fe4a
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x54c7c858d168fe4a Time: 1.12506
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xace5d20e37ef6496
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0xace5d20e37ef6496 Time: 1.26161
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xcdd0ebe7641aaa23
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0xcdd0ebe7641aaa23 Time: 1.71907
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0xa767fa987290f29d
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0xa767fa987290f29d Time: 0.898194
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0xd851a0a9f1b84453
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0xd851a0a9f1b84453 Time: 0.86923
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0x8ac200f673e0c67c
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x8ac200f673e0c67c Time: 0.937929
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x4a79db7911ff56ec
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x4a79db7911ff56ec Time: 0.798743
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0xf6d111860199beba
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0xf6d111860199beba Time: 0.819058
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_linkable Tactic: 0x7a6e06a0138186b0
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x7a6e06a0138186b0 Time: 1.74207
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xe8c4479eeddac419
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0xe8c4479eeddac419 Time: 0.795785
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x15f0bde1f6cbbf61
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x15f0bde1f6cbbf61 Time: 0.711488
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x4ce54017654ce216
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x4ce54017654ce216 Time: 0.744494
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x16bf68703cf6ee6c
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x16bf68703cf6ee6c Time: 0.830976
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x98744862ba458d9a
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x98744862ba458d9a Time: 0.893179
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x5e22a4cd8f7a7f29
[10/31/2023-17:58:28] [V] [TRT] Tactic: 0x5e22a4cd8f7a7f29 Time: 0.959392
[10/31/2023-17:58:28] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x8aed0c9812ff1e34
[10/31/2023-17:58:29] [V] [TRT] Tactic: 0x8aed0c9812ff1e34 Time: 0.777339
[10/31/2023-17:58:29] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xd8559956fb70c5c0
[10/31/2023-17:58:29] [V] [TRT] Tactic: 0xd8559956fb70c5c0 Time: 1.46205
[10/31/2023-17:58:29] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x6eb8c2607f51ccff
[10/31/2023-17:58:29] [V] [TRT] Tactic: 0x6eb8c2607f51ccff Time: 1.26459
[10/31/2023-17:58:29] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0xb3e028693e71e990
[10/31/2023-17:58:29] [V] [TRT] Tactic: 0xb3e028693e71e990 Time: 1.16418
[10/31/2023-17:58:29] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x90af0952ccc337b8
[10/31/2023-17:58:29] [V] [TRT] Tactic: 0x90af0952ccc337b8 Time: 1.40262
[10/31/2023-17:58:29] [V] [TRT] Fastest Tactic: 0x15f0bde1f6cbbf61 Time: 0.711488
[10/31/2023-17:58:29] [V] [TRT] --------------- Timing Runner: neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:29] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:29] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x15f0bde1f6cbbf61
[10/31/2023-17:58:29] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:29] [V] [TRT] *************** Autotuning format combination: Int8(4274688,33396,242,1) -> Int8(8549376,33396,242,1) ***************
[10/31/2023-17:58:29] [V] [TRT] --------------- Timing Runner: neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:29] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:29] [V] [TRT] --------------- Timing Runner: neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:29] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:29] [V] [TRT] *************** Autotuning format combination: Int8(1068672,33396:4,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:58:29] [V] [TRT] --------------- Timing Runner: neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:29] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:29] [V] [TRT] --------------- Timing Runner: neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:29] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:29] [V] [TRT] --------------- Timing Runner: neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:29] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:29] [V] [TRT] --------------- Timing Runner: neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:29] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:29] [V] [TRT] *************** Autotuning format combination: Int8(1068672,33396:4,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:58:29] [V] [TRT] --------------- Timing Runner: neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:29] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:29] [V] [TRT] --------------- Timing Runner: neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:29] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:29] [V] [TRT] *************** Autotuning format combination: Int8(133584,33396:32,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:58:29] [V] [TRT] --------------- Timing Runner: neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) (CudaGroupConvolution)
[10/31/2023-17:58:29] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:29] [V] [TRT] --------------- Timing Runner: neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:29] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:29] [V] [TRT] --------------- Timing Runner: neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:29] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:29] [V] [TRT] --------------- Timing Runner: neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:29] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0x7ddd3b3f55023bab
[10/31/2023-17:58:29] [V] [TRT] Tactic: 0x7ddd3b3f55023bab Time: 6.46378
[10/31/2023-17:58:29] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_linkable Tactic: 0x1f54ac6f2468a25d
[10/31/2023-17:58:29] [V] [TRT] Tactic: 0x1f54ac6f2468a25d Time: 3.20372
[10/31/2023-17:58:29] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0x4a15c8f5eb51fe8e
[10/31/2023-17:58:29] [V] [TRT] Tactic: 0x4a15c8f5eb51fe8e Time: 3.12552
[10/31/2023-17:58:29] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x3f084feb7c0e1257
[10/31/2023-17:58:29] [V] [TRT] Tactic: 0x3f084feb7c0e1257 Time: 6.51227
[10/31/2023-17:58:29] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xb960a0bfa8850b75
[10/31/2023-17:58:29] [V] [TRT] Tactic: 0xb960a0bfa8850b75 Time: 4.09248
[10/31/2023-17:58:29] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xa70be914883e5aff
[10/31/2023-17:58:29] [V] [TRT] Tactic: 0xa70be914883e5aff Time: 3.22691
[10/31/2023-17:58:29] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x5e3d76022ffbdce2
[10/31/2023-17:58:29] [V] [TRT] Tactic: 0x5e3d76022ffbdce2 Time: 7.73534
[10/31/2023-17:58:29] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xa197727afc8dee05
[10/31/2023-17:58:29] [V] [TRT] Tactic: 0xa197727afc8dee05 Time: 2.74318
[10/31/2023-17:58:29] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0x9b90c142f7ba1756
[10/31/2023-17:58:29] [V] [TRT] Tactic: 0x9b90c142f7ba1756 Time: 4.43645
[10/31/2023-17:58:29] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0xd7c8759ddafac114
[10/31/2023-17:58:30] [V] [TRT] Tactic: 0xd7c8759ddafac114 Time: 4.08375
[10/31/2023-17:58:30] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0x71d6803b42b3138e
[10/31/2023-17:58:30] [V] [TRT] Tactic: 0x71d6803b42b3138e Time: 3.07589
[10/31/2023-17:58:30] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0x83caf00de690c0ec
[10/31/2023-17:58:30] [V] [TRT] Tactic: 0x83caf00de690c0ec Time: 5.4636
[10/31/2023-17:58:30] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x4bb804b3b091b301
[10/31/2023-17:58:30] [V] [TRT] Tactic: 0x4bb804b3b091b301 Time: 6.59314
[10/31/2023-17:58:30] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x2a8d3d5ae3647db4
[10/31/2023-17:58:30] [V] [TRT] Tactic: 0x2a8d3d5ae3647db4 Time: 5.08317
[10/31/2023-17:58:30] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0xc69ab4295ea0103e
[10/31/2023-17:58:30] [V] [TRT] Tactic: 0xc69ab4295ea0103e Time: 5.14113
[10/31/2023-17:58:30] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x892689d63e1f5cb0
[10/31/2023-17:58:30] [V] [TRT] Tactic: 0x892689d63e1f5cb0 Time: 3.83396
[10/31/2023-17:58:30] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_linkable Tactic: 0x58613478e5ec50a9
[10/31/2023-17:58:30] [V] [TRT] Tactic: 0x58613478e5ec50a9 Time: 3.35572
[10/31/2023-17:58:30] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_linkable Tactic: 0x826da74f523d0cce
[10/31/2023-17:58:30] [V] [TRT] Tactic: 0x826da74f523d0cce Time: 3.29089
[10/31/2023-17:58:30] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x459509ec55322f60
[10/31/2023-17:58:30] [V] [TRT] Tactic: 0x459509ec55322f60 Time: 3.45158
[10/31/2023-17:58:30] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x10c7e671ac664e67
[10/31/2023-17:58:30] [V] [TRT] Tactic: 0x10c7e671ac664e67 Time: 3.24759
[10/31/2023-17:58:30] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x54c7c858d168fe4a
[10/31/2023-17:58:30] [V] [TRT] Tactic: 0x54c7c858d168fe4a Time: 4.2344
[10/31/2023-17:58:30] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xace5d20e37ef6496
[10/31/2023-17:58:30] [V] [TRT] Tactic: 0xace5d20e37ef6496 Time: 4.83075
[10/31/2023-17:58:30] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xcdd0ebe7641aaa23
[10/31/2023-17:58:30] [V] [TRT] Tactic: 0xcdd0ebe7641aaa23 Time: 6.56065
[10/31/2023-17:58:30] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0x8ac200f673e0c67c
[10/31/2023-17:58:30] [V] [TRT] Tactic: 0x8ac200f673e0c67c Time: 3.0099
[10/31/2023-17:58:30] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0x1ba323eb9858b2a9
[10/31/2023-17:58:30] [V] [TRT] Tactic: 0x1ba323eb9858b2a9 Time: 2.99478
[10/31/2023-17:58:30] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xb35096a016bb733a
[10/31/2023-17:58:30] [V] [TRT] Tactic: 0xb35096a016bb733a Time: 3.99573
[10/31/2023-17:58:30] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_linkable Tactic: 0x7a6e06a0138186b0
[10/31/2023-17:58:30] [V] [TRT] Tactic: 0x7a6e06a0138186b0 Time: 3.13531
[10/31/2023-17:58:30] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xe8c4479eeddac419
[10/31/2023-17:58:30] [V] [TRT] Tactic: 0xe8c4479eeddac419 Time: 2.79422
[10/31/2023-17:58:30] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xfbcd5a1da537e27f
[10/31/2023-17:58:30] [V] [TRT] Tactic: 0xfbcd5a1da537e27f Time: 3.33542
[10/31/2023-17:58:30] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0x5e921a04a7b0e583
[10/31/2023-17:58:30] [V] [TRT] Tactic: 0x5e921a04a7b0e583 Time: 4.95917
[10/31/2023-17:58:30] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x98744862ba458d9a
[10/31/2023-17:58:30] [V] [TRT] Tactic: 0x98744862ba458d9a Time: 3.22763
[10/31/2023-17:58:30] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xa085d13614221ec4
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0xa085d13614221ec4 Time: 4.00019
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xd8559956fb70c5c0
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0xd8559956fb70c5c0 Time: 5.69904
[10/31/2023-17:58:31] [V] [TRT] Fastest Tactic: 0xa197727afc8dee05 Time: 2.74318
[10/31/2023-17:58:31] [V] [TRT] --------------- Timing Runner: neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:31] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:31] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa197727afc8dee05
[10/31/2023-17:58:31] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:31] [V] [TRT] *************** Autotuning format combination: Int8(8549376,33396,242,1) -> Int8(4274688,33396,242,1) ***************
[10/31/2023-17:58:31] [V] [TRT] --------------- Timing Runner: neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:31] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:31] [V] [TRT] --------------- Timing Runner: neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:31] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:31] [V] [TRT] *************** Autotuning format combination: Int8(2137344,33396:4,242,1) -> Int8(1068672,33396:4,242,1) ***************
[10/31/2023-17:58:31] [V] [TRT] --------------- Timing Runner: neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:31] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:31] [V] [TRT] --------------- Timing Runner: neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:31] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:31] [V] [TRT] --------------- Timing Runner: neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:31] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:31] [V] [TRT] --------------- Timing Runner: neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:31] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:31] [V] [TRT] *************** Autotuning format combination: Int8(2137344,33396:4,242,1) -> Int8(133584,33396:32,242,1) ***************
[10/31/2023-17:58:31] [V] [TRT] --------------- Timing Runner: neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:31] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:31] [V] [TRT] --------------- Timing Runner: neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:31] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:31] [V] [TRT] *************** Autotuning format combination: Int8(267168,33396:32,242,1) -> Int8(133584,33396:32,242,1) ***************
[10/31/2023-17:58:31] [V] [TRT] --------------- Timing Runner: neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) (CudaGroupConvolution)
[10/31/2023-17:58:31] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:31] [V] [TRT] --------------- Timing Runner: neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:31] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:31] [V] [TRT] --------------- Timing Runner: neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:31] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:31] [V] [TRT] --------------- Timing Runner: neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0xeb1602207b0c3d49
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0xeb1602207b0c3d49 Time: 1.11509
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_linkable Tactic: 0x1f54ac6f2468a25d
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0x1f54ac6f2468a25d Time: 0.80805
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x25bf7139bbd93bc8
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0x25bf7139bbd93bc8 Time: 0.669426
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x6d066fa755f280f9
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0x6d066fa755f280f9 Time: 0.705344
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x3f084feb7c0e1257
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0x3f084feb7c0e1257 Time: 1.37797
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x4df7e35b8de312d7
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0x4df7e35b8de312d7 Time: 0.787159
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xb960a0bfa8850b75
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0xb960a0bfa8850b75 Time: 0.905221
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x5e3d76022ffbdce2
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0x5e3d76022ffbdce2 Time: 1.61072
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x3321b9c38b0ba08d
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0x3321b9c38b0ba08d Time: 0.744667
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0x9b90c142f7ba1756
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0x9b90c142f7ba1756 Time: 1.11439
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0xa40d21cb7ebb07bf
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0xa40d21cb7ebb07bf Time: 0.941952
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x0779fa4ac8f9f2ee
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0x0779fa4ac8f9f2ee Time: 0.73749
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x14db55f2724dda9f
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0x14db55f2724dda9f Time: 1.43048
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0xd7c8759ddafac114
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0xd7c8759ddafac114 Time: 1.00984
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x4bb804b3b091b301
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0x4bb804b3b091b301 Time: 1.52571
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x2a8d3d5ae3647db4
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0x2a8d3d5ae3647db4 Time: 1.24957
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0xc69ab4295ea0103e
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0xc69ab4295ea0103e Time: 1.04997
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x892689d63e1f5cb0
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0x892689d63e1f5cb0 Time: 0.828841
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x9ca4b256db721f9d
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0x9ca4b256db721f9d Time: 1.44732
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x4862d9a1cd4ec818
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0x4862d9a1cd4ec818 Time: 0.989586
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_linkable Tactic: 0x58613478e5ec50a9
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0x58613478e5ec50a9 Time: 0.810459
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_linkable Tactic: 0x826da74f523d0cce
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0x826da74f523d0cce Time: 1.53545
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x459509ec55322f60
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0x459509ec55322f60 Time: 0.830569
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x10c7e671ac664e67
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0x10c7e671ac664e67 Time: 0.771858
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x54c7c858d168fe4a
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0x54c7c858d168fe4a Time: 0.929189
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xace5d20e37ef6496
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0xace5d20e37ef6496 Time: 1.03781
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xcdd0ebe7641aaa23
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0xcdd0ebe7641aaa23 Time: 1.41689
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0xa767fa987290f29d
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0xa767fa987290f29d Time: 0.785838
[10/31/2023-17:58:31] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0xd851a0a9f1b84453
[10/31/2023-17:58:31] [V] [TRT] Tactic: 0xd851a0a9f1b84453 Time: 0.755909
[10/31/2023-17:58:32] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0x8ac200f673e0c67c
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x8ac200f673e0c67c Time: 0.83264
[10/31/2023-17:58:32] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x4a79db7911ff56ec
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x4a79db7911ff56ec Time: 0.685573
[10/31/2023-17:58:32] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0xf6d111860199beba
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0xf6d111860199beba Time: 0.742656
[10/31/2023-17:58:32] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_linkable Tactic: 0x7a6e06a0138186b0
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x7a6e06a0138186b0 Time: 1.50747
[10/31/2023-17:58:32] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xe8c4479eeddac419
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0xe8c4479eeddac419 Time: 0.71179
[10/31/2023-17:58:32] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x15f0bde1f6cbbf61
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x15f0bde1f6cbbf61 Time: 0.631003
[10/31/2023-17:58:32] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x4ce54017654ce216
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x4ce54017654ce216 Time: 0.658491
[10/31/2023-17:58:32] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x16bf68703cf6ee6c
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x16bf68703cf6ee6c Time: 0.735008
[10/31/2023-17:58:32] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x98744862ba458d9a
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x98744862ba458d9a Time: 0.772873
[10/31/2023-17:58:32] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x5e22a4cd8f7a7f29
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x5e22a4cd8f7a7f29 Time: 0.825723
[10/31/2023-17:58:32] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x8aed0c9812ff1e34
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x8aed0c9812ff1e34 Time: 0.682331
[10/31/2023-17:58:32] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xd8559956fb70c5c0
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0xd8559956fb70c5c0 Time: 1.22962
[10/31/2023-17:58:32] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x6eb8c2607f51ccff
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x6eb8c2607f51ccff Time: 1.03632
[10/31/2023-17:58:32] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0xb3e028693e71e990
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0xb3e028693e71e990 Time: 1.01548
[10/31/2023-17:58:32] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x90af0952ccc337b8
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x90af0952ccc337b8 Time: 1.17604
[10/31/2023-17:58:32] [V] [TRT] Fastest Tactic: 0x15f0bde1f6cbbf61 Time: 0.631003
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x15f0bde1f6cbbf61
[10/31/2023-17:58:32] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:32] [V] [TRT] *************** Autotuning format combination: Int8(4274688,33396,242,1) -> Int8(8549376,33396,242,1) ***************
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.detect2.conv4.conv.weight + /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv4/conv/Conv + PWN(/neck/detect2/conv4/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:32] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.detect2.conv4.conv.weight + /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv4/conv/Conv + PWN(/neck/detect2/conv4/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] *************** Autotuning format combination: Int8(1068672,33396:4,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.detect2.conv4.conv.weight + /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv4/conv/Conv + PWN(/neck/detect2/conv4/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:32] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.detect2.conv4.conv.weight + /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv4/conv/Conv + PWN(/neck/detect2/conv4/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:32] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.detect2.conv4.conv.weight + /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv4/conv/Conv + PWN(/neck/detect2/conv4/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:32] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.detect2.conv4.conv.weight + /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv4/conv/Conv + PWN(/neck/detect2/conv4/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] *************** Autotuning format combination: Int8(1068672,33396:4,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.detect2.conv4.conv.weight + /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv4/conv/Conv + PWN(/neck/detect2/conv4/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:32] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.detect2.conv4.conv.weight + /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv4/conv/Conv + PWN(/neck/detect2/conv4/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] *************** Autotuning format combination: Int8(133584,33396:32,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:58:32] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:32] [V] [TRT] *************** Autotuning format combination: Int8(8549376,33396,242,1) -> Int8(4274688,33396,242,1) ***************
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.detect2.conv5.conv.weight + /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv5/conv/Conv + PWN(/neck/detect2/conv5/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:32] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.detect2.conv5.conv.weight + /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv5/conv/Conv + PWN(/neck/detect2/conv5/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] *************** Autotuning format combination: Int8(2137344,33396:4,242,1) -> Int8(1068672,33396:4,242,1) ***************
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.detect2.conv5.conv.weight + /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv5/conv/Conv + PWN(/neck/detect2/conv5/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:32] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.detect2.conv5.conv.weight + /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv5/conv/Conv + PWN(/neck/detect2/conv5/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:32] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.detect2.conv5.conv.weight + /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv5/conv/Conv + PWN(/neck/detect2/conv5/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:32] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.detect2.conv5.conv.weight + /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv5/conv/Conv + PWN(/neck/detect2/conv5/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] *************** Autotuning format combination: Int8(2137344,33396:4,242,1) -> Int8(133584,33396:32,242,1) ***************
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.detect2.conv5.conv.weight + /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv5/conv/Conv + PWN(/neck/detect2/conv5/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:32] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.detect2.conv5.conv.weight + /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv5/conv/Conv + PWN(/neck/detect2/conv5/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] *************** Autotuning format combination: Int8(267168,33396:32,242,1) -> Int8(133584,33396:32,242,1) ***************
[10/31/2023-17:58:32] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:32] [V] [TRT] *************** Autotuning format combination: Int8(4274688,33396,242,1) -> Int8(2137344,33396,242,1) ***************
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:32] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] *************** Autotuning format combination: Int8(1068672,33396:4,242,1) -> Int8(534336,33396:4,242,1) ***************
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:32] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:32] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:32] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] *************** Autotuning format combination: Int8(1068672,33396:4,242,1) -> Int8(66792,33396:32,242,1) ***************
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:32] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:32] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] *************** Autotuning format combination: Int8(133584,33396:32,242,1) -> Int8(66792,33396:32,242,1) ***************
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) (CudaGroupConvolution)
[10/31/2023-17:58:32] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:32] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:32] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:32] [V] [TRT] --------------- Timing Runner: neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:32] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0xeb1602207b0c3d49
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0xeb1602207b0c3d49 Time: 0.450784
[10/31/2023-17:58:32] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_linkable Tactic: 0x1f54ac6f2468a25d
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x1f54ac6f2468a25d Time: 0.676969
[10/31/2023-17:58:32] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x25bf7139bbd93bc8
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x25bf7139bbd93bc8 Time: 0.288023
[10/31/2023-17:58:32] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x6d066fa755f280f9
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x6d066fa755f280f9 Time: 0.323753
[10/31/2023-17:58:32] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x3f084feb7c0e1257
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x3f084feb7c0e1257 Time: 0.518949
[10/31/2023-17:58:32] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x4df7e35b8de312d7
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x4df7e35b8de312d7 Time: 0.339063
[10/31/2023-17:58:32] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xb960a0bfa8850b75
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0xb960a0bfa8850b75 Time: 0.381294
[10/31/2023-17:58:32] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x5e3d76022ffbdce2
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x5e3d76022ffbdce2 Time: 0.606053
[10/31/2023-17:58:32] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x3321b9c38b0ba08d
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x3321b9c38b0ba08d Time: 0.309385
[10/31/2023-17:58:32] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0x9b90c142f7ba1756
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x9b90c142f7ba1756 Time: 0.497842
[10/31/2023-17:58:32] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0xa40d21cb7ebb07bf
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0xa40d21cb7ebb07bf Time: 0.391136
[10/31/2023-17:58:32] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x0779fa4ac8f9f2ee
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x0779fa4ac8f9f2ee Time: 0.312818
[10/31/2023-17:58:32] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x14db55f2724dda9f
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x14db55f2724dda9f Time: 1.16339
[10/31/2023-17:58:32] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0xd7c8759ddafac114
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0xd7c8759ddafac114 Time: 0.405879
[10/31/2023-17:58:32] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x4bb804b3b091b301
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x4bb804b3b091b301 Time: 0.584475
[10/31/2023-17:58:32] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x2a8d3d5ae3647db4
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x2a8d3d5ae3647db4 Time: 0.483346
[10/31/2023-17:58:32] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0xc69ab4295ea0103e
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0xc69ab4295ea0103e Time: 0.414226
[10/31/2023-17:58:32] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x892689d63e1f5cb0
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x892689d63e1f5cb0 Time: 0.628439
[10/31/2023-17:58:32] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x9ca4b256db721f9d
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x9ca4b256db721f9d Time: 1.20168
[10/31/2023-17:58:32] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x4862d9a1cd4ec818
[10/31/2023-17:58:32] [V] [TRT] Tactic: 0x4862d9a1cd4ec818 Time: 0.401038
[10/31/2023-17:58:32] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_linkable Tactic: 0x58613478e5ec50a9
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x58613478e5ec50a9 Time: 0.677312
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_linkable Tactic: 0x826da74f523d0cce
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x826da74f523d0cce Time: 1.26787
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x459509ec55322f60
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x459509ec55322f60 Time: 0.342958
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x10c7e671ac664e67
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x10c7e671ac664e67 Time: 0.621632
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x54c7c858d168fe4a
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x54c7c858d168fe4a Time: 0.357234
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xace5d20e37ef6496
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xace5d20e37ef6496 Time: 0.397513
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xcdd0ebe7641aaa23
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xcdd0ebe7641aaa23 Time: 0.537518
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0xa767fa987290f29d
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xa767fa987290f29d Time: 0.655826
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0xd851a0a9f1b84453
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xd851a0a9f1b84453 Time: 0.62731
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0x8ac200f673e0c67c
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x8ac200f673e0c67c Time: 0.369467
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x4a79db7911ff56ec
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x4a79db7911ff56ec Time: 0.567671
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0xf6d111860199beba
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xf6d111860199beba Time: 0.339168
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_linkable Tactic: 0x7a6e06a0138186b0
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x7a6e06a0138186b0 Time: 1.25197
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xe8c4479eeddac419
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xe8c4479eeddac419 Time: 0.584494
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x15f0bde1f6cbbf61
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x15f0bde1f6cbbf61 Time: 0.534318
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x4ce54017654ce216
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x4ce54017654ce216 Time: 0.558103
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x16bf68703cf6ee6c
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x16bf68703cf6ee6c Time: 0.316718
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x98744862ba458d9a
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x98744862ba458d9a Time: 0.60965
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x5e22a4cd8f7a7f29
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x5e22a4cd8f7a7f29 Time: 0.353678
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x8aed0c9812ff1e34
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x8aed0c9812ff1e34 Time: 0.545573
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xd8559956fb70c5c0
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xd8559956fb70c5c0 Time: 0.503333
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x6eb8c2607f51ccff
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x6eb8c2607f51ccff Time: 0.426587
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0xb3e028693e71e990
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xb3e028693e71e990 Time: 0.430437
[10/31/2023-17:58:33] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x90af0952ccc337b8
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x90af0952ccc337b8 Time: 0.486455
[10/31/2023-17:58:33] [V] [TRT] Fastest Tactic: 0x25bf7139bbd93bc8 Time: 0.288023
[10/31/2023-17:58:33] [V] [TRT] --------------- Timing Runner: neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:33] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x25bf7139bbd93bc8
[10/31/2023-17:58:33] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:33] [V] [TRT] *************** Autotuning format combination: Int8(4274688,33396,242,1) -> Int8(8549376,33396,242,1) ***************
[10/31/2023-17:58:33] [V] [TRT] --------------- Timing Runner: neck.detect2.conv6.conv.weight + /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv6/conv/Conv + PWN(/neck/detect2/conv6/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:33] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:33] [V] [TRT] --------------- Timing Runner: neck.detect2.conv6.conv.weight + /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv6/conv/Conv + PWN(/neck/detect2/conv6/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:33] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:33] [V] [TRT] *************** Autotuning format combination: Int8(1068672,33396:4,242,1) -> Int8(2137344,33396:4,242,1) ***************
[10/31/2023-17:58:33] [V] [TRT] --------------- Timing Runner: neck.detect2.conv6.conv.weight + /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv6/conv/Conv + PWN(/neck/detect2/conv6/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:33] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:33] [V] [TRT] --------------- Timing Runner: neck.detect2.conv6.conv.weight + /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv6/conv/Conv + PWN(/neck/detect2/conv6/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:33] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:33] [V] [TRT] --------------- Timing Runner: neck.detect2.conv6.conv.weight + /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv6/conv/Conv + PWN(/neck/detect2/conv6/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:33] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:33] [V] [TRT] --------------- Timing Runner: neck.detect2.conv6.conv.weight + /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv6/conv/Conv + PWN(/neck/detect2/conv6/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:33] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:33] [V] [TRT] *************** Autotuning format combination: Int8(1068672,33396:4,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:58:33] [V] [TRT] --------------- Timing Runner: neck.detect2.conv6.conv.weight + /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv6/conv/Conv + PWN(/neck/detect2/conv6/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:33] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:33] [V] [TRT] --------------- Timing Runner: neck.detect2.conv6.conv.weight + /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv6/conv/Conv + PWN(/neck/detect2/conv6/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:33] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:33] [V] [TRT] *************** Autotuning format combination: Int8(133584,33396:32,242,1) -> Int8(267168,33396:32,242,1) ***************
[10/31/2023-17:58:33] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:33] [V] [TRT] *************** Autotuning format combination: Int8(2137344,33396,242,1) -> Int8(8549376,133584,484,1) ***************
[10/31/2023-17:58:33] [V] [TRT] --------------- Timing Runner: /neck/Resize_1 (Resize)
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x0000000000000000 Time: 4.90469
[10/31/2023-17:58:33] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 4.90469
[10/31/2023-17:58:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000000
[10/31/2023-17:58:33] [V] [TRT] *************** Autotuning format combination: Int8(66792,33396:32,242,1) -> Int8(267168,133584:32,484,1) ***************
[10/31/2023-17:58:33] [V] [TRT] --------------- Timing Runner: /neck/Resize_1 (Resize)
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x0000000000000003 Time: 1.09326
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x0000000000000005 Time: 0.571945
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x0000000000000006 Time: 0.51515
[10/31/2023-17:58:33] [V] [TRT] Fastest Tactic: 0x0000000000000006 Time: 0.51515
[10/31/2023-17:58:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000006
[10/31/2023-17:58:33] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:33] [V] [TRT] *************** Autotuning format combination: Int8(2137344,33396:4,242,1) -> Float(2103948,33396,242,1) ***************
[10/31/2023-17:58:33] [V] [TRT] --------------- Timing Runner: neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv (CudaDepthwiseConvolution)
[10/31/2023-17:58:33] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:33] [V] [TRT] --------------- Timing Runner: neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv (CaskConvolution)
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 0xb29abdd00304c881
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xb29abdd00304c881 Time: 1.06281
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: 0xc25454de2efcccdc
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xc25454de2efcccdc Time: 1.08319
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xfe80445f7bb61a99 Time: 1.1688
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 0x14ef32afe5695907
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x14ef32afe5695907 Time: 2.0108
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xff6944b17d5b2e32 Time: 0.989422
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 0xec391424db39a74f
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xec391424db39a74f Time: 1.19004
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x69c4e2ca38eadce2 Time: 1.05425
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 0x4d9d0d03129bceb1
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x4d9d0d03129bceb1 Time: 1.1128
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 0xe01ccc14da28fa6f
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xe01ccc14da28fa6f Time: 1.19904
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xa4ae2d82115c3e83 Time: 1.08483
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: 0x1db6e8cf1382fbe0
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x1db6e8cf1382fbe0 Time: 1.99776
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 0x2cf4271504d30e29
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x2cf4271504d30e29 Time: 2.02292
[10/31/2023-17:58:33] [V] [TRT] Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.989422
[10/31/2023-17:58:33] [V] [TRT] --------------- Timing Runner: neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv (CaskFlattenConvolution)
[10/31/2023-17:58:33] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[10/31/2023-17:58:33] [V] [TRT] *************** Autotuning format combination: Int8(267168,33396:32,242,1) -> Float(2103948,33396,242,1) ***************
[10/31/2023-17:58:33] [V] [TRT] --------------- Timing Runner: neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv (CaskConvolution)
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xf04572b287451f42 Time: 0.706139
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x733ba2a91a48d431 Time: 0.59493
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x5e4f6d7c83746fd6 Time: 0.80117
[10/31/2023-17:58:33] [V] [TRT] Fastest Tactic: 0x733ba2a91a48d431 Time: 0.59493
[10/31/2023-17:58:33] [V] [TRT] --------------- Timing Runner: neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv (CaskFlattenConvolution)
[10/31/2023-17:58:33] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:33] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431
[10/31/2023-17:58:33] [V] [TRT] *************** Autotuning format combination: Int8(267168,33396:32,242,1) -> Float(66792,33396:32,242,1) ***************
[10/31/2023-17:58:33] [V] [TRT] --------------- Timing Runner: neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv (CaskConvolution)
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x33a5c6dd086942c1 Time: 0.569769
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xe742f4598442d2f1 Time: 0.577202
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xdf7e1bd6a496d667 Time: 0.553303
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xbeb5d91e1874a437 Time: 0.529554
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x3f2b14dec582741e Time: 0.706505
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xcf64a2ae51bf6b36 Time: 0.633701
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x65fbe45b4cb1d8a5 Time: 0.611703
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xb86b920539eb9c79 Time: 0.577152
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xa71946688cad8664 Time: 0.533394
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xdc1f355deb032b87 Time: 0.736005
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x6986b0c6a136276e Time: 0.667963
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x91930a570b557437 Time: 1.03525
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x7720f198395e7d3d Time: 0.466921
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x271b998fe31732ef Time: 0.498875
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x2468d082d0ff7c9a Time: 0.664567
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xcd229658c16b33cd Time: 0.796334
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x1e55f8b415964e81 Time: 0.474418
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x8141573686849b61 Time: 0.756613
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x60b880e28fee7a0c Time: 1.09351
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x6e9c17a33c93d9b0 Time: 0.650985
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0xb5f710b331ba8377 Time: 0.826437
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x960e9baa2a6cad5b Time: 0.519456
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x748f926574b7bdc4 Time: 1.22533
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x5f1a472d416ff35e Time: 0.599648
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x5bd8221bd57baf93 Time: 0.693838
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x9003f5f7ff9b1aec Time: 0.760366
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33
[10/31/2023-17:58:33] [V] [TRT] Tactic: 0x991db9fd58152c33 Time: 0.545189
[10/31/2023-17:58:33] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0x84942841d92b0552 Time: 0.487589
[10/31/2023-17:58:34] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0x6d377e4222886190 Time: 0.674135
[10/31/2023-17:58:34] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0x445983715412fbda Time: 0.828681
[10/31/2023-17:58:34] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0x9ec201b34455146e Time: 0.539854
[10/31/2023-17:58:34] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0x844ea9c00f711f19 Time: 0.6864
[10/31/2023-17:58:34] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0xa2f15b0a75b7dcec Time: 0.760663
[10/31/2023-17:58:34] [V] [TRT] Fastest Tactic: 0x7720f198395e7d3d Time: 0.466921
[10/31/2023-17:58:34] [V] [TRT] --------------- Timing Runner: neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv (CaskFlattenConvolution)
[10/31/2023-17:58:34] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:34] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7720f198395e7d3d
[10/31/2023-17:58:34] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:34] [V] [TRT] *************** Autotuning format combination: Int8(25648128,133584,484,1) -> Int8(8549376,133584,484,1) ***************
[10/31/2023-17:58:34] [V] [TRT] --------------- Timing Runner: neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:34] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:34] [V] [TRT] --------------- Timing Runner: neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:34] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:34] [V] [TRT] *************** Autotuning format combination: Int8(6412032,133584:4,484,1) -> Int8(2137344,133584:4,484,1) ***************
[10/31/2023-17:58:34] [V] [TRT] --------------- Timing Runner: neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:34] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:34] [V] [TRT] --------------- Timing Runner: neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:34] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:34] [V] [TRT] --------------- Timing Runner: neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:34] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:34] [V] [TRT] --------------- Timing Runner: neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:34] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:34] [V] [TRT] *************** Autotuning format combination: Int8(6412032,133584:4,484,1) -> Int8(267168,133584:32,484,1) ***************
[10/31/2023-17:58:34] [V] [TRT] --------------- Timing Runner: neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:34] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:34] [V] [TRT] --------------- Timing Runner: neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:34] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:34] [V] [TRT] *************** Autotuning format combination: Int8(801504,133584:32,484,1) -> Int8(267168,133584:32,484,1) ***************
[10/31/2023-17:58:34] [V] [TRT] --------------- Timing Runner: neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) (CudaGroupConvolution)
[10/31/2023-17:58:34] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:34] [V] [TRT] --------------- Timing Runner: neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:34] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:34] [V] [TRT] --------------- Timing Runner: neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:34] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:34] [V] [TRT] --------------- Timing Runner: neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:34] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0xeb1602207b0c3d49
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0xeb1602207b0c3d49 Time: 1.95603
[10/31/2023-17:58:34] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_linkable Tactic: 0x1f54ac6f2468a25d
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0x1f54ac6f2468a25d Time: 2.8654
[10/31/2023-17:58:34] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x25bf7139bbd93bc8
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0x25bf7139bbd93bc8 Time: 1.21132
[10/31/2023-17:58:34] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x6d066fa755f280f9
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0x6d066fa755f280f9 Time: 1.31365
[10/31/2023-17:58:34] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x3f084feb7c0e1257
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0x3f084feb7c0e1257 Time: 2.40595
[10/31/2023-17:58:34] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x4df7e35b8de312d7
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0x4df7e35b8de312d7 Time: 1.43554
[10/31/2023-17:58:34] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xb960a0bfa8850b75
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0xb960a0bfa8850b75 Time: 1.61306
[10/31/2023-17:58:34] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x5e3d76022ffbdce2
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0x5e3d76022ffbdce2 Time: 2.7799
[10/31/2023-17:58:34] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x3321b9c38b0ba08d
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0x3321b9c38b0ba08d Time: 1.3274
[10/31/2023-17:58:34] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0x9b90c142f7ba1756
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0x9b90c142f7ba1756 Time: 2.09945
[10/31/2023-17:58:34] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0xa40d21cb7ebb07bf
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0xa40d21cb7ebb07bf Time: 1.66024
[10/31/2023-17:58:34] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x0779fa4ac8f9f2ee
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0x0779fa4ac8f9f2ee Time: 1.33258
[10/31/2023-17:58:34] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x14db55f2724dda9f
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0x14db55f2724dda9f Time: 5.04265
[10/31/2023-17:58:34] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0xd7c8759ddafac114
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0xd7c8759ddafac114 Time: 1.78703
[10/31/2023-17:58:34] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x4bb804b3b091b301
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0x4bb804b3b091b301 Time: 2.66745
[10/31/2023-17:58:34] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x2a8d3d5ae3647db4
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0x2a8d3d5ae3647db4 Time: 2.25129
[10/31/2023-17:58:34] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0xc69ab4295ea0103e
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0xc69ab4295ea0103e Time: 1.87984
[10/31/2023-17:58:34] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x892689d63e1f5cb0
[10/31/2023-17:58:34] [V] [TRT] Tactic: 0x892689d63e1f5cb0 Time: 2.81457
[10/31/2023-17:58:34] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x9ca4b256db721f9d
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0x9ca4b256db721f9d Time: 5.19442
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x4862d9a1cd4ec818
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0x4862d9a1cd4ec818 Time: 1.73422
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_linkable Tactic: 0x58613478e5ec50a9
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0x58613478e5ec50a9 Time: 2.84933
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_linkable Tactic: 0x826da74f523d0cce
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0x826da74f523d0cce Time: 5.54017
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x459509ec55322f60
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0x459509ec55322f60 Time: 1.46784
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x10c7e671ac664e67
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0x10c7e671ac664e67 Time: 2.67111
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x54c7c858d168fe4a
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0x54c7c858d168fe4a Time: 1.58212
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xace5d20e37ef6496
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0xace5d20e37ef6496 Time: 1.79418
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xcdd0ebe7641aaa23
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0xcdd0ebe7641aaa23 Time: 2.45682
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0xa767fa987290f29d
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0xa767fa987290f29d Time: 2.80873
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0xd851a0a9f1b84453
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0xd851a0a9f1b84453 Time: 2.69003
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0x8ac200f673e0c67c
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0x8ac200f673e0c67c Time: 1.55401
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x4a79db7911ff56ec
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0x4a79db7911ff56ec Time: 2.46804
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0xf6d111860199beba
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0xf6d111860199beba Time: 1.3821
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_linkable Tactic: 0x7a6e06a0138186b0
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0x7a6e06a0138186b0 Time: 5.42024
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xe8c4479eeddac419
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0xe8c4479eeddac419 Time: 2.52465
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x15f0bde1f6cbbf61
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0x15f0bde1f6cbbf61 Time: 2.2417
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x4ce54017654ce216
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0x4ce54017654ce216 Time: 2.37013
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x16bf68703cf6ee6c
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0x16bf68703cf6ee6c Time: 1.33681
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x98744862ba458d9a
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0x98744862ba458d9a Time: 2.6998
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x5e22a4cd8f7a7f29
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0x5e22a4cd8f7a7f29 Time: 1.48939
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x8aed0c9812ff1e34
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0x8aed0c9812ff1e34 Time: 2.3866
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xd8559956fb70c5c0
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0xd8559956fb70c5c0 Time: 2.19209
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x6eb8c2607f51ccff
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0x6eb8c2607f51ccff Time: 1.82854
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0xb3e028693e71e990
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0xb3e028693e71e990 Time: 1.8286
[10/31/2023-17:58:35] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x90af0952ccc337b8
[10/31/2023-17:58:35] [V] [TRT] Tactic: 0x90af0952ccc337b8 Time: 2.10023
[10/31/2023-17:58:35] [V] [TRT] Fastest Tactic: 0x25bf7139bbd93bc8 Time: 1.21132
[10/31/2023-17:58:35] [V] [TRT] --------------- Timing Runner: neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:35] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:35] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x25bf7139bbd93bc8
[10/31/2023-17:58:35] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:35] [V] [TRT] *************** Autotuning format combination: Int8(8549376,133584,484,1) -> Int8(17098752,133584,484,1) ***************
[10/31/2023-17:58:35] [V] [TRT] --------------- Timing Runner: neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:35] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:35] [V] [TRT] --------------- Timing Runner: neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:35] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:35] [V] [TRT] *************** Autotuning format combination: Int8(2137344,133584:4,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:58:35] [V] [TRT] --------------- Timing Runner: neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:35] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:35] [V] [TRT] --------------- Timing Runner: neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:35] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:35] [V] [TRT] --------------- Timing Runner: neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:35] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:35] [V] [TRT] --------------- Timing Runner: neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:35] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:35] [V] [TRT] *************** Autotuning format combination: Int8(2137344,133584:4,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:58:35] [V] [TRT] --------------- Timing Runner: neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:35] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:35] [V] [TRT] --------------- Timing Runner: neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:35] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:35] [V] [TRT] *************** Autotuning format combination: Int8(267168,133584:32,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:58:35] [V] [TRT] --------------- Timing Runner: neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) (CudaGroupConvolution)
[10/31/2023-17:58:35] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:35] [V] [TRT] --------------- Timing Runner: neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:35] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:35] [V] [TRT] --------------- Timing Runner: neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:35] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:36] [V] [TRT] --------------- Timing Runner: neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:36] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0x7ddd3b3f55023bab
[10/31/2023-17:58:36] [V] [TRT] Tactic: 0x7ddd3b3f55023bab Time: 7.96805
[10/31/2023-17:58:36] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_linkable Tactic: 0x1f54ac6f2468a25d
[10/31/2023-17:58:36] [V] [TRT] Tactic: 0x1f54ac6f2468a25d Time: 4.27393
[10/31/2023-17:58:36] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0x4a15c8f5eb51fe8e
[10/31/2023-17:58:36] [V] [TRT] Tactic: 0x4a15c8f5eb51fe8e Time: 4.1169
[10/31/2023-17:58:36] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x3f084feb7c0e1257
[10/31/2023-17:58:36] [V] [TRT] Tactic: 0x3f084feb7c0e1257 Time: 8.31953
[10/31/2023-17:58:36] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xb960a0bfa8850b75
[10/31/2023-17:58:36] [V] [TRT] Tactic: 0xb960a0bfa8850b75 Time: 5.37485
[10/31/2023-17:58:36] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xa70be914883e5aff
[10/31/2023-17:58:36] [V] [TRT] Tactic: 0xa70be914883e5aff Time: 3.99616
[10/31/2023-17:58:36] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x5e3d76022ffbdce2
[10/31/2023-17:58:36] [V] [TRT] Tactic: 0x5e3d76022ffbdce2 Time: 9.99296
[10/31/2023-17:58:36] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xa197727afc8dee05
[10/31/2023-17:58:36] [V] [TRT] Tactic: 0xa197727afc8dee05 Time: 3.5263
[10/31/2023-17:58:36] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0x9b90c142f7ba1756
[10/31/2023-17:58:36] [V] [TRT] Tactic: 0x9b90c142f7ba1756 Time: 5.92137
[10/31/2023-17:58:36] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0xd7c8759ddafac114
[10/31/2023-17:58:36] [V] [TRT] Tactic: 0xd7c8759ddafac114 Time: 5.08955
[10/31/2023-17:58:36] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0x71d6803b42b3138e
[10/31/2023-17:58:36] [V] [TRT] Tactic: 0x71d6803b42b3138e Time: 7.93547
[10/31/2023-17:58:36] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0x83caf00de690c0ec
[10/31/2023-17:58:37] [V] [TRT] Tactic: 0x83caf00de690c0ec Time: 6.77025
[10/31/2023-17:58:37] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x4bb804b3b091b301
[10/31/2023-17:58:37] [V] [TRT] Tactic: 0x4bb804b3b091b301 Time: 8.39649
[10/31/2023-17:58:37] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x2a8d3d5ae3647db4
[10/31/2023-17:58:37] [V] [TRT] Tactic: 0x2a8d3d5ae3647db4 Time: 6.68505
[10/31/2023-17:58:37] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0xc69ab4295ea0103e
[10/31/2023-17:58:37] [V] [TRT] Tactic: 0xc69ab4295ea0103e Time: 6.14209
[10/31/2023-17:58:37] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x892689d63e1f5cb0
[10/31/2023-17:58:37] [V] [TRT] Tactic: 0x892689d63e1f5cb0 Time: 4.88577
[10/31/2023-17:58:37] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_linkable Tactic: 0x58613478e5ec50a9
[10/31/2023-17:58:37] [V] [TRT] Tactic: 0x58613478e5ec50a9 Time: 4.3537
[10/31/2023-17:58:37] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_linkable Tactic: 0x826da74f523d0cce
[10/31/2023-17:58:37] [V] [TRT] Tactic: 0x826da74f523d0cce Time: 8.47142
[10/31/2023-17:58:37] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x459509ec55322f60
[10/31/2023-17:58:37] [V] [TRT] Tactic: 0x459509ec55322f60 Time: 4.54341
[10/31/2023-17:58:37] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x10c7e671ac664e67
[10/31/2023-17:58:37] [V] [TRT] Tactic: 0x10c7e671ac664e67 Time: 4.11274
[10/31/2023-17:58:37] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x54c7c858d168fe4a
[10/31/2023-17:58:37] [V] [TRT] Tactic: 0x54c7c858d168fe4a Time: 5.46777
[10/31/2023-17:58:37] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xace5d20e37ef6496
[10/31/2023-17:58:37] [V] [TRT] Tactic: 0xace5d20e37ef6496 Time: 6.33952
[10/31/2023-17:58:37] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xcdd0ebe7641aaa23
[10/31/2023-17:58:37] [V] [TRT] Tactic: 0xcdd0ebe7641aaa23 Time: 8.58204
[10/31/2023-17:58:37] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0x8ac200f673e0c67c
[10/31/2023-17:58:37] [V] [TRT] Tactic: 0x8ac200f673e0c67c Time: 4.08196
[10/31/2023-17:58:37] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0x1ba323eb9858b2a9
[10/31/2023-17:58:37] [V] [TRT] Tactic: 0x1ba323eb9858b2a9 Time: 3.91415
[10/31/2023-17:58:37] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xb35096a016bb733a
[10/31/2023-17:58:37] [V] [TRT] Tactic: 0xb35096a016bb733a Time: 4.80043
[10/31/2023-17:58:37] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_linkable Tactic: 0x7a6e06a0138186b0
[10/31/2023-17:58:37] [V] [TRT] Tactic: 0x7a6e06a0138186b0 Time: 8.19285
[10/31/2023-17:58:37] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xe8c4479eeddac419
[10/31/2023-17:58:37] [V] [TRT] Tactic: 0xe8c4479eeddac419 Time: 3.64429
[10/31/2023-17:58:37] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xfbcd5a1da537e27f
[10/31/2023-17:58:38] [V] [TRT] Tactic: 0xfbcd5a1da537e27f Time: 4.1887
[10/31/2023-17:58:38] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0x5e921a04a7b0e583
[10/31/2023-17:58:38] [V] [TRT] Tactic: 0x5e921a04a7b0e583 Time: 6.50476
[10/31/2023-17:58:38] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x98744862ba458d9a
[10/31/2023-17:58:38] [V] [TRT] Tactic: 0x98744862ba458d9a Time: 4.22048
[10/31/2023-17:58:38] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xa085d13614221ec4
[10/31/2023-17:58:38] [V] [TRT] Tactic: 0xa085d13614221ec4 Time: 4.97216
[10/31/2023-17:58:38] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xd8559956fb70c5c0
[10/31/2023-17:58:38] [V] [TRT] Tactic: 0xd8559956fb70c5c0 Time: 7.46059
[10/31/2023-17:58:38] [V] [TRT] Fastest Tactic: 0xa197727afc8dee05 Time: 3.5263
[10/31/2023-17:58:38] [V] [TRT] --------------- Timing Runner: neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:38] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:38] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa197727afc8dee05
[10/31/2023-17:58:38] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:38] [V] [TRT] *************** Autotuning format combination: Int8(17098752,133584,484,1) -> Int8(8549376,133584,484,1) ***************
[10/31/2023-17:58:38] [V] [TRT] --------------- Timing Runner: neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:38] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:38] [V] [TRT] --------------- Timing Runner: neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:38] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:38] [V] [TRT] *************** Autotuning format combination: Int8(4274688,133584:4,484,1) -> Int8(2137344,133584:4,484,1) ***************
[10/31/2023-17:58:38] [V] [TRT] --------------- Timing Runner: neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:38] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:38] [V] [TRT] --------------- Timing Runner: neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:38] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:38] [V] [TRT] --------------- Timing Runner: neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:38] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:38] [V] [TRT] --------------- Timing Runner: neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:38] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:38] [V] [TRT] *************** Autotuning format combination: Int8(4274688,133584:4,484,1) -> Int8(267168,133584:32,484,1) ***************
[10/31/2023-17:58:38] [V] [TRT] --------------- Timing Runner: neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:38] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:38] [V] [TRT] --------------- Timing Runner: neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:38] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:38] [V] [TRT] *************** Autotuning format combination: Int8(534336,133584:32,484,1) -> Int8(267168,133584:32,484,1) ***************
[10/31/2023-17:58:38] [V] [TRT] --------------- Timing Runner: neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) (CudaGroupConvolution)
[10/31/2023-17:58:38] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:38] [V] [TRT] --------------- Timing Runner: neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:38] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:38] [V] [TRT] --------------- Timing Runner: neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:38] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:38] [V] [TRT] --------------- Timing Runner: neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:38] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0xeb1602207b0c3d49
[10/31/2023-17:58:38] [V] [TRT] Tactic: 0xeb1602207b0c3d49 Time: 1.74108
[10/31/2023-17:58:38] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_linkable Tactic: 0x1f54ac6f2468a25d
[10/31/2023-17:58:38] [V] [TRT] Tactic: 0x1f54ac6f2468a25d Time: 2.60401
[10/31/2023-17:58:38] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x25bf7139bbd93bc8
[10/31/2023-17:58:38] [V] [TRT] Tactic: 0x25bf7139bbd93bc8 Time: 1.09211
[10/31/2023-17:58:38] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x6d066fa755f280f9
[10/31/2023-17:58:38] [V] [TRT] Tactic: 0x6d066fa755f280f9 Time: 1.20661
[10/31/2023-17:58:38] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x3f084feb7c0e1257
[10/31/2023-17:58:38] [V] [TRT] Tactic: 0x3f084feb7c0e1257 Time: 1.99792
[10/31/2023-17:58:38] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x4df7e35b8de312d7
[10/31/2023-17:58:38] [V] [TRT] Tactic: 0x4df7e35b8de312d7 Time: 1.29454
[10/31/2023-17:58:38] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xb960a0bfa8850b75
[10/31/2023-17:58:38] [V] [TRT] Tactic: 0xb960a0bfa8850b75 Time: 1.46001
[10/31/2023-17:58:38] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x5e3d76022ffbdce2
[10/31/2023-17:58:38] [V] [TRT] Tactic: 0x5e3d76022ffbdce2 Time: 2.34961
[10/31/2023-17:58:38] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x3321b9c38b0ba08d
[10/31/2023-17:58:38] [V] [TRT] Tactic: 0x3321b9c38b0ba08d Time: 1.17256
[10/31/2023-17:58:38] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0x9b90c142f7ba1756
[10/31/2023-17:58:38] [V] [TRT] Tactic: 0x9b90c142f7ba1756 Time: 1.91882
[10/31/2023-17:58:38] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0xa40d21cb7ebb07bf
[10/31/2023-17:58:38] [V] [TRT] Tactic: 0xa40d21cb7ebb07bf Time: 1.50154
[10/31/2023-17:58:38] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x0779fa4ac8f9f2ee
[10/31/2023-17:58:38] [V] [TRT] Tactic: 0x0779fa4ac8f9f2ee Time: 1.19639
[10/31/2023-17:58:38] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x14db55f2724dda9f
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0x14db55f2724dda9f Time: 4.57051
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0xd7c8759ddafac114
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0xd7c8759ddafac114 Time: 1.55506
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x4bb804b3b091b301
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0x4bb804b3b091b301 Time: 2.27467
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_linkable Tactic: 0x2a8d3d5ae3647db4
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0x2a8d3d5ae3647db4 Time: 1.88069
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0xc69ab4295ea0103e
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0xc69ab4295ea0103e Time: 1.57716
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x892689d63e1f5cb0
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0x892689d63e1f5cb0 Time: 2.44363
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x9ca4b256db721f9d
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0x9ca4b256db721f9d Time: 4.70631
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x4862d9a1cd4ec818
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0x4862d9a1cd4ec818 Time: 1.54673
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_linkable Tactic: 0x58613478e5ec50a9
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0x58613478e5ec50a9 Time: 2.6197
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_linkable Tactic: 0x826da74f523d0cce
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0x826da74f523d0cce Time: 4.97984
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x459509ec55322f60
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0x459509ec55322f60 Time: 1.31122
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x10c7e671ac664e67
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0x10c7e671ac664e67 Time: 2.39543
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x54c7c858d168fe4a
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0x54c7c858d168fe4a Time: 1.36268
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xace5d20e37ef6496
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0xace5d20e37ef6496 Time: 1.52609
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xcdd0ebe7641aaa23
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0xcdd0ebe7641aaa23 Time: 2.09994
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0xa767fa987290f29d
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0xa767fa987290f29d Time: 2.53512
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0xd851a0a9f1b84453
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0xd851a0a9f1b84453 Time: 2.4226
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_linkable Tactic: 0x8ac200f673e0c67c
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0x8ac200f673e0c67c Time: 1.39353
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x4a79db7911ff56ec
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0x4a79db7911ff56ec Time: 2.21247
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0xf6d111860199beba
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0xf6d111860199beba Time: 1.27134
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_linkable Tactic: 0x7a6e06a0138186b0
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0x7a6e06a0138186b0 Time: 4.91441
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xe8c4479eeddac419
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0xe8c4479eeddac419 Time: 2.26152
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x15f0bde1f6cbbf61
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0x15f0bde1f6cbbf61 Time: 2.04381
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x4ce54017654ce216
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0x4ce54017654ce216 Time: 2.15384
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x16bf68703cf6ee6c
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0x16bf68703cf6ee6c Time: 1.20982
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0x98744862ba458d9a
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0x98744862ba458d9a Time: 2.36983
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x5e22a4cd8f7a7f29
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0x5e22a4cd8f7a7f29 Time: 1.34663
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x8aed0c9812ff1e34
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0x8aed0c9812ff1e34 Time: 2.13174
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_linkable Tactic: 0xd8559956fb70c5c0
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0xd8559956fb70c5c0 Time: 1.96597
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x6eb8c2607f51ccff
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0x6eb8c2607f51ccff Time: 1.64259
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0xb3e028693e71e990
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0xb3e028693e71e990 Time: 1.65966
[10/31/2023-17:58:39] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_linkable Tactic: 0x90af0952ccc337b8
[10/31/2023-17:58:39] [V] [TRT] Tactic: 0x90af0952ccc337b8 Time: 1.88455
[10/31/2023-17:58:39] [V] [TRT] Fastest Tactic: 0x25bf7139bbd93bc8 Time: 1.09211
[10/31/2023-17:58:39] [V] [TRT] --------------- Timing Runner: neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:39] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x25bf7139bbd93bc8
[10/31/2023-17:58:39] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:39] [V] [TRT] *************** Autotuning format combination: Int8(8549376,133584,484,1) -> Int8(17098752,133584,484,1) ***************
[10/31/2023-17:58:39] [V] [TRT] --------------- Timing Runner: neck.detect3.conv4.conv.weight + /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv4/conv/Conv + PWN(/neck/detect3/conv4/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:39] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:39] [V] [TRT] --------------- Timing Runner: neck.detect3.conv4.conv.weight + /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv4/conv/Conv + PWN(/neck/detect3/conv4/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:39] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:39] [V] [TRT] *************** Autotuning format combination: Int8(2137344,133584:4,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:58:39] [V] [TRT] --------------- Timing Runner: neck.detect3.conv4.conv.weight + /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv4/conv/Conv + PWN(/neck/detect3/conv4/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:39] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:39] [V] [TRT] --------------- Timing Runner: neck.detect3.conv4.conv.weight + /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv4/conv/Conv + PWN(/neck/detect3/conv4/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:39] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv4.conv.weight + /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv4/conv/Conv + PWN(/neck/detect3/conv4/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:40] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv4.conv.weight + /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv4/conv/Conv + PWN(/neck/detect3/conv4/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] *************** Autotuning format combination: Int8(2137344,133584:4,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv4.conv.weight + /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv4/conv/Conv + PWN(/neck/detect3/conv4/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:40] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv4.conv.weight + /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv4/conv/Conv + PWN(/neck/detect3/conv4/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] *************** Autotuning format combination: Int8(267168,133584:32,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:58:40] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:40] [V] [TRT] *************** Autotuning format combination: Int8(17098752,133584,484,1) -> Int8(8549376,133584,484,1) ***************
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv5.conv.weight + /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv5/conv/Conv + PWN(/neck/detect3/conv5/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:40] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv5.conv.weight + /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv5/conv/Conv + PWN(/neck/detect3/conv5/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] *************** Autotuning format combination: Int8(4274688,133584:4,484,1) -> Int8(2137344,133584:4,484,1) ***************
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv5.conv.weight + /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv5/conv/Conv + PWN(/neck/detect3/conv5/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:40] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv5.conv.weight + /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv5/conv/Conv + PWN(/neck/detect3/conv5/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:40] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv5.conv.weight + /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv5/conv/Conv + PWN(/neck/detect3/conv5/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:40] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv5.conv.weight + /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv5/conv/Conv + PWN(/neck/detect3/conv5/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] *************** Autotuning format combination: Int8(4274688,133584:4,484,1) -> Int8(267168,133584:32,484,1) ***************
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv5.conv.weight + /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv5/conv/Conv + PWN(/neck/detect3/conv5/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:40] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv5.conv.weight + /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv5/conv/Conv + PWN(/neck/detect3/conv5/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] *************** Autotuning format combination: Int8(534336,133584:32,484,1) -> Int8(267168,133584:32,484,1) ***************
[10/31/2023-17:58:40] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:40] [V] [TRT] *************** Autotuning format combination: Int8(8549376,133584,484,1) -> Int8(17098752,133584,484,1) ***************
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv6.conv.weight + /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv6/conv/Conv + PWN(/neck/detect3/conv6/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:40] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv6.conv.weight + /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv6/conv/Conv + PWN(/neck/detect3/conv6/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] *************** Autotuning format combination: Int8(2137344,133584:4,484,1) -> Int8(4274688,133584:4,484,1) ***************
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv6.conv.weight + /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv6/conv/Conv + PWN(/neck/detect3/conv6/relu/LeakyRelu) (CudaDepthwiseConvolution)
[10/31/2023-17:58:40] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv6.conv.weight + /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv6/conv/Conv + PWN(/neck/detect3/conv6/relu/LeakyRelu) (FusedConvActConvolution)
[10/31/2023-17:58:40] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv6.conv.weight + /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv6/conv/Conv + PWN(/neck/detect3/conv6/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:40] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv6.conv.weight + /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv6/conv/Conv + PWN(/neck/detect3/conv6/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] *************** Autotuning format combination: Int8(2137344,133584:4,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv6.conv.weight + /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv6/conv/Conv + PWN(/neck/detect3/conv6/relu/LeakyRelu) (CaskConvolution)
[10/31/2023-17:58:40] [V] [TRT] CaskConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv6.conv.weight + /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv6/conv/Conv + PWN(/neck/detect3/conv6/relu/LeakyRelu) (CaskFlattenConvolution)
[10/31/2023-17:58:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] *************** Autotuning format combination: Int8(267168,133584:32,484,1) -> Int8(534336,133584:32,484,1) ***************
[10/31/2023-17:58:40] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:40] [V] [TRT] *************** Autotuning format combination: Int8(4274688,133584:4,484,1) -> Float(8415792,133584,484,1) ***************
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv (CudaDepthwiseConvolution)
[10/31/2023-17:58:40] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv (CaskConvolution)
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 0xb29abdd00304c881
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0xb29abdd00304c881 Time: 2.51438
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: 0xc25454de2efcccdc
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0xc25454de2efcccdc Time: 2.80101
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0xfe80445f7bb61a99 Time: 2.84431
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 0x14ef32afe5695907
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0x14ef32afe5695907 Time: 4.98264
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0xff6944b17d5b2e32 Time: 2.32239
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 0xec391424db39a74f
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0xec391424db39a74f Time: 3.1001
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0x69c4e2ca38eadce2 Time: 2.47596
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 0x4d9d0d03129bceb1
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0x4d9d0d03129bceb1 Time: 2.89453
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 0xe01ccc14da28fa6f
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0xe01ccc14da28fa6f Time: 2.9211
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0xa4ae2d82115c3e83 Time: 2.66425
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: 0x1db6e8cf1382fbe0
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0x1db6e8cf1382fbe0 Time: 4.92226
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 0x2cf4271504d30e29
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0x2cf4271504d30e29 Time: 4.99993
[10/31/2023-17:58:40] [V] [TRT] Fastest Tactic: 0xff6944b17d5b2e32 Time: 2.32239
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv (CaskFlattenConvolution)
[10/31/2023-17:58:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[10/31/2023-17:58:40] [V] [TRT] *************** Autotuning format combination: Int8(534336,133584:32,484,1) -> Float(8415792,133584,484,1) ***************
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv (CaskConvolution)
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0xf04572b287451f42 Time: 2.07052
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0x733ba2a91a48d431 Time: 1.81545
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0x5e4f6d7c83746fd6 Time: 2.47285
[10/31/2023-17:58:40] [V] [TRT] Fastest Tactic: 0x733ba2a91a48d431 Time: 1.81545
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv (CaskFlattenConvolution)
[10/31/2023-17:58:40] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431
[10/31/2023-17:58:40] [V] [TRT] *************** Autotuning format combination: Int8(534336,133584:32,484,1) -> Float(267168,133584:32,484,1) ***************
[10/31/2023-17:58:40] [V] [TRT] --------------- Timing Runner: neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv (CaskConvolution)
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0x33a5c6dd086942c1 Time: 1.76258
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0xe742f4598442d2f1 Time: 1.86415
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0xdf7e1bd6a496d667 Time: 1.87446
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0xbeb5d91e1874a437 Time: 1.76575
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0x3f2b14dec582741e Time: 1.99805
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0xcf64a2ae51bf6b36 Time: 1.93582
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0x65fbe45b4cb1d8a5 Time: 1.86241
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79
[10/31/2023-17:58:40] [V] [TRT] Tactic: 0xb86b920539eb9c79 Time: 1.91754
[10/31/2023-17:58:40] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0xa71946688cad8664 Time: 1.79105
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0xdc1f355deb032b87 Time: 2.03866
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0x6986b0c6a136276e Time: 2.37156
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0x91930a570b557437 Time: 3.19962
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0x7720f198395e7d3d Time: 1.60053
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0x271b998fe31732ef Time: 1.71019
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0x2468d082d0ff7c9a Time: 2.09079
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0xcd229658c16b33cd Time: 2.72013
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0x1e55f8b415964e81 Time: 1.63597
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0x8141573686849b61 Time: 2.31018
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0x60b880e28fee7a0c Time: 3.4624
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0x6e9c17a33c93d9b0 Time: 2.17509
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0xb5f710b331ba8377 Time: 2.78893
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0x960e9baa2a6cad5b Time: 1.76411
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0x748f926574b7bdc4 Time: 3.91704
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0x5f1a472d416ff35e Time: 1.87003
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0x5bd8221bd57baf93 Time: 2.08042
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0x9003f5f7ff9b1aec Time: 2.58446
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0x991db9fd58152c33 Time: 1.76164
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0x84942841d92b0552 Time: 1.73088
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0x6d377e4222886190 Time: 1.97357
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0x445983715412fbda Time: 2.36774
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0x9ec201b34455146e Time: 1.63688
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0x844ea9c00f711f19 Time: 2.2842
[10/31/2023-17:58:41] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec
[10/31/2023-17:58:41] [V] [TRT] Tactic: 0xa2f15b0a75b7dcec Time: 2.33364
[10/31/2023-17:58:41] [V] [TRT] Fastest Tactic: 0x7720f198395e7d3d Time: 1.60053
[10/31/2023-17:58:41] [V] [TRT] --------------- Timing Runner: neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv (CaskFlattenConvolution)
[10/31/2023-17:58:41] [V] [TRT] CaskFlattenConvolution has no valid tactics for this config, skipping
[10/31/2023-17:58:41] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7720f198395e7d3d
[10/31/2023-17:58:41] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:41] [V] [TRT] *************** Autotuning format combination: Float(8415792,133584,484,1), Float(2103948,33396,242,1), Float(525987,8349,121,1) -> Float(8415792,16,1), Int32(1), Float(3840,3840,1), Float(3840,1,1), Float(15360,4,1) ***************
[10/31/2023-17:58:41] [V] [TRT] --------------- Timing Runner: {ForeignNode[/head/Constant_2_output_0.../Unsqueeze_2]} (Myelin)
[10/31/2023-17:58:43] [V] [TRT]  (foreignNode) Set user's cuda kernel library
[10/31/2023-17:58:43] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes
[10/31/2023-17:58:43] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes
[10/31/2023-17:58:43] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes
[10/31/2023-17:58:43] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes
[10/31/2023-17:58:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 101.937
[10/31/2023-17:58:44] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 101.937
[10/31/2023-17:58:44] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Myelin Tactic: 0x0000000000000000
[10/31/2023-17:58:44] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:44] [V] [TRT] *************** Autotuning format combination: Float(15360,4,1), Float(3840,1,1), Int32(), Float(), Float() -> Int32(3,1), Int32() ***************
[10/31/2023-17:58:44] [V] [TRT] --------------- Timing Runner: /NonMaxSuppression_246 (NMS)
[10/31/2023-17:58:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.151429
[10/31/2023-17:58:44] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.151429
[10/31/2023-17:58:44] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: NMS Tactic: 0x0000000000000000
[10/31/2023-17:58:44] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:44] [V] [TRT] *************** Autotuning format combination: Int32() ->  ***************
[10/31/2023-17:58:44] [V] [TRT] --------------- Timing Runner: (Unnamed Layer* 718) [NMS]_1_output[DevicetoShapeHostCopy] (DeviceToShapeHost)
[10/31/2023-17:58:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0224967
[10/31/2023-17:58:44] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0224967
[10/31/2023-17:58:44] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: DeviceToShapeHost Tactic: 0x0000000000000000
[10/31/2023-17:58:44] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:44] [V] [TRT] *************** Autotuning format combination:  ->  ***************
[10/31/2023-17:58:44] [V] [TRT] --------------- Timing Runner: [trainStation2] (TrainStation)
[10/31/2023-17:58:44] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.000338373
[10/31/2023-17:58:44] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.000338373
[10/31/2023-17:58:44] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: TrainStation Tactic: 0x0000000000000000
[10/31/2023-17:58:44] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:44] [V] [TRT] *************** Autotuning format combination: Float(8415792,16,1), Int32(1), Float(3840,3840,1), Float(15360,4,1), Int32(3,1), Int32() -> Float(21,1) ***************
[10/31/2023-17:58:44] [V] [TRT] --------------- Timing Runner: {ForeignNode[/Split_2.../Concat_1]} (Myelin)
[10/31/2023-17:58:45] [V] [TRT]  (foreignNode) Set user's cuda kernel library
[10/31/2023-17:58:45] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes
[10/31/2023-17:58:45] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes
[10/31/2023-17:58:45] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes
[10/31/2023-17:58:45] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes
[10/31/2023-17:58:45] [V] [TRT]  (foreignNode) Set user's cuda kernel library
[10/31/2023-17:58:45] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes
[10/31/2023-17:58:45] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes
[10/31/2023-17:58:45] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes
[10/31/2023-17:58:45] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes
[10/31/2023-17:58:45] [V] [TRT]  (foreignNode) Set user's cuda kernel library
[10/31/2023-17:58:45] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes
[10/31/2023-17:58:45] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes
[10/31/2023-17:58:45] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes
[10/31/2023-17:58:45] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes
[10/31/2023-17:58:46] [V] [TRT]  (foreignNode) Set user's cuda kernel library
[10/31/2023-17:58:46] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes
[10/31/2023-17:58:46] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes
[10/31/2023-17:58:46] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes
[10/31/2023-17:58:46] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes
[10/31/2023-17:58:46] [V] [TRT]  (foreignNode) Set user's cuda kernel library
[10/31/2023-17:58:46] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes
[10/31/2023-17:58:46] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes
[10/31/2023-17:58:46] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes
[10/31/2023-17:58:46] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes
[10/31/2023-17:58:47] [V] [TRT]  (foreignNode) Set user's cuda kernel library
[10/31/2023-17:58:47] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes
[10/31/2023-17:58:47] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes
[10/31/2023-17:58:47] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes
[10/31/2023-17:58:47] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes
[10/31/2023-17:58:47] [V] [TRT]  (foreignNode) Set user's cuda kernel library
[10/31/2023-17:58:47] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes
[10/31/2023-17:58:47] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes
[10/31/2023-17:58:47] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes
[10/31/2023-17:58:47] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes
[10/31/2023-17:58:47] [V] [TRT]  (foreignNode) Set user's cuda kernel library
[10/31/2023-17:58:47] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes
[10/31/2023-17:58:47] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes
[10/31/2023-17:58:47] [V] [TRT]  (foreignNode) Pass fuse_conv_padding is currently skipped for dynamic shapes
[10/31/2023-17:58:47] [V] [TRT]  (foreignNode) Pass pad_conv_channel is currently skipped for dynamic shapes
[10/31/2023-17:58:48] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.0386286
[10/31/2023-17:58:48] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.0386286
[10/31/2023-17:58:48] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Myelin Tactic: 0x0000000000000000
[10/31/2023-17:58:48] [V] [TRT] =============== Computing costs for 
[10/31/2023-17:58:48] [V] [TRT] *************** Autotuning format combination:  ->  ***************
[10/31/2023-17:58:48] [V] [TRT] --------------- Timing Runner: [trainStation3] (TrainStation)
[10/31/2023-17:58:48] [V] [TRT] Tactic: 0x0000000000000000 Time: 0.000358313
[10/31/2023-17:58:48] [V] [TRT] Fastest Tactic: 0x0000000000000000 Time: 0.000358313
[10/31/2023-17:58:48] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: TrainStation Tactic: 0x0000000000000000
[10/31/2023-17:58:48] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv (/backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear_output_0) from Int8(8549376,534336:4,968,1) to Int8(1068672,534336:32,968,1)
[10/31/2023-17:58:48] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to backbone.layer1.1.conv1.weight + /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.1/conv1/Conv (/backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear_output_0) from Int8(8549376,534336:4,968,1) to Int8(1068672,534336:32,968,1)
[10/31/2023-17:58:48] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv (/backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0) from Int8(8549376,534336:4,968,1) to Int8(1068672,534336:32,968,1)
[10/31/2023-17:58:48] [V] [TRT] Adding reformat layer: Reformatted Input Tensor 0 to backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv (/backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear_output_0) from Int8(4274688,133584:4,484,1) to Int8(534336,133584:32,484,1)
[10/31/2023-17:58:48] [V] [TRT] Formats and tactics selection completed in 309.95 seconds.
[10/31/2023-17:58:48] [V] [TRT] After reformat layers: 86 layers
[10/31/2023-17:58:48] [V] [TRT] Total number of blocks in pre-optimized block assignment: 75
[10/31/2023-17:58:48] [I] [TRT] Total Activation Memory: 10399819264
[10/31/2023-17:58:48] [I] [TRT] Detected 1 inputs and 1 output network tensors.
[10/31/2023-17:58:48] [V] [TRT] backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear + /backbone/conv1/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 0xb29abdd00304c881
[10/31/2023-17:58:48] [V] [TRT] /backbone/maxpool/MaxPool Set Tactic Name: sm50_xmma_pooling_tiled_FP32NCHW_kMAX_custom_tP4_tQ32_tRS3_tUV2 Tactic: 0x2639d3932b27ac67
[10/31/2023-17:58:48] [V] [TRT] backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6
[10/31/2023-17:58:48] [V] [TRT] backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e
[10/31/2023-17:58:48] [V] [TRT] backbone.layer1.1.conv1.weight + /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6
[10/31/2023-17:58:48] [V] [TRT] backbone.layer1.1.conv2.weight + /backbone/layer1/layer1.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.1/conv2/Conv + /backbone/layer1/layer1.1/Add + /backbone/layer1/layer1.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e
[10/31/2023-17:58:48] [V] [TRT] backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32
[10/31/2023-17:58:48] [V] [TRT] backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96
[10/31/2023-17:58:48] [V] [TRT] backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e
[10/31/2023-17:58:48] [V] [TRT] backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96
[10/31/2023-17:58:48] [V] [TRT] backbone.layer2.1.conv2.weight + /backbone/layer2/layer2.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv2/Conv + /backbone/layer2/layer2.1/Add + /backbone/layer2/layer2.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e
[10/31/2023-17:58:48] [V] [TRT] backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431
[10/31/2023-17:58:48] [V] [TRT] backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96
[10/31/2023-17:58:48] [V] [TRT] backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e
[10/31/2023-17:58:48] [V] [TRT] backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96
[10/31/2023-17:58:48] [V] [TRT] backbone.layer3.1.conv2.weight + /backbone/layer3/layer3.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv2/Conv + /backbone/layer3/layer3.1/Add + /backbone/layer3/layer3.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e
[10/31/2023-17:58:48] [V] [TRT] backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431
[10/31/2023-17:58:48] [V] [TRT] backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96
[10/31/2023-17:58:48] [V] [TRT] backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808
[10/31/2023-17:58:48] [V] [TRT] backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96
[10/31/2023-17:58:48] [V] [TRT] backbone.layer4.1.conv2.weight + /backbone/layer4/layer4.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv2/Conv + /backbone/layer4/layer4.1/Add + /backbone/layer4/layer4.1/relu_1/Relu Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808
[10/31/2023-17:58:48] [V] [TRT] neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x15f0bde1f6cbbf61
[10/31/2023-17:58:48] [V] [TRT] neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xa197727afc8dee05
[10/31/2023-17:58:48] [V] [TRT] neck.detect1.conv3.conv.weight + /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv3/conv/Conv + PWN(/neck/detect1/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x15f0bde1f6cbbf61
[10/31/2023-17:58:48] [V] [TRT] neck.detect1.conv4.conv.weight + /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv4/conv/Conv + PWN(/neck/detect1/conv4/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xa197727afc8dee05
[10/31/2023-17:58:49] [V] [TRT] neck.detect1.conv5.conv.weight + /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv5/conv/Conv + PWN(/neck/detect1/conv5/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x15f0bde1f6cbbf61
[10/31/2023-17:58:49] [V] [TRT] neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x15f0bde1f6cbbf61
[10/31/2023-17:58:49] [V] [TRT] neck.detect1.conv6.conv.weight + /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv6/conv/Conv + PWN(/neck/detect1/conv6/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xa197727afc8dee05
[10/31/2023-17:58:49] [V] [TRT] neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431
[10/31/2023-17:58:49] [V] [TRT] neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x15f0bde1f6cbbf61
[10/31/2023-17:58:49] [V] [TRT] neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xa197727afc8dee05
[10/31/2023-17:58:49] [V] [TRT] neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x15f0bde1f6cbbf61
[10/31/2023-17:58:49] [V] [TRT] neck.detect2.conv4.conv.weight + /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv4/conv/Conv + PWN(/neck/detect2/conv4/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xa197727afc8dee05
[10/31/2023-17:58:50] [V] [TRT] neck.detect2.conv5.conv.weight + /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv5/conv/Conv + PWN(/neck/detect2/conv5/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x15f0bde1f6cbbf61
[10/31/2023-17:58:50] [V] [TRT] neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x25bf7139bbd93bc8
[10/31/2023-17:58:50] [V] [TRT] neck.detect2.conv6.conv.weight + /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv6/conv/Conv + PWN(/neck/detect2/conv6/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xa197727afc8dee05
[10/31/2023-17:58:50] [V] [TRT] neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431
[10/31/2023-17:58:50] [V] [TRT] neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x25bf7139bbd93bc8
[10/31/2023-17:58:50] [V] [TRT] neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xa197727afc8dee05
[10/31/2023-17:58:50] [V] [TRT] neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x25bf7139bbd93bc8
[10/31/2023-17:58:50] [V] [TRT] neck.detect3.conv4.conv.weight + /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv4/conv/Conv + PWN(/neck/detect3/conv4/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xa197727afc8dee05
[10/31/2023-17:58:50] [V] [TRT] neck.detect3.conv5.conv.weight + /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv5/conv/Conv + PWN(/neck/detect3/conv5/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_linkable Tactic: 0x25bf7139bbd93bc8
[10/31/2023-17:58:51] [V] [TRT] neck.detect3.conv6.conv.weight + /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv6/conv/Conv + PWN(/neck/detect3/conv6/relu/LeakyRelu) Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_linkable Tactic: 0xa197727afc8dee05
[10/31/2023-17:58:51] [V] [TRT] neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431
[10/31/2023-17:58:51] [V] [TRT] Layer: backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear + /backbone/conv1/Conv Host Persistent: 2176 Device Persistent: 12824576 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: /backbone/maxpool/MaxPool Host Persistent: 1344 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: backbone.layer1.1.conv1.weight + /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.1/conv1/Conv Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: backbone.layer1.1.conv2.weight + /backbone/layer1/layer1.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.1/conv2/Conv + /backbone/layer1/layer1.1/Add + /backbone/layer1/layer1.1/relu_1/Relu Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Host Persistent: 3200 Device Persistent: 801792 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: PWN((Unnamed Layer* 112) [ElementWise], PWN(PWN((Unnamed Layer* 109) [Constant] + (Unnamed Layer* 110) [ElementWise], (Unnamed Layer* 111) [Unary]), (Unnamed Layer* 113) [ElementWise])) Host Persistent: 476 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: backbone.layer2.1.conv2.weight + /backbone/layer2/layer2.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv2/Conv + /backbone/layer2/layer2.1/Add + /backbone/layer2/layer2.1/relu_1/Relu Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: PWN((Unnamed Layer* 186) [ElementWise], PWN(PWN((Unnamed Layer* 183) [Constant] + (Unnamed Layer* 184) [ElementWise], (Unnamed Layer* 185) [Unary]), (Unnamed Layer* 187) [ElementWise])) Host Persistent: 476 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: backbone.layer3.1.conv2.weight + /backbone/layer3/layer3.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv2/Conv + /backbone/layer3/layer3.1/Add + /backbone/layer3/layer3.1/relu_1/Relu Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: PWN((Unnamed Layer* 260) [ElementWise], PWN(PWN((Unnamed Layer* 257) [Constant] + (Unnamed Layer* 258) [ElementWise], (Unnamed Layer* 259) [Unary]), (Unnamed Layer* 261) [ElementWise])) Host Persistent: 476 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: backbone.layer4.1.conv2.weight + /backbone/layer4/layer4.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv2/Conv + /backbone/layer4/layer4.1/Add + /backbone/layer4/layer4.1/relu_1/Relu Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.detect1.conv3.conv.weight + /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv3/conv/Conv + PWN(/neck/detect1/conv3/relu/LeakyRelu) Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.detect1.conv4.conv.weight + /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv4/conv/Conv + PWN(/neck/detect1/conv4/relu/LeakyRelu) Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.detect1.conv5.conv.weight + /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv5/conv/Conv + PWN(/neck/detect1/conv5/relu/LeakyRelu) Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.detect1.conv6.conv.weight + /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv6/conv/Conv + PWN(/neck/detect1/conv6/relu/LeakyRelu) Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.detect2.conv4.conv.weight + /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv4/conv/Conv + PWN(/neck/detect2/conv4/relu/LeakyRelu) Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.detect2.conv5.conv.weight + /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv5/conv/Conv + PWN(/neck/detect2/conv5/relu/LeakyRelu) Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.detect2.conv6.conv.weight + /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv6/conv/Conv + PWN(/neck/detect2/conv6/relu/LeakyRelu) Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.detect3.conv4.conv.weight + /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv4/conv/Conv + PWN(/neck/detect3/conv4/relu/LeakyRelu) Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.detect3.conv5.conv.weight + /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv5/conv/Conv + PWN(/neck/detect3/conv5/relu/LeakyRelu) Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.detect3.conv6.conv.weight + /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv6/conv/Conv + PWN(/neck/detect3/conv6/relu/LeakyRelu) Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Host Persistent: 2816 Device Persistent: 0 Scratch Memory: 0
[10/31/2023-17:58:51] [V] [TRT] Layer: {ForeignNode[/head/Constant_2_output_0.../Unsqueeze_2]} Host Persistent: 24 Device Persistent: 0 Scratch Memory: 89769472
[10/31/2023-17:58:51] [V] [TRT] Layer: /NonMaxSuppression_246 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 108544
[10/31/2023-17:58:51] [V] [TRT] Layer: {ForeignNode[/Split_2.../Concat_1]} Host Persistent: 192 Device Persistent: 0 Scratch Memory: 10240
[10/31/2023-17:58:51] [V] [TRT] Skipped printing memory information for 36 layers with 0 memory size i.e. Host Persistent + Device Persistent + Scratch Memory == 0.
[10/31/2023-17:58:51] [I] [TRT] Total Host Persistent Memory: 123840
[10/31/2023-17:58:51] [I] [TRT] Total Device Persistent Memory: 13626368
[10/31/2023-17:58:51] [I] [TRT] Total Scratch Memory: 89769472
[10/31/2023-17:58:51] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 79 MiB, GPU 1047 MiB
[10/31/2023-17:58:51] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 77 steps to complete.
[10/31/2023-17:58:51] [V] [TRT] STILL ALIVE: Started step 26 of 77
[10/31/2023-17:58:51] [V] [TRT] STILL ALIVE: Started step 76 of 77
[10/31/2023-17:58:51] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 4.6652ms to assign 9 blocks to 77 nodes requiring 756727296 bytes.
[10/31/2023-17:58:51] [V] [TRT] Total number of blocks in optimized block assignment: 9
[10/31/2023-17:58:51] [I] [TRT] Total Activation Memory: 756727296
[10/31/2023-17:58:51] [V] [TRT] Finalize: backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear + /backbone/conv1/Conv Set kernel index: 0
[10/31/2023-17:58:51] [V] [TRT] Finalize: /backbone/maxpool/MaxPool Set kernel index: 1
[10/31/2023-17:58:51] [V] [TRT] Finalize: backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv Set kernel index: 2
[10/31/2023-17:58:51] [V] [TRT] Finalize: backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu Set kernel index: 3
[10/31/2023-17:58:51] [V] [TRT] Finalize: backbone.layer1.1.conv1.weight + /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.1/conv1/Conv Set kernel index: 2
[10/31/2023-17:58:51] [V] [TRT] Finalize: backbone.layer1.1.conv2.weight + /backbone/layer1/layer1.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.1/conv2/Conv + /backbone/layer1/layer1.1/Add + /backbone/layer1/layer1.1/relu_1/Relu Set kernel index: 3
[10/31/2023-17:58:51] [V] [TRT] Finalize: backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv Set kernel index: 4
[10/31/2023-17:58:51] [V] [TRT] Finalize: backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv Set kernel index: 5
[10/31/2023-17:58:51] [V] [TRT] Finalize: PWN((Unnamed Layer* 112) [ElementWise], PWN(PWN((Unnamed Layer* 109) [Constant] + (Unnamed Layer* 110) [ElementWise], (Unnamed Layer* 111) [Unary]), (Unnamed Layer* 113) [ElementWise])) Set kernel index: 6
[10/31/2023-17:58:51] [V] [TRT] Finalize: backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu Set kernel index: 3
[10/31/2023-17:58:51] [V] [TRT] Finalize: backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv Set kernel index: 5
[10/31/2023-17:58:51] [V] [TRT] Finalize: backbone.layer2.1.conv2.weight + /backbone/layer2/layer2.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv2/Conv + /backbone/layer2/layer2.1/Add + /backbone/layer2/layer2.1/relu_1/Relu Set kernel index: 3
[10/31/2023-17:58:51] [V] [TRT] Finalize: backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv Set kernel index: 7
[10/31/2023-17:58:51] [V] [TRT] Finalize: backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv Set kernel index: 5
[10/31/2023-17:58:51] [V] [TRT] Finalize: PWN((Unnamed Layer* 186) [ElementWise], PWN(PWN((Unnamed Layer* 183) [Constant] + (Unnamed Layer* 184) [ElementWise], (Unnamed Layer* 185) [Unary]), (Unnamed Layer* 187) [ElementWise])) Set kernel index: 6
[10/31/2023-17:58:51] [V] [TRT] Finalize: backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu Set kernel index: 3
[10/31/2023-17:58:51] [V] [TRT] Finalize: backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv Set kernel index: 5
[10/31/2023-17:58:51] [V] [TRT] Finalize: backbone.layer3.1.conv2.weight + /backbone/layer3/layer3.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv2/Conv + /backbone/layer3/layer3.1/Add + /backbone/layer3/layer3.1/relu_1/Relu Set kernel index: 3
[10/31/2023-17:58:51] [V] [TRT] Finalize: backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv Set kernel index: 7
[10/31/2023-17:58:51] [V] [TRT] Finalize: backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv Set kernel index: 5
[10/31/2023-17:58:51] [V] [TRT] Finalize: PWN((Unnamed Layer* 260) [ElementWise], PWN(PWN((Unnamed Layer* 257) [Constant] + (Unnamed Layer* 258) [ElementWise], (Unnamed Layer* 259) [Unary]), (Unnamed Layer* 261) [ElementWise])) Set kernel index: 8
[10/31/2023-17:58:51] [V] [TRT] Finalize: backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu Set kernel index: 9
[10/31/2023-17:58:51] [V] [TRT] Finalize: backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv Set kernel index: 5
[10/31/2023-17:58:51] [V] [TRT] Finalize: backbone.layer4.1.conv2.weight + /backbone/layer4/layer4.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv2/Conv + /backbone/layer4/layer4.1/Add + /backbone/layer4/layer4.1/relu_1/Relu Set kernel index: 9
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu) Set kernel index: 10
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu) Set kernel index: 11
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.detect1.conv3.conv.weight + /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv3/conv/Conv + PWN(/neck/detect1/conv3/relu/LeakyRelu) Set kernel index: 12
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.detect1.conv4.conv.weight + /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv4/conv/Conv + PWN(/neck/detect1/conv4/relu/LeakyRelu) Set kernel index: 13
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.detect1.conv5.conv.weight + /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv5/conv/Conv + PWN(/neck/detect1/conv5/relu/LeakyRelu) Set kernel index: 14
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu) Set kernel index: 15
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.detect1.conv6.conv.weight + /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv6/conv/Conv + PWN(/neck/detect1/conv6/relu/LeakyRelu) Set kernel index: 16
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv Set kernel index: 7
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu) Set kernel index: 17
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu) Set kernel index: 18
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu) Set kernel index: 19
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.detect2.conv4.conv.weight + /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv4/conv/Conv + PWN(/neck/detect2/conv4/relu/LeakyRelu) Set kernel index: 20
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.detect2.conv5.conv.weight + /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv5/conv/Conv + PWN(/neck/detect2/conv5/relu/LeakyRelu) Set kernel index: 21
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu) Set kernel index: 22
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.detect2.conv6.conv.weight + /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv6/conv/Conv + PWN(/neck/detect2/conv6/relu/LeakyRelu) Set kernel index: 23
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv Set kernel index: 7
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu) Set kernel index: 24
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu) Set kernel index: 25
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu) Set kernel index: 26
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.detect3.conv4.conv.weight + /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv4/conv/Conv + PWN(/neck/detect3/conv4/relu/LeakyRelu) Set kernel index: 27
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.detect3.conv5.conv.weight + /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv5/conv/Conv + PWN(/neck/detect3/conv5/relu/LeakyRelu) Set kernel index: 28
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.detect3.conv6.conv.weight + /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv6/conv/Conv + PWN(/neck/detect3/conv6/relu/LeakyRelu) Set kernel index: 29
[10/31/2023-17:58:51] [V] [TRT] Finalize: neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv Set kernel index: 7
[10/31/2023-17:58:51] [V] [TRT] Total number of generated kernels selected for the engine: 30
[10/31/2023-17:58:51] [V] [TRT] Kernel: 0 CASK_STATIC
[10/31/2023-17:58:51] [V] [TRT] Kernel: 1 CASK_STATIC
[10/31/2023-17:58:51] [V] [TRT] Kernel: 2 CASK_STATIC
[10/31/2023-17:58:51] [V] [TRT] Kernel: 3 CASK_STATIC
[10/31/2023-17:58:51] [V] [TRT] Kernel: 4 CASK_STATIC
[10/31/2023-17:58:51] [V] [TRT] Kernel: 5 CASK_STATIC
[10/31/2023-17:58:51] [V] [TRT] Kernel: 6 TRT_SERIALIZABLE:generatedNativePointwise
[10/31/2023-17:58:51] [V] [TRT] Kernel: 7 CASK_STATIC
[10/31/2023-17:58:51] [V] [TRT] Kernel: 8 TRT_SERIALIZABLE:generatedNativePointwise
[10/31/2023-17:58:51] [V] [TRT] Kernel: 9 CASK_STATIC
[10/31/2023-17:58:51] [V] [TRT] Kernel: 10 CASK_LINKABLE
[10/31/2023-17:58:51] [V] [TRT] Kernel: 11 CASK_LINKABLE
[10/31/2023-17:58:51] [V] [TRT] Kernel: 12 CASK_LINKABLE
[10/31/2023-17:58:51] [V] [TRT] Kernel: 13 CASK_LINKABLE
[10/31/2023-17:58:51] [V] [TRT] Kernel: 14 CASK_LINKABLE
[10/31/2023-17:58:51] [V] [TRT] Kernel: 15 CASK_LINKABLE
[10/31/2023-17:58:51] [V] [TRT] Kernel: 16 CASK_LINKABLE
[10/31/2023-17:58:51] [V] [TRT] Kernel: 17 CASK_LINKABLE
[10/31/2023-17:58:51] [V] [TRT] Kernel: 18 CASK_LINKABLE
[10/31/2023-17:58:51] [V] [TRT] Kernel: 19 CASK_LINKABLE
[10/31/2023-17:58:51] [V] [TRT] Kernel: 20 CASK_LINKABLE
[10/31/2023-17:58:51] [V] [TRT] Kernel: 21 CASK_LINKABLE
[10/31/2023-17:58:51] [V] [TRT] Kernel: 22 CASK_LINKABLE
[10/31/2023-17:58:51] [V] [TRT] Kernel: 23 CASK_LINKABLE
[10/31/2023-17:58:51] [V] [TRT] Kernel: 24 CASK_LINKABLE
[10/31/2023-17:58:51] [V] [TRT] Kernel: 25 CASK_LINKABLE
[10/31/2023-17:58:51] [V] [TRT] Kernel: 26 CASK_LINKABLE
[10/31/2023-17:58:51] [V] [TRT] Kernel: 27 CASK_LINKABLE
[10/31/2023-17:58:51] [V] [TRT] Kernel: 28 CASK_LINKABLE
[10/31/2023-17:58:51] [V] [TRT] Kernel: 29 CASK_LINKABLE
[10/31/2023-17:58:51] [V] [TRT] Disabling unused tactic source: CUDNN
[10/31/2023-17:58:51] [V] [TRT] Disabling unused tactic source: CUBLAS, CUBLAS_LT
[10/31/2023-17:58:51] [V] [TRT] Disabling unused tactic source: JIT_CONVOLUTIONS
[10/31/2023-17:58:51] [V] [TRT] Engine generation completed in 314.751 seconds.
[10/31/2023-17:58:51] [V] [TRT] Deleting timing cache: 317 entries, served 334 hits since creation.
[10/31/2023-17:58:51] [V] [TRT] Engine Layer Information:
Layer(TrainStation): [trainStation1], Tactic: 0x0000000000000000,  -> 
Layer(Reformat): /backbone/conv1/_input_quantizer/QuantizeLinear, Tactic: 0x0000000000000000, input (Float[1,3,2208,3872]) -> /backbone/conv1/_input_quantizer/QuantizeLinear_output_0 (Int8[1,3:4,2208,3872])
Layer(Constant): backbone.layer2.0.downsample.1.running_var + (Unnamed Layer* 108) [Shuffle], Tactic: 0x0000000000000000,  -> (Unnamed Layer* 108) [Shuffle]_output (Float[1,128,1,1])
Layer(Constant): backbone.layer2.0.downsample.1.running_mean + (Unnamed Layer* 107) [Shuffle], Tactic: 0x0000000000000000,  -> (Unnamed Layer* 107) [Shuffle]_output (Float[1,128,1,1])
Layer(Constant): backbone.layer3.0.downsample.1.running_var + (Unnamed Layer* 182) [Shuffle], Tactic: 0x0000000000000000,  -> (Unnamed Layer* 182) [Shuffle]_output (Float[1,256,1,1])
Layer(Constant): backbone.layer3.0.downsample.1.running_mean + (Unnamed Layer* 181) [Shuffle], Tactic: 0x0000000000000000,  -> (Unnamed Layer* 181) [Shuffle]_output (Float[1,256,1,1])
Layer(Constant): backbone.layer4.0.downsample.1.running_var + (Unnamed Layer* 256) [Shuffle], Tactic: 0x0000000000000000,  -> (Unnamed Layer* 256) [Shuffle]_output (Float[1,512,1,1])
Layer(Constant): backbone.layer4.0.downsample.1.running_mean + (Unnamed Layer* 255) [Shuffle], Tactic: 0x0000000000000000,  -> (Unnamed Layer* 255) [Shuffle]_output (Float[1,512,1,1])
Layer(Constant): (Unnamed Layer* 714) [Constant], Tactic: 0x0000000000000000,  -> (Unnamed Layer* 714) [Constant]_output (Int32[])
Layer(Constant): (Unnamed Layer* 715) [Constant], Tactic: 0x0000000000000000,  -> (Unnamed Layer* 715) [Constant]_output (Float[])
Layer(Constant): (Unnamed Layer* 716) [Constant], Tactic: 0x0000000000000000,  -> (Unnamed Layer* 716) [Constant]_output (Float[])
Layer(CaskConvolution): backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear + /backbone/conv1/Conv, Tactic: 0xb29abdd00304c881, /backbone/conv1/_input_quantizer/QuantizeLinear_output_0 (Int8[1,3:4,2208,3872]) -> /backbone/relu/Relu_output_0 (Float[1,64,1104,1936])
Layer(CaskPooling): /backbone/maxpool/MaxPool, Tactic: 0x2639d3932b27ac67, /backbone/relu/Relu_output_0 (Float[1,64,1104,1936]) -> /backbone/maxpool/MaxPool_output_0 (Float[1,64,552,968])
Layer(Reformat): /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear, Tactic: 0x0000000000000000, /backbone/maxpool/MaxPool_output_0 (Float[1,64,552,968]) -> /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear_output_0 (Int8[1,64:4,552,968])
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv, Tactic: 0x0000000000000000, /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear_output_0 (Int8[1,64:4,552,968]) -> Reformatted Input Tensor 0 to backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv (Int8[1,64:32,552,968])
Layer(CaskConvolution): backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv, Tactic: 0xd3d41ef6de22d9b6, Reformatted Input Tensor 0 to backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv (Int8[1,64:32,552,968]) -> /backbone/layer1/layer1.0/conv2/_input_quantizer/QuantizeLinear_output_0 (Int8[1,64:32,552,968])
Layer(CaskConvolution): backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu, Tactic: 0x23b890da05937b9e, /backbone/layer1/layer1.0/conv2/_input_quantizer/QuantizeLinear_output_0 (Int8[1,64:32,552,968]), /backbone/maxpool/MaxPool_output_0 (Float[1,64,552,968]) -> /backbone/layer1/layer1.0/relu_1/Relu_output_0 (Float[1,64,552,968])
Layer(Reformat): /backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear, Tactic: 0x0000000000000000, /backbone/layer1/layer1.0/relu_1/Relu_output_0 (Float[1,64,552,968]) -> /backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear_output_0 (Int8[1,64:4,552,968])
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to backbone.layer1.1.conv1.weight + /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.1/conv1/Conv, Tactic: 0x0000000000000000, /backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear_output_0 (Int8[1,64:4,552,968]) -> Reformatted Input Tensor 0 to backbone.layer1.1.conv1.weight + /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.1/conv1/Conv (Int8[1,64:32,552,968])
Layer(CaskConvolution): backbone.layer1.1.conv1.weight + /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.1/conv1/Conv, Tactic: 0xd3d41ef6de22d9b6, Reformatted Input Tensor 0 to backbone.layer1.1.conv1.weight + /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.1/conv1/Conv (Int8[1,64:32,552,968]) -> /backbone/layer1/layer1.1/conv2/_input_quantizer/QuantizeLinear_output_0 (Int8[1,64:32,552,968])
Layer(CaskConvolution): backbone.layer1.1.conv2.weight + /backbone/layer1/layer1.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.1/conv2/Conv + /backbone/layer1/layer1.1/Add + /backbone/layer1/layer1.1/relu_1/Relu, Tactic: 0x23b890da05937b9e, /backbone/layer1/layer1.1/conv2/_input_quantizer/QuantizeLinear_output_0 (Int8[1,64:32,552,968]), /backbone/layer1/layer1.0/relu_1/Relu_output_0 (Float[1,64,552,968]) -> /backbone/layer1/layer1.1/relu_1/Relu_output_0 (Float[1,64,552,968])
Layer(Reformat): /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear, Tactic: 0x0000000000000000, /backbone/layer1/layer1.1/relu_1/Relu_output_0 (Float[1,64,552,968]) -> /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0 (Int8[1,64:4,552,968])
Layer(CaskConvolution): backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv, Tactic: 0xff6944b17d5b2e32, /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0 (Int8[1,64:4,552,968]) -> /backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 (Float[1,128,276,484])
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv, Tactic: 0x0000000000000000, /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0 (Int8[1,64:4,552,968]) -> Reformatted Input Tensor 0 to backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv (Int8[1,64:32,552,968])
Layer(CaskConvolution): backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv, Tactic: 0xea88b51105501f96, Reformatted Input Tensor 0 to backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv (Int8[1,64:32,552,968]) -> /backbone/layer2/layer2.0/conv2/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:32,276,484])
Layer(PointWiseV2): PWN((Unnamed Layer* 112) [ElementWise], PWN(PWN((Unnamed Layer* 109) [Constant] + (Unnamed Layer* 110) [ElementWise], (Unnamed Layer* 111) [Unary]), (Unnamed Layer* 113) [ElementWise])), Tactic: 0x0000000000000007, /backbone/layer2/layer2.0/downsample/downsample.0/Conv_output_0 (Float[1,128,276,484]), (Unnamed Layer* 107) [Shuffle]_output (Float[1,128,1,1]), (Unnamed Layer* 108) [Shuffle]_output (Float[1,128,1,1]) -> (Unnamed Layer* 113) [ElementWise]_output (Float[1,128,276,484])
Layer(Scale): backbone.layer2.0.downsample.1.weight + (Unnamed Layer* 105) [Shuffle] + (Unnamed Layer* 114) [ElementWise] + backbone.layer2.0.bn2.bias + Identity_2 + (Unnamed Layer* 106) [Shuffle] + /backbone/layer2/layer2.0/downsample/downsample.1/BatchNormalization, Tactic: 0x0000000000000000, (Unnamed Layer* 113) [ElementWise]_output (Float[1,128,276,484]) -> /backbone/layer2/layer2.0/downsample/downsample.1/BatchNormalization_output_0 (Float[1,128,276,484])
Layer(CaskConvolution): backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu, Tactic: 0x23b890da05937b9e, /backbone/layer2/layer2.0/conv2/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:32,276,484]), /backbone/layer2/layer2.0/downsample/downsample.1/BatchNormalization_output_0 (Float[1,128,276,484]) -> /backbone/layer2/layer2.0/relu_1/Relu_output_0 (Float[1,128,276,484])
Layer(Reformat): /backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear, Tactic: 0x0000000000000000, /backbone/layer2/layer2.0/relu_1/Relu_output_0 (Float[1,128,276,484]) -> /backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:4,276,484])
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv, Tactic: 0x0000000000000000, /backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:4,276,484]) -> Reformatted Input Tensor 0 to backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv (Int8[1,128:32,276,484])
Layer(CaskConvolution): backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv, Tactic: 0xea88b51105501f96, Reformatted Input Tensor 0 to backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv (Int8[1,128:32,276,484]) -> /backbone/layer2/layer2.1/conv2/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:32,276,484])
Layer(CaskConvolution): backbone.layer2.1.conv2.weight + /backbone/layer2/layer2.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv2/Conv + /backbone/layer2/layer2.1/Add + /backbone/layer2/layer2.1/relu_1/Relu, Tactic: 0x23b890da05937b9e, /backbone/layer2/layer2.1/conv2/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:32,276,484]), /backbone/layer2/layer2.0/relu_1/Relu_output_0 (Float[1,128,276,484]) -> /backbone/layer2/layer2.1/relu_1/Relu_output_0 (Float[1,128,276,484])
Layer(Reformat): /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear, Tactic: 0x00000000000003ea, /backbone/layer2/layer2.1/relu_1/Relu_output_0 (Float[1,128,276,484]) -> /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:32,276,484])
Layer(Reformat): /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_clone_1, Tactic: 0x00000000000003ea, /backbone/layer2/layer2.1/relu_1/Relu_output_0 (Float[1,128,276,484]) -> /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:32,276,484])
Layer(CaskConvolution): backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv, Tactic: 0x733ba2a91a48d431, /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:32,276,484]) -> /backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 (Float[1,256,138,242])
Layer(CaskConvolution): backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv, Tactic: 0xea88b51105501f96, /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:32,276,484]) -> /backbone/layer3/layer3.0/conv2/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,138,242])
Layer(PointWiseV2): PWN((Unnamed Layer* 186) [ElementWise], PWN(PWN((Unnamed Layer* 183) [Constant] + (Unnamed Layer* 184) [ElementWise], (Unnamed Layer* 185) [Unary]), (Unnamed Layer* 187) [ElementWise])), Tactic: 0x0000000000000007, /backbone/layer3/layer3.0/downsample/downsample.0/Conv_output_0 (Float[1,256,138,242]), (Unnamed Layer* 181) [Shuffle]_output (Float[1,256,1,1]), (Unnamed Layer* 182) [Shuffle]_output (Float[1,256,1,1]) -> (Unnamed Layer* 187) [ElementWise]_output (Float[1,256,138,242])
Layer(Scale): backbone.layer3.0.downsample.1.weight + (Unnamed Layer* 179) [Shuffle] + (Unnamed Layer* 188) [ElementWise] + backbone.layer3.0.bn2.bias + Identity_1 + (Unnamed Layer* 180) [Shuffle] + /backbone/layer3/layer3.0/downsample/downsample.1/BatchNormalization, Tactic: 0x0000000000000000, (Unnamed Layer* 187) [ElementWise]_output (Float[1,256,138,242]) -> /backbone/layer3/layer3.0/downsample/downsample.1/BatchNormalization_output_0 (Float[1,256,138,242])
Layer(CaskConvolution): backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu, Tactic: 0x23b890da05937b9e, /backbone/layer3/layer3.0/conv2/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,138,242]), /backbone/layer3/layer3.0/downsample/downsample.1/BatchNormalization_output_0 (Float[1,256,138,242]) -> /backbone/layer3/layer3.0/relu_1/Relu_output_0 (Float[1,256,138,242])
Layer(Reformat): /backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear, Tactic: 0x00000000000003ea, /backbone/layer3/layer3.0/relu_1/Relu_output_0 (Float[1,256,138,242]) -> /backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,138,242])
Layer(CaskConvolution): backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv, Tactic: 0xea88b51105501f96, /backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,138,242]) -> /backbone/layer3/layer3.1/conv2/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,138,242])
Layer(CaskConvolution): backbone.layer3.1.conv2.weight + /backbone/layer3/layer3.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv2/Conv + /backbone/layer3/layer3.1/Add + /backbone/layer3/layer3.1/relu_1/Relu, Tactic: 0x23b890da05937b9e, /backbone/layer3/layer3.1/conv2/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,138,242]), /backbone/layer3/layer3.0/relu_1/Relu_output_0 (Float[1,256,138,242]) -> /backbone/layer3/layer3.1/relu_1/Relu_output_0 (Float[1,256,138,242])
Layer(Reformat): /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear, Tactic: 0x00000000000003ea, /backbone/layer3/layer3.1/relu_1/Relu_output_0 (Float[1,256,138,242]) -> /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,138,242])
Layer(Reformat): /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_clone_1, Tactic: 0x00000000000003ea, /backbone/layer3/layer3.1/relu_1/Relu_output_0 (Float[1,256,138,242]) -> /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,138,242])
Layer(CaskConvolution): backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv, Tactic: 0x733ba2a91a48d431, /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,138,242]) -> /backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 (Float[1,512,69,121])
Layer(CaskConvolution): backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv, Tactic: 0xea88b51105501f96, /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,138,242]) -> /backbone/layer4/layer4.0/conv2/_input_quantizer/QuantizeLinear_output_0 (Int8[1,512:32,69,121])
Layer(PointWiseV2): PWN((Unnamed Layer* 260) [ElementWise], PWN(PWN((Unnamed Layer* 257) [Constant] + (Unnamed Layer* 258) [ElementWise], (Unnamed Layer* 259) [Unary]), (Unnamed Layer* 261) [ElementWise])), Tactic: 0x0000000000000004, /backbone/layer4/layer4.0/downsample/downsample.0/Conv_output_0 (Float[1,512,69,121]), (Unnamed Layer* 255) [Shuffle]_output (Float[1,512,1,1]), (Unnamed Layer* 256) [Shuffle]_output (Float[1,512,1,1]) -> (Unnamed Layer* 261) [ElementWise]_output (Float[1,512,69,121])
Layer(Scale): backbone.layer4.0.downsample.1.weight + (Unnamed Layer* 253) [Shuffle] + (Unnamed Layer* 262) [ElementWise] + backbone.layer4.0.bn2.bias + Identity_0 + (Unnamed Layer* 254) [Shuffle] + /backbone/layer4/layer4.0/downsample/downsample.1/BatchNormalization, Tactic: 0x0000000000000000, (Unnamed Layer* 261) [ElementWise]_output (Float[1,512,69,121]) -> /backbone/layer4/layer4.0/downsample/downsample.1/BatchNormalization_output_0 (Float[1,512,69,121])
Layer(CaskConvolution): backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu, Tactic: 0xac914b235d066808, /backbone/layer4/layer4.0/conv2/_input_quantizer/QuantizeLinear_output_0 (Int8[1,512:32,69,121]), /backbone/layer4/layer4.0/downsample/downsample.1/BatchNormalization_output_0 (Float[1,512,69,121]) -> /backbone/layer4/layer4.0/relu_1/Relu_output_0 (Float[1,512,69,121])
Layer(Reformat): /backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear, Tactic: 0x00000000000003ea, /backbone/layer4/layer4.0/relu_1/Relu_output_0 (Float[1,512,69,121]) -> /backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear_output_0 (Int8[1,512:32,69,121])
Layer(CaskConvolution): backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv, Tactic: 0xea88b51105501f96, /backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear_output_0 (Int8[1,512:32,69,121]) -> /backbone/layer4/layer4.1/conv2/_input_quantizer/QuantizeLinear_output_0 (Int8[1,512:32,69,121])
Layer(CaskConvolution): backbone.layer4.1.conv2.weight + /backbone/layer4/layer4.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv2/Conv + /backbone/layer4/layer4.1/Add + /backbone/layer4/layer4.1/relu_1/Relu, Tactic: 0xac914b235d066808, /backbone/layer4/layer4.1/conv2/_input_quantizer/QuantizeLinear_output_0 (Int8[1,512:32,69,121]), /backbone/layer4/layer4.0/relu_1/Relu_output_0 (Float[1,512,69,121]) -> /backbone/layer4/layer4.1/relu_1/Relu_output_0 (Float[1,512,69,121])
Layer(Reformat): /neck/detect1/conv1/conv/_input_quantizer/QuantizeLinear, Tactic: 0x00000000000003ea, /backbone/layer4/layer4.1/relu_1/Relu_output_0 (Float[1,512,69,121]) -> /neck/detect1/conv1/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,512:32,69,121])
Layer(CaskConvolution): neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu), Tactic: 0x15f0bde1f6cbbf61, /neck/detect1/conv1/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,512:32,69,121]) -> /neck/detect1/conv2/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,69,121])
Layer(CaskConvolution): neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu), Tactic: 0xa197727afc8dee05, /neck/detect1/conv2/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,69,121]) -> /neck/detect1/conv3/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,512:32,69,121])
Layer(CaskConvolution): neck.detect1.conv3.conv.weight + /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv3/conv/Conv + PWN(/neck/detect1/conv3/relu/LeakyRelu), Tactic: 0x15f0bde1f6cbbf61, /neck/detect1/conv3/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,512:32,69,121]) -> /neck/detect1/conv4/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,69,121])
Layer(CaskConvolution): neck.detect1.conv4.conv.weight + /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv4/conv/Conv + PWN(/neck/detect1/conv4/relu/LeakyRelu), Tactic: 0xa197727afc8dee05, /neck/detect1/conv4/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,69,121]) -> /neck/detect1/conv5/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,512:32,69,121])
Layer(CaskConvolution): neck.detect1.conv5.conv.weight + /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv5/conv/Conv + PWN(/neck/detect1/conv5/relu/LeakyRelu), Tactic: 0x15f0bde1f6cbbf61, /neck/detect1/conv5/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,512:32,69,121]) -> /neck/conv1/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,69,121])
Layer(CaskConvolution): neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu), Tactic: 0x15f0bde1f6cbbf61, /neck/conv1/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,69,121]) -> /neck/Resize_output_0 (Int8[1,128:32,69,121])
Layer(CaskConvolution): neck.detect1.conv6.conv.weight + /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv6/conv/Conv + PWN(/neck/detect1/conv6/relu/LeakyRelu), Tactic: 0xa197727afc8dee05, /neck/conv1/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,69,121]) -> /neck/detect1/conv7/_input_quantizer/QuantizeLinear_output_0 (Int8[1,512:32,69,121])
Layer(Resize): /neck/Resize, Tactic: 0x0000000000000006, /neck/Resize_output_0 (Int8[1,128:32,69,121]) -> /neck/Concat_/neck/Resize_output_0_clone_0 (Int8[1,128:32,138,242])
Layer(CaskConvolution): neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv, Tactic: 0x733ba2a91a48d431, /neck/detect1/conv7/_input_quantizer/QuantizeLinear_output_0 (Int8[1,512:32,69,121]) -> /neck/detect1/conv7/Conv_output_0 (Float[1,63,69,121])
Layer(Reformat): /neck/Concat_/neck/Resize_output_0_clone_0 copy, Tactic: 0x0000000000000000, /neck/Concat_/neck/Resize_output_0_clone_0 (Int8[1,128:32,138,242]) -> /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:32,138,242])
Layer(CaskConvolution): neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu), Tactic: 0x15f0bde1f6cbbf61, /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,384:32,138,242]) -> /neck/detect2/conv2/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:32,138,242])
Layer(CaskConvolution): neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu), Tactic: 0xa197727afc8dee05, /neck/detect2/conv2/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:32,138,242]) -> /neck/detect2/conv3/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,138,242])
Layer(CaskConvolution): neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu), Tactic: 0x15f0bde1f6cbbf61, /neck/detect2/conv3/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,138,242]) -> /neck/detect2/conv4/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:32,138,242])
Layer(CaskConvolution): neck.detect2.conv4.conv.weight + /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv4/conv/Conv + PWN(/neck/detect2/conv4/relu/LeakyRelu), Tactic: 0xa197727afc8dee05, /neck/detect2/conv4/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:32,138,242]) -> /neck/detect2/conv5/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,138,242])
Layer(CaskConvolution): neck.detect2.conv5.conv.weight + /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv5/conv/Conv + PWN(/neck/detect2/conv5/relu/LeakyRelu), Tactic: 0x15f0bde1f6cbbf61, /neck/detect2/conv5/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,138,242]) -> /neck/conv2/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:32,138,242])
Layer(CaskConvolution): neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu), Tactic: 0x25bf7139bbd93bc8, /neck/conv2/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:32,138,242]) -> /neck/Resize_1_output_0 (Int8[1,64:32,138,242])
Layer(CaskConvolution): neck.detect2.conv6.conv.weight + /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv6/conv/Conv + PWN(/neck/detect2/conv6/relu/LeakyRelu), Tactic: 0xa197727afc8dee05, /neck/conv2/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:32,138,242]) -> /neck/detect2/conv7/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,138,242])
Layer(Resize): /neck/Resize_1, Tactic: 0x0000000000000006, /neck/Resize_1_output_0 (Int8[1,64:32,138,242]) -> /neck/Concat_1_/neck/Resize_1_output_0_clone_0 (Int8[1,64:32,276,484])
Layer(CaskConvolution): neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv, Tactic: 0x733ba2a91a48d431, /neck/detect2/conv7/_input_quantizer/QuantizeLinear_output_0 (Int8[1,256:32,138,242]) -> /neck/detect2/conv7/Conv_output_0 (Float[1,63,138,242])
Layer(Reformat): /neck/Concat_1_/neck/Resize_1_output_0_clone_0 copy, Tactic: 0x0000000000000000, /neck/Concat_1_/neck/Resize_1_output_0_clone_0 (Int8[1,64:32,276,484]) -> /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,64:32,276,484])
Layer(CaskConvolution): neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu), Tactic: 0x25bf7139bbd93bc8, /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,192:32,276,484]) -> /neck/detect3/conv2/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,64:32,276,484])
Layer(CaskConvolution): neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu), Tactic: 0xa197727afc8dee05, /neck/detect3/conv2/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,64:32,276,484]) -> /neck/detect3/conv3/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:32,276,484])
Layer(CaskConvolution): neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu), Tactic: 0x25bf7139bbd93bc8, /neck/detect3/conv3/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:32,276,484]) -> /neck/detect3/conv4/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,64:32,276,484])
Layer(CaskConvolution): neck.detect3.conv4.conv.weight + /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv4/conv/Conv + PWN(/neck/detect3/conv4/relu/LeakyRelu), Tactic: 0xa197727afc8dee05, /neck/detect3/conv4/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,64:32,276,484]) -> /neck/detect3/conv5/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:32,276,484])
Layer(CaskConvolution): neck.detect3.conv5.conv.weight + /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv5/conv/Conv + PWN(/neck/detect3/conv5/relu/LeakyRelu), Tactic: 0x25bf7139bbd93bc8, /neck/detect3/conv5/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:32,276,484]) -> /neck/detect3/conv6/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,64:32,276,484])
Layer(CaskConvolution): neck.detect3.conv6.conv.weight + /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv6/conv/Conv + PWN(/neck/detect3/conv6/relu/LeakyRelu), Tactic: 0xa197727afc8dee05, /neck/detect3/conv6/conv/_input_quantizer/QuantizeLinear_output_0 (Int8[1,64:32,276,484]) -> /neck/detect3/conv7/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:32,276,484])
Layer(CaskConvolution): neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv, Tactic: 0x733ba2a91a48d431, /neck/detect3/conv7/_input_quantizer/QuantizeLinear_output_0 (Int8[1,128:32,276,484]) -> /neck/detect3/conv7/Conv_output_0 (Float[1,63,276,484])
Layer(Myelin): {ForeignNode[/head/Constant_2_output_0.../Unsqueeze_2]}, Tactic: 0x0000000000000000, /neck/detect3/conv7/Conv_output_0 (Float[1,63,276,484]), /neck/detect2/conv7/Conv_output_0 (Float[1,63,138,242]), /neck/detect1/conv7/Conv_output_0 (Float[1,63,69,121]) -> /Slice_1_output_0 (Float[1,525987,16]), /TopK_output_1 (Int32[3840]), /Unsqueeze_1_output_0 (Float[1,1,3840]), (Unnamed Layer* 717) [Shuffle]_output (Float[1,3840,1]), /Unsqueeze_2_output_0 (Float[1,3840,4])
Layer(NMS): /NonMaxSuppression_246, Tactic: 0x0000000000000000, /Unsqueeze_2_output_0 (Float[1,3840,4]), (Unnamed Layer* 717) [Shuffle]_output (Float[1,3840,1]), (Unnamed Layer* 714) [Constant]_output (Int32[]), (Unnamed Layer* 715) [Constant]_output (Float[]), (Unnamed Layer* 716) [Constant]_output (Float[]) -> /NonMaxSuppression_output_0 (Int32[-1,3]), (Unnamed Layer* 718) [NMS]_1_output (Int32[])
Layer(DeviceToShapeHost): (Unnamed Layer* 718) [NMS]_1_output[DevicetoShapeHostCopy], Tactic: 0x0000000000000000, (Unnamed Layer* 718) [NMS]_1_output (Int32[]) -> 
Layer(TrainStation): [trainStation2], Tactic: 0x0000000000000000,  -> 
Layer(Myelin): {ForeignNode[/Split_2.../Concat_1]}, Tactic: 0x0000000000000000, /Slice_1_output_0 (Float[1,525987,16]), /TopK_output_1 (Int32[3840]), /Unsqueeze_1_output_0 (Float[1,1,3840]), /Unsqueeze_2_output_0 (Float[1,3840,4]), /NonMaxSuppression_output_0 (Int32[-1,3]), (Unnamed Layer* 718) [NMS]_1_output (Int32[]) -> output (Float[-1,21])
Layer(TrainStation): [trainStation3], Tactic: 0x0000000000000000,  -> 
[10/31/2023-17:58:51] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +15, GPU +29, now: CPU 15, GPU 29 (MiB)
[10/31/2023-17:58:51] [I] Engine built in 320.636 sec.
[10/31/2023-17:58:51] [I] [TRT] Loaded engine size: 18 MiB
[10/31/2023-17:58:51] [V] [TRT] Deserialization required 46178 microseconds.
[10/31/2023-17:58:51] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +28, now: CPU 0, GPU 28 (MiB)
[10/31/2023-17:58:51] [I] Engine deserialized in 0.0468014 sec.
[10/31/2023-17:58:51] [V] [TRT] Total per-runner device persistent memory is 13626368
[10/31/2023-17:58:51] [V] [TRT] Total per-runner host persistent memory is 123840
[10/31/2023-17:58:52] [V] [TRT] Allocated activation device memory of size 756727296
[10/31/2023-17:58:52] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +735, now: CPU 0, GPU 763 (MiB)
[10/31/2023-17:58:52] [I] Setting persistentCacheLimit to 0 bytes.
[10/31/2023-17:58:52] [V] Using enqueueV3.
[10/31/2023-17:58:52] [I] Using random values for input input
[10/31/2023-17:58:52] [I] Created input binding for input with dimensions 1x3x2208x3872
[10/31/2023-17:58:52] [I] Using random values for output output
[10/31/2023-17:58:52] [I] Created output binding for output with dimensions -1x21
[10/31/2023-17:58:52] [I] [TRT] The profiling verbosity was set to ProfilingVerbosity::kLAYER_NAMES_ONLY when the engine was built, so only the layer names will be returned. Rebuild the engine with ProfilingVerbosity::kDETAILED to get more verbose layer information.
[10/31/2023-17:58:52] [I] Starting inference
[10/31/2023-17:58:57] [I] The e2e network timing is not reported since it is inaccurate due to the extra synchronizations when the profiler is enabled.
[10/31/2023-17:58:57] [I] To show e2e network timing report, add --separateProfileRun to profile layer timing in a separate run or remove --dumpProfile to disable the profiler.
[10/31/2023-17:58:57] [I] 
[10/31/2023-17:58:57] [I] === Profile (11 iterations ) ===
[10/31/2023-17:58:57] [I]                                                                                                                                                                                                                                                          Layer   Time (ms)   Avg. Time (ms)   Median Time (ms)   Time %
[10/31/2023-17:58:57] [I]                                                                                                                                                                                                                                                [trainStation1]       16.72           1.5200             1.0890      0.4
[10/31/2023-17:58:57] [I]                                                                                                                                                                                                                /backbone/conv1/_input_quantizer/QuantizeLinear       46.15           4.1954             4.0638      1.0
[10/31/2023-17:58:57] [I]                                                                                                                                                                backbone.conv1.weight + /backbone/conv1/_weight_quantizer/QuantizeLinear + /backbone/conv1/Conv      657.65          59.7861            58.6879     14.8
[10/31/2023-17:58:57] [I]                                                                                                                                                                                                                                      /backbone/maxpool/MaxPool      192.64          17.5129            17.4594      4.3
[10/31/2023-17:58:57] [I]                                                                                                                                                                                                /backbone/layer1/layer1.0/conv1/_input_quantizer/QuantizeLinear       45.77           4.1607             4.1571      1.0
[10/31/2023-17:58:57] [I]                                                                           Reformatting CopyNode for Input Tensor 0 to backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv       42.04           3.8220             3.8222      0.9
[10/31/2023-17:58:57] [I]                                                                                                                       backbone.layer1.0.conv1.weight + /backbone/layer1/layer1.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv1/Conv       72.47           6.5886             6.5919      1.6
[10/31/2023-17:58:57] [I]                                               backbone.layer1.0.conv2.weight + /backbone/layer1/layer1.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.0/conv2/Conv + /backbone/layer1/layer1.0/Add + /backbone/layer1/layer1.0/relu_1/Relu      156.53          14.2299            14.2284      3.5
[10/31/2023-17:58:57] [I]                                                                                                                                                                                                /backbone/layer1/layer1.1/conv1/_input_quantizer/QuantizeLinear       43.30           3.9361             3.9367      1.0
[10/31/2023-17:58:57] [I]                                                                           Reformatting CopyNode for Input Tensor 0 to backbone.layer1.1.conv1.weight + /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.1/conv1/Conv       42.03           3.8209             3.8205      0.9
[10/31/2023-17:58:57] [I]                                                                                                                       backbone.layer1.1.conv1.weight + /backbone/layer1/layer1.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.1/conv1/Conv       72.37           6.5789             6.5778      1.6
[10/31/2023-17:58:57] [I]                                               backbone.layer1.1.conv2.weight + /backbone/layer1/layer1.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer1/layer1.1/conv2/Conv + /backbone/layer1/layer1.1/Add + /backbone/layer1/layer1.1/relu_1/Relu      156.51          14.2285            14.2342      3.5
[10/31/2023-17:58:57] [I]                                                                                                                                                                              /backbone/layer2/layer2.0/downsample/downsample.0/_input_quantizer/QuantizeLinear       43.30           3.9359             3.9379      1.0
[10/31/2023-17:58:57] [I]                                                                            backbone.layer2.0.downsample.0.weight + /backbone/layer2/layer2.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/downsample/downsample.0/Conv       39.93           3.6304             3.6315      0.9
[10/31/2023-17:58:57] [I]                                                                           Reformatting CopyNode for Input Tensor 0 to backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv       41.86           3.8057             3.8070      0.9
[10/31/2023-17:58:57] [I]                                                                                                                       backbone.layer2.0.conv1.weight + /backbone/layer2/layer2.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv1/Conv       34.06           3.0964             3.0948      0.8
[10/31/2023-17:58:57] [I]                                                                      PWN((Unnamed Layer* 112) [ElementWise], PWN(PWN((Unnamed Layer* 109) [Constant] + (Unnamed Layer* 110) [ElementWise], (Unnamed Layer* 111) [Unary]), (Unnamed Layer* 113) [ElementWise]))       44.52           4.0473             4.0484      1.0
[10/31/2023-17:58:57] [I]  backbone.layer2.0.downsample.1.weight + (Unnamed Layer* 105) [Shuffle] + (Unnamed Layer* 114) [ElementWise] + backbone.layer2.0.bn2.bias + Identity_2 + (Unnamed Layer* 106) [Shuffle] + /backbone/layer2/layer2.0/downsample/downsample.1/BatchNormalization       36.74           3.3400             3.3405      0.8
[10/31/2023-17:58:57] [I]                                               backbone.layer2.0.conv2.weight + /backbone/layer2/layer2.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.0/conv2/Conv + /backbone/layer2/layer2.0/Add + /backbone/layer2/layer2.0/relu_1/Relu       97.35           8.8501             8.8511      2.2
[10/31/2023-17:58:57] [I]                                                                                                                                                                                                /backbone/layer2/layer2.1/conv1/_input_quantizer/QuantizeLinear       21.12           1.9204             1.9214      0.5
[10/31/2023-17:58:57] [I]                                                                           Reformatting CopyNode for Input Tensor 0 to backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv       23.85           2.1683             2.1699      0.5
[10/31/2023-17:58:57] [I]                                                                                                                       backbone.layer2.1.conv1.weight + /backbone/layer2/layer2.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv1/Conv       54.90           4.9906             4.9918      1.2
[10/31/2023-17:58:57] [I]                                               backbone.layer2.1.conv2.weight + /backbone/layer2/layer2.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer2/layer2.1/conv2/Conv + /backbone/layer2/layer2.1/Add + /backbone/layer2/layer2.1/relu_1/Relu       97.49           8.8627             8.8561      2.2
[10/31/2023-17:58:57] [I]                                                                                                                                                                              /backbone/layer3/layer3.0/downsample/downsample.0/_input_quantizer/QuantizeLinear       45.14           4.1036             4.0994      1.0
[10/31/2023-17:58:57] [I]                                                                                                                                                                                               /neck/detect3/conv1/conv/_input_quantizer/QuantizeLinear_clone_1       45.06           4.0967             4.0961      1.0
[10/31/2023-17:58:57] [I]                                                                            backbone.layer3.0.downsample.0.weight + /backbone/layer3/layer3.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/downsample/downsample.0/Conv       22.37           2.0338             2.0271      0.5
[10/31/2023-17:58:57] [I]                                                                                                                       backbone.layer3.0.conv1.weight + /backbone/layer3/layer3.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv1/Conv       28.88           2.6251             2.6238      0.7
[10/31/2023-17:58:57] [I]                                                                      PWN((Unnamed Layer* 186) [ElementWise], PWN(PWN((Unnamed Layer* 183) [Constant] + (Unnamed Layer* 184) [ElementWise], (Unnamed Layer* 185) [Unary]), (Unnamed Layer* 187) [ElementWise]))       22.02           2.0016             1.9840      0.5
[10/31/2023-17:58:57] [I]  backbone.layer3.0.downsample.1.weight + (Unnamed Layer* 179) [Shuffle] + (Unnamed Layer* 188) [ElementWise] + backbone.layer3.0.bn2.bias + Identity_1 + (Unnamed Layer* 180) [Shuffle] + /backbone/layer3/layer3.0/downsample/downsample.1/BatchNormalization       18.26           1.6601             1.6528      0.4
[10/31/2023-17:58:57] [I]                                               backbone.layer3.0.conv2.weight + /backbone/layer3/layer3.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.0/conv2/Conv + /backbone/layer3/layer3.0/Add + /backbone/layer3/layer3.0/relu_1/Relu       70.90           6.4453             6.4199      1.6
[10/31/2023-17:58:57] [I]                                                                                                                                                                                                /backbone/layer3/layer3.1/conv1/_input_quantizer/QuantizeLinear       22.60           2.0548             2.0557      0.5
[10/31/2023-17:58:57] [I]                                                                                                                       backbone.layer3.1.conv1.weight + /backbone/layer3/layer3.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv1/Conv       50.40           4.5822             4.5815      1.1
[10/31/2023-17:58:57] [I]                                               backbone.layer3.1.conv2.weight + /backbone/layer3/layer3.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer3/layer3.1/conv2/Conv + /backbone/layer3/layer3.1/Add + /backbone/layer3/layer3.1/relu_1/Relu       70.74           6.4310             6.4209      1.6
[10/31/2023-17:58:57] [I]                                                                                                                                                                              /backbone/layer4/layer4.0/downsample/downsample.0/_input_quantizer/QuantizeLinear       22.61           2.0553             2.0544      0.5
[10/31/2023-17:58:57] [I]                                                                                                                                                                                               /neck/detect2/conv1/conv/_input_quantizer/QuantizeLinear_clone_1       22.60           2.0543             2.0556      0.5
[10/31/2023-17:58:57] [I]                                                                            backbone.layer4.0.downsample.0.weight + /backbone/layer4/layer4.0/downsample/downsample.0/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/downsample/downsample.0/Conv       16.04           1.4581             1.4608      0.4
[10/31/2023-17:58:57] [I]                                                                                                                       backbone.layer4.0.conv1.weight + /backbone/layer4/layer4.0/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv1/Conv       26.30           2.3906             2.3892      0.6
[10/31/2023-17:58:57] [I]                                                                      PWN((Unnamed Layer* 260) [ElementWise], PWN(PWN((Unnamed Layer* 257) [Constant] + (Unnamed Layer* 258) [ElementWise], (Unnamed Layer* 259) [Unary]), (Unnamed Layer* 261) [ElementWise]))       14.72           1.3381             1.3371      0.3
[10/31/2023-17:58:57] [I]  backbone.layer4.0.downsample.1.weight + (Unnamed Layer* 253) [Shuffle] + (Unnamed Layer* 262) [ElementWise] + backbone.layer4.0.bn2.bias + Identity_0 + (Unnamed Layer* 254) [Shuffle] + /backbone/layer4/layer4.0/downsample/downsample.1/BatchNormalization        9.22           0.8379             0.8340      0.2
[10/31/2023-17:58:57] [I]                                               backbone.layer4.0.conv2.weight + /backbone/layer4/layer4.0/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.0/conv2/Conv + /backbone/layer4/layer4.0/Add + /backbone/layer4/layer4.0/relu_1/Relu       56.26           5.1141             5.1096      1.3
[10/31/2023-17:58:57] [I]                                                                                                                                                                                                /backbone/layer4/layer4.1/conv1/_input_quantizer/QuantizeLinear       11.46           1.0416             1.0422      0.3
[10/31/2023-17:58:57] [I]                                                                                                                       backbone.layer4.1.conv1.weight + /backbone/layer4/layer4.1/conv1/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv1/Conv       48.11           4.3740             4.3739      1.1
[10/31/2023-17:58:57] [I]                                               backbone.layer4.1.conv2.weight + /backbone/layer4/layer4.1/conv2/_weight_quantizer/QuantizeLinear + /backbone/layer4/layer4.1/conv2/Conv + /backbone/layer4/layer4.1/Add + /backbone/layer4/layer4.1/relu_1/Relu       56.12           5.1017             5.0729      1.3
[10/31/2023-17:58:57] [I]                                                                                                                                                                                                       /neck/detect1/conv1/conv/_input_quantizer/QuantizeLinear       11.44           1.0400             1.0422      0.3
[10/31/2023-17:58:57] [I]                                                                                           neck.detect1.conv1.conv.weight + /neck/detect1/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv1/conv/Conv + PWN(/neck/detect1/conv1/relu/LeakyRelu)        4.82           0.4386             0.4353      0.1
[10/31/2023-17:58:57] [I]                                                                                           neck.detect1.conv2.conv.weight + /neck/detect1/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv2/conv/Conv + PWN(/neck/detect1/conv2/relu/LeakyRelu)       26.73           2.4304             2.4297      0.6
[10/31/2023-17:58:57] [I]                                                                                           neck.detect1.conv3.conv.weight + /neck/detect1/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv3/conv/Conv + PWN(/neck/detect1/conv3/relu/LeakyRelu)        4.82           0.4382             0.4368      0.1
[10/31/2023-17:58:57] [I]                                                                                           neck.detect1.conv4.conv.weight + /neck/detect1/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv4/conv/Conv + PWN(/neck/detect1/conv4/relu/LeakyRelu)       26.78           2.4345             2.4344      0.6
[10/31/2023-17:58:57] [I]                                                                                           neck.detect1.conv5.conv.weight + /neck/detect1/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv5/conv/Conv + PWN(/neck/detect1/conv5/relu/LeakyRelu)        4.81           0.4373             0.4351      0.1
[10/31/2023-17:58:57] [I]                                                                                                                           neck.conv1.conv.weight + /neck/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/conv1/conv/Conv + PWN(/neck/conv1/relu/LeakyRelu)        2.17           0.1969             0.1945      0.0
[10/31/2023-17:58:57] [I]                                                                                           neck.detect1.conv6.conv.weight + /neck/detect1/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect1/conv6/conv/Conv + PWN(/neck/detect1/conv6/relu/LeakyRelu)       26.77           2.4335             2.4329      0.6
[10/31/2023-17:58:57] [I]                                                                                                                                                                                                                                                   /neck/Resize        2.97           0.2697             0.2697      0.1
[10/31/2023-17:58:57] [I]                                                                                                                                                    neck.detect1.conv7.weight + /neck/detect1/conv7/_weight_quantizer/QuantizeLinear + /neck/detect1/conv7/Conv        2.65           0.2405             0.2398      0.1
[10/31/2023-17:58:57] [I]                                                                                                                                                                                                                /neck/Concat_/neck/Resize_output_0_clone_0 copy        4.59           0.4176             0.4079      0.1
[10/31/2023-17:58:57] [I]                                                                                           neck.detect2.conv1.conv.weight + /neck/detect2/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv1/conv/Conv + PWN(/neck/detect2/conv1/relu/LeakyRelu)        8.04           0.7305             0.7239      0.2
[10/31/2023-17:58:57] [I]                                                                                           neck.detect2.conv2.conv.weight + /neck/detect2/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv2/conv/Conv + PWN(/neck/detect2/conv2/relu/LeakyRelu)       30.34           2.7581             2.7587      0.7
[10/31/2023-17:58:57] [I]                                                                                           neck.detect2.conv3.conv.weight + /neck/detect2/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv3/conv/Conv + PWN(/neck/detect2/conv3/relu/LeakyRelu)        7.12           0.6474             0.6453      0.2
[10/31/2023-17:58:57] [I]                                                                                           neck.detect2.conv4.conv.weight + /neck/detect2/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv4/conv/Conv + PWN(/neck/detect2/conv4/relu/LeakyRelu)       30.36           2.7597             2.7583      0.7
[10/31/2023-17:58:57] [I]                                                                                           neck.detect2.conv5.conv.weight + /neck/detect2/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv5/conv/Conv + PWN(/neck/detect2/conv5/relu/LeakyRelu)        7.04           0.6398             0.6395      0.2
[10/31/2023-17:58:57] [I]                                                                                                                           neck.conv2.conv.weight + /neck/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/conv2/conv/Conv + PWN(/neck/conv2/relu/LeakyRelu)        3.15           0.2866             0.2860      0.1
[10/31/2023-17:58:57] [I]                                                                                           neck.detect2.conv6.conv.weight + /neck/detect2/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect2/conv6/conv/Conv + PWN(/neck/detect2/conv6/relu/LeakyRelu)       30.37           2.7611             2.7613      0.7
[10/31/2023-17:58:57] [I]                                                                                                                                                                                                                                                 /neck/Resize_1        5.67           0.5157             0.5169      0.1
[10/31/2023-17:58:57] [I]                                                                                                                                                    neck.detect2.conv7.weight + /neck/detect2/conv7/_weight_quantizer/QuantizeLinear + /neck/detect2/conv7/Conv        6.59           0.5989             0.6003      0.1
[10/31/2023-17:58:57] [I]                                                                                                                                                                                                            /neck/Concat_1_/neck/Resize_1_output_0_clone_0 copy        8.82           0.8016             0.7951      0.2
[10/31/2023-17:58:57] [I]                                                                                           neck.detect3.conv1.conv.weight + /neck/detect3/conv1/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv1/conv/Conv + PWN(/neck/detect3/conv1/relu/LeakyRelu)       13.32           1.2110             1.2078      0.3
[10/31/2023-17:58:57] [I]                                                                                           neck.detect3.conv2.conv.weight + /neck/detect3/conv2/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv2/conv/Conv + PWN(/neck/detect3/conv2/relu/LeakyRelu)       38.96           3.5422             3.5423      0.9
[10/31/2023-17:58:57] [I]                                                                                           neck.detect3.conv3.conv.weight + /neck/detect3/conv3/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv3/conv/Conv + PWN(/neck/detect3/conv3/relu/LeakyRelu)       11.86           1.0782             1.0741      0.3
[10/31/2023-17:58:57] [I]                                                                                           neck.detect3.conv4.conv.weight + /neck/detect3/conv4/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv4/conv/Conv + PWN(/neck/detect3/conv4/relu/LeakyRelu)       38.82           3.5287             3.5302      0.9
[10/31/2023-17:58:57] [I]                                                                                           neck.detect3.conv5.conv.weight + /neck/detect3/conv5/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv5/conv/Conv + PWN(/neck/detect3/conv5/relu/LeakyRelu)       11.92           1.0840             1.0813      0.3
[10/31/2023-17:58:57] [I]                                                                                           neck.detect3.conv6.conv.weight + /neck/detect3/conv6/conv/_weight_quantizer/QuantizeLinear + /neck/detect3/conv6/conv/Conv + PWN(/neck/detect3/conv6/relu/LeakyRelu)       38.95           3.5408             3.5406      0.9
[10/31/2023-17:58:57] [I]                                                                                                                                                    neck.detect3.conv7.weight + /neck/detect3/conv7/_weight_quantizer/QuantizeLinear + /neck/detect3/conv7/Conv       19.85           1.8048             1.7997      0.4
[10/31/2023-17:58:57] [I]                                                                                                                                                                                                        {ForeignNode[/head/Constant_2_output_0.../Unsqueeze_2]}     1245.08         113.1891           113.1503     28.0
[10/31/2023-17:58:57] [I]                                                                                                                                                                                                                                         /NonMaxSuppression_246        2.22           0.2019             0.1812      0.1
[10/31/2023-17:58:57] [I]                                                                                                                                                                                                     (Unnamed Layer* 718) [NMS]_1_output[DevicetoShapeHostCopy]        1.42           0.1288             0.0598      0.0
[10/31/2023-17:58:57] [I]                                                                                                                                                                                                                                                [trainStation2]        0.72           0.0657             0.0244      0.0
[10/31/2023-17:58:57] [I]                                                                                                                                                                                                                            {ForeignNode[/Split_2.../Concat_1]}        2.70           0.2456             0.0952      0.1
[10/31/2023-17:58:57] [I]                                                                                                                                                                                                                                                [trainStation3]        1.00           0.0909             0.0543      0.0
[10/31/2023-17:58:57] [I]                                                                                                                                                                                                                                                          Total     4441.97         403.8158           402.0093    100.0
[10/31/2023-17:58:57] [I] 
&&&& PASSED TensorRT.trtexec [TensorRT v8502] # /usr/src/tensorrt/bin/trtexec --int8 --onnx=yolo_pytorch_quantized.onnx --exportTimes=times.json --dumpProfile --exportProfile=profile.json --exportLayerInfo=layer_info.json --verbose
